{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOQsdUdvILUsWTIwlwKRZpC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. import necessary libraries"],"metadata":{"id":"KunKPGTzyoKL"}},{"cell_type":"code","source":["import argparse\n","import logging\n","import os\n","import random\n","import numpy as np\n","import torch\n","import torch.backends.cudnn as cudnn\n","from networks.vision_transformer import SwinUnet as ViT_seg\n","from trainer import trainer_synapse\n","from config import get_config"],"metadata":{"id":"OiZm4wmNwgeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. based on the deafult argument of parser we create the required paths and put the required files in the directories\n","\n","the most important ones are:\n","1. data > train and test\n","2. the train data list\n","3. and pretrained model"],"metadata":{"id":"4V4xQdCi_X1T"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--root_path', type=str, default='../data/Synapse/train_npz', help='root dir for data')\n","parser.add_argument('--dataset', type=str, default='Synapse', help='experiment_name')\n","parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse', help='list dir')\n","parser.add_argument('--num_classes', type=int,default=9, help='output channel of network')\n","parser.add_argument('--output_dir', type=str, help='output dir')                   \n","parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n","parser.add_argument('--max_epochs', type=int, default=150, help='maximum epoch number to train')\n","parser.add_argument('--batch_size', type=int, default=24, help='batch_size per gpu')\n","parser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\n","parser.add_argument('--deterministic', type=int,  default=1,  help='whether use deterministic training')\n","parser.add_argument('--base_lr', type=float,  default=0.01,help='segmentation network learning rate')\n","parser.add_argument('--img_size', type=int,default=224, help='input patch size of network input')\n","parser.add_argument('--seed', type=int,default=1234, help='random seed')\n","parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n","parser.add_argument(  \"--opts\", help=\"Modify config options by adding 'KEY VALUE' pairs. \", default=None,nargs='+',)\n","parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n","parser.add_argument('--resume', help='resume from checkpoint')\n","parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n","parser.add_argument('--use-checkpoint', action='store_true', help=\"whether to use gradient checkpointing to save memory\")\n","parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],  help='mixed precision opt level, if O0, no amp is used')\n","parser.add_argument('--tag', help='tag of experiment')\n","parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n","parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n","parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],help='no: no cache, '\n","                                                                                                   'full: cache all data, '\n","                                                                                              'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n"],"metadata":{"id":"lcR0N-V4xQ7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. we are working on synapse dataset in this code: so the code is picking the right direcotry where we put our synapse dataset if Synapse is chosen"],"metadata":{"id":"Z70oHSMgyycn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKW74KCLWYj7"},"outputs":[],"source":["args = parser.parse_args()\n","if args.dataset == \"Synapse\":           \n","    args.root_path = os.path.join(args.root_path, \"train_npz\")"]},{"cell_type":"markdown","source":["4. there is a configuration file. by using get_config we are getting the parsed arguments and updating the configuration file and creating a new config file to be used later on"],"metadata":{"id":"3ECur_ot1y--"}},{"cell_type":"code","source":["config = get_config(args)"],"metadata":{"id":"L8YJpU7I1xTP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. here now we are ready to train. before that we create our model to be trained"],"metadata":{"id":"wIx5or_4GUzv"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    if not args.deterministic:\n","        cudnn.benchmark = True\n","        cudnn.deterministic = False\n","    else:\n","        cudnn.benchmark = False\n","        cudnn.deterministic = True\n","\n","    #preprarion elements that need random number with a seed so random numbers created by them be predictable\n","    random.seed(args.seed) #random (predictable) number\n","    np.random.seed(args.seed) #random (predictable) numbpy array\n","    torch.manual_seed(args.seed) #Sets the seed for generating random numbers. Returns a torch.Generator object.\n","    torch.cuda.manual_seed(args.seed) #Sets the seed for generating random numbers for the current GPU. Itâ€™s safe to call this function if CUDA is not available; in that case, it is silently ignored.\n","\n","    dataset_name = args.dataset #retrieve the dataset name from args set before and if it is synapse configure a dict for it as follows\n","    #for the Synapse dataset we have the following dictionary that for dataset=synapse there is a set of configurations\n","    #if the dataset is something except Synapse then its config is added in the following dict and ( read *)\n","    dataset_config = {\n","        'Synapse': {\n","            'root_path': args.root_path,\n","            'list_dir': './lists/lists_Synapse',    \n","            'num_classes': 9,\n","        },\n","    }\n","\n","    if args.batch_size != 24 and args.batch_size % 6 == 0:\n","        args.base_lr *= args.batch_size / 24\n","    # * ..... and then values such as sumber of classes would be retrived based on name of dataset from dataset config > here we use synapse dataset so numberof classes\n","    #is as it is indicated in Synampese dataset_config\n","    args.num_classes = dataset_config[dataset_name]['num_classes']\n","    args.root_path = dataset_config[dataset_name]['root_path']\n","    args.list_dir = dataset_config[dataset_name]['list_dir']\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    #we have now directories and data probabaly ready for our NN layer\n","    #########################################################################################################################\n","    #first we create a SwinUNet Layer object as following, this object takes the config, and other parameters as below\n","    net = ViT_seg(config, img_size=args.img_size, num_classes=args.num_classes).cuda()  # how ? check  SwinUNet in network>vision_transformer\n","    #loads the pretrained model : how ? check the function load_from : in networks>vision_transformer\n","    net.load_from(config)\n","    # now we have a model with pretrained model parameters\n","    ##########################################################################################################################\n","    \n","    trainer = {'Synapse': trainer_synapse,}\n","    trainer[dataset_name](args, net, args.output_dir) #from trainer import trainer_synapse giving arguments : args, net, args.output_dir"],"metadata":{"id":"d2da2yoOylnQ"},"execution_count":null,"outputs":[]}]}