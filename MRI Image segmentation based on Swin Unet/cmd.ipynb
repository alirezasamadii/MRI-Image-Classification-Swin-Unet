{"cells":[{"cell_type":"markdown","source":["some main parts to be discussed ( and the realted subparts)\n","1. CMD\n","2. trainer\n","3. train\n","4. test\n","5. results visulization (implemented by me , it was missing in the original code by the  author)\n","\n"],"metadata":{"id":"kWN7_F4996dB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyCNAj5uz-3f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/MyDrive/Colab Notebooks/nn new\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7zxp67z0I7M"},"outputs":[],"source":["!pip install -r TransUNet/requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"dXcVnW4MtsMF"},"source":["the following libraries were missing from requirements.txt so I install them seperatly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysMXxUj42OTu"},"outputs":[],"source":["!pip install einops\n","!pip install timm\n","!pip install yacs\n","!pip install -U PyYAML"]},{"cell_type":"markdown","metadata":{"id":"x7pjm893BiLA"},"source":["# **Note that training 150 epochs on colab GPU will take around 6 hours**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gB7OcVGI0MBT","executionInfo":{"status":"ok","timestamp":1649079190880,"user_tz":-120,"elapsed":5637014,"user":{"displayName":"alireza samadifardheris","userId":"04854581352341172090"}},"outputId":"54b696af-6114-44f8-fd32-ac57a41b223e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","iteration 8955 : loss : 0.050373, loss_ce: 0.011911\n","iteration 8956 : loss : 0.046522, loss_ce: 0.012492\n","iteration 8957 : loss : 0.039208, loss_ce: 0.011954\n","iteration 8958 : loss : 0.055066, loss_ce: 0.016470\n","iteration 8959 : loss : 0.051830, loss_ce: 0.020356\n","iteration 8960 : loss : 0.047871, loss_ce: 0.020709\n","iteration 8961 : loss : 0.041421, loss_ce: 0.008731\n","iteration 8962 : loss : 0.051462, loss_ce: 0.012462\n","iteration 8963 : loss : 0.051229, loss_ce: 0.011296\n","iteration 8964 : loss : 0.104709, loss_ce: 0.014365\n","iteration 8965 : loss : 0.043950, loss_ce: 0.011368\n","iteration 8966 : loss : 0.048946, loss_ce: 0.010275\n","iteration 8967 : loss : 0.068758, loss_ce: 0.021403\n","iteration 8968 : loss : 0.045537, loss_ce: 0.017893\n","iteration 8969 : loss : 0.110088, loss_ce: 0.013156\n","iteration 8970 : loss : 0.062752, loss_ce: 0.008836\n","iteration 8971 : loss : 0.097893, loss_ce: 0.010524\n","iteration 8972 : loss : 0.054641, loss_ce: 0.013625\n","iteration 8973 : loss : 0.039988, loss_ce: 0.011913\n","iteration 8974 : loss : 0.049696, loss_ce: 0.012398\n","iteration 8975 : loss : 0.070918, loss_ce: 0.018600\n","iteration 8976 : loss : 0.049371, loss_ce: 0.016266\n","iteration 8977 : loss : 0.038771, loss_ce: 0.011925\n","iteration 8978 : loss : 0.040412, loss_ce: 0.011374\n","iteration 8979 : loss : 0.159308, loss_ce: 0.014771\n","iteration 8980 : loss : 0.034696, loss_ce: 0.011253\n","iteration 8981 : loss : 0.044339, loss_ce: 0.012324\n","iteration 8982 : loss : 0.043931, loss_ce: 0.011142\n","iteration 8983 : loss : 0.042054, loss_ce: 0.016999\n","iteration 8984 : loss : 0.110101, loss_ce: 0.011569\n","iteration 8985 : loss : 0.056457, loss_ce: 0.012135\n","iteration 8986 : loss : 0.043129, loss_ce: 0.015532\n","iteration 8987 : loss : 0.040693, loss_ce: 0.011778\n","iteration 8988 : loss : 0.050429, loss_ce: 0.017284\n","iteration 8989 : loss : 0.052089, loss_ce: 0.014391\n","iteration 8990 : loss : 0.044277, loss_ce: 0.009478\n","iteration 8991 : loss : 0.045601, loss_ce: 0.019058\n","iteration 8992 : loss : 0.031283, loss_ce: 0.010127\n","iteration 8993 : loss : 0.064227, loss_ce: 0.013685\n","iteration 8994 : loss : 0.049513, loss_ce: 0.026703\n","iteration 8995 : loss : 0.049274, loss_ce: 0.013793\n","iteration 8996 : loss : 0.073691, loss_ce: 0.011302\n","iteration 8997 : loss : 0.055361, loss_ce: 0.012490\n","iteration 8998 : loss : 0.051331, loss_ce: 0.011381\n","iteration 8999 : loss : 0.038716, loss_ce: 0.011341\n","iteration 9000 : loss : 0.098352, loss_ce: 0.011815\n","iteration 9001 : loss : 0.042692, loss_ce: 0.017497\n","iteration 9002 : loss : 0.040753, loss_ce: 0.010229\n","iteration 9003 : loss : 0.049530, loss_ce: 0.021626\n","iteration 9004 : loss : 0.047831, loss_ce: 0.017847\n","iteration 9005 : loss : 0.046490, loss_ce: 0.011864\n","iteration 9006 : loss : 0.037278, loss_ce: 0.011951\n","iteration 9007 : loss : 0.034894, loss_ce: 0.010456\n","iteration 9008 : loss : 0.055667, loss_ce: 0.011792\n","iteration 9009 : loss : 0.122675, loss_ce: 0.007867\n","iteration 9010 : loss : 0.041406, loss_ce: 0.008449\n","iteration 9011 : loss : 0.045740, loss_ce: 0.014209\n","iteration 9012 : loss : 0.060111, loss_ce: 0.021978\n","iteration 9013 : loss : 0.044888, loss_ce: 0.015452\n","iteration 9014 : loss : 0.054163, loss_ce: 0.016240\n","iteration 9015 : loss : 0.052566, loss_ce: 0.013338\n","iteration 9016 : loss : 0.041694, loss_ce: 0.012292\n","iteration 9017 : loss : 0.033862, loss_ce: 0.011294\n","iteration 9018 : loss : 0.045933, loss_ce: 0.021313\n","iteration 9019 : loss : 0.067841, loss_ce: 0.011352\n","iteration 9020 : loss : 0.041430, loss_ce: 0.020235\n","iteration 9021 : loss : 0.282587, loss_ce: 0.012456\n"," 65%|██████████████████          | 97/150 [1:54:15<1:02:06, 70.31s/it]iteration 9022 : loss : 0.100061, loss_ce: 0.008032\n","iteration 9023 : loss : 0.035478, loss_ce: 0.014666\n","iteration 9024 : loss : 0.038202, loss_ce: 0.013445\n","iteration 9025 : loss : 0.039878, loss_ce: 0.012073\n","iteration 9026 : loss : 0.050158, loss_ce: 0.011657\n","iteration 9027 : loss : 0.042586, loss_ce: 0.013486\n","iteration 9028 : loss : 0.049026, loss_ce: 0.021422\n","iteration 9029 : loss : 0.041019, loss_ce: 0.011800\n","iteration 9030 : loss : 0.045414, loss_ce: 0.012454\n","iteration 9031 : loss : 0.033967, loss_ce: 0.007903\n","iteration 9032 : loss : 0.041639, loss_ce: 0.008636\n","iteration 9033 : loss : 0.062192, loss_ce: 0.011578\n","iteration 9034 : loss : 0.048632, loss_ce: 0.020287\n","iteration 9035 : loss : 0.056941, loss_ce: 0.015057\n","iteration 9036 : loss : 0.082233, loss_ce: 0.012491\n","iteration 9037 : loss : 0.035950, loss_ce: 0.014884\n","iteration 9038 : loss : 0.099477, loss_ce: 0.008378\n","iteration 9039 : loss : 0.047518, loss_ce: 0.013587\n","iteration 9040 : loss : 0.036774, loss_ce: 0.016928\n","iteration 9041 : loss : 0.221127, loss_ce: 0.004839\n","iteration 9042 : loss : 0.045570, loss_ce: 0.016862\n","iteration 9043 : loss : 0.047508, loss_ce: 0.017644\n","iteration 9044 : loss : 0.035846, loss_ce: 0.014886\n","iteration 9045 : loss : 0.159835, loss_ce: 0.008318\n","iteration 9046 : loss : 0.041369, loss_ce: 0.011759\n","iteration 9047 : loss : 0.042978, loss_ce: 0.014841\n","iteration 9048 : loss : 0.040497, loss_ce: 0.014391\n","iteration 9049 : loss : 0.069231, loss_ce: 0.010915\n","iteration 9050 : loss : 0.074563, loss_ce: 0.022255\n","iteration 9051 : loss : 0.047376, loss_ce: 0.014034\n","iteration 9052 : loss : 0.097563, loss_ce: 0.011934\n","iteration 9053 : loss : 0.100387, loss_ce: 0.012531\n","iteration 9054 : loss : 0.040614, loss_ce: 0.017136\n","iteration 9055 : loss : 0.052423, loss_ce: 0.008759\n","iteration 9056 : loss : 0.045523, loss_ce: 0.010396\n","iteration 9057 : loss : 0.037894, loss_ce: 0.014673\n","iteration 9058 : loss : 0.058170, loss_ce: 0.012056\n","iteration 9059 : loss : 0.047619, loss_ce: 0.017501\n","iteration 9060 : loss : 0.114993, loss_ce: 0.007960\n","iteration 9061 : loss : 0.043539, loss_ce: 0.011540\n","iteration 9062 : loss : 0.038856, loss_ce: 0.015072\n","iteration 9063 : loss : 0.046149, loss_ce: 0.011129\n","iteration 9064 : loss : 0.041723, loss_ce: 0.010220\n","iteration 9065 : loss : 0.057897, loss_ce: 0.014007\n","iteration 9066 : loss : 0.037093, loss_ce: 0.012158\n","iteration 9067 : loss : 0.044924, loss_ce: 0.016649\n","iteration 9068 : loss : 0.043867, loss_ce: 0.017068\n","iteration 9069 : loss : 0.038079, loss_ce: 0.013010\n","iteration 9070 : loss : 0.045612, loss_ce: 0.015081\n","iteration 9071 : loss : 0.038264, loss_ce: 0.010858\n","iteration 9072 : loss : 0.042914, loss_ce: 0.020141\n","iteration 9073 : loss : 0.102727, loss_ce: 0.007800\n","iteration 9074 : loss : 0.053115, loss_ce: 0.016320\n","iteration 9075 : loss : 0.041819, loss_ce: 0.010486\n","iteration 9076 : loss : 0.037719, loss_ce: 0.008408\n","iteration 9077 : loss : 0.050467, loss_ce: 0.015906\n","iteration 9078 : loss : 0.046905, loss_ce: 0.016525\n","iteration 9079 : loss : 0.045431, loss_ce: 0.018448\n","iteration 9080 : loss : 0.047889, loss_ce: 0.014167\n","iteration 9081 : loss : 0.050485, loss_ce: 0.010759\n","iteration 9082 : loss : 0.046395, loss_ce: 0.011532\n","iteration 9083 : loss : 0.043414, loss_ce: 0.016605\n","iteration 9084 : loss : 0.040229, loss_ce: 0.015217\n","iteration 9085 : loss : 0.037788, loss_ce: 0.011741\n","iteration 9086 : loss : 0.044824, loss_ce: 0.008664\n","iteration 9087 : loss : 0.046074, loss_ce: 0.010450\n","iteration 9088 : loss : 0.040081, loss_ce: 0.014453\n","iteration 9089 : loss : 0.048066, loss_ce: 0.012438\n","iteration 9090 : loss : 0.100419, loss_ce: 0.010293\n","iteration 9091 : loss : 0.054460, loss_ce: 0.017010\n","iteration 9092 : loss : 0.043277, loss_ce: 0.013074\n","iteration 9093 : loss : 0.034052, loss_ce: 0.004021\n","iteration 9094 : loss : 0.039166, loss_ce: 0.012562\n","iteration 9095 : loss : 0.047323, loss_ce: 0.014939\n","iteration 9096 : loss : 0.038072, loss_ce: 0.009710\n","iteration 9097 : loss : 0.040712, loss_ce: 0.011339\n","iteration 9098 : loss : 0.069588, loss_ce: 0.013017\n","iteration 9099 : loss : 0.046262, loss_ce: 0.019714\n","iteration 9100 : loss : 0.043398, loss_ce: 0.012627\n","iteration 9101 : loss : 0.042412, loss_ce: 0.015635\n","iteration 9102 : loss : 0.043955, loss_ce: 0.010434\n","iteration 9103 : loss : 0.052768, loss_ce: 0.012899\n","iteration 9104 : loss : 0.042620, loss_ce: 0.010737\n","iteration 9105 : loss : 0.038146, loss_ce: 0.011918\n","iteration 9106 : loss : 0.058015, loss_ce: 0.010961\n","iteration 9107 : loss : 0.058838, loss_ce: 0.015074\n","iteration 9108 : loss : 0.041848, loss_ce: 0.017289\n","iteration 9109 : loss : 0.041763, loss_ce: 0.018067\n","iteration 9110 : loss : 0.036744, loss_ce: 0.009604\n","iteration 9111 : loss : 0.037922, loss_ce: 0.011370\n","iteration 9112 : loss : 0.039477, loss_ce: 0.014459\n","iteration 9113 : loss : 0.042828, loss_ce: 0.016889\n","iteration 9114 : loss : 0.237848, loss_ce: 0.000059\n"," 65%|██████████████████▎         | 98/150 [1:55:25<1:00:55, 70.29s/it]iteration 9115 : loss : 0.078967, loss_ce: 0.011031\n","iteration 9116 : loss : 0.061530, loss_ce: 0.007171\n","iteration 9117 : loss : 0.064616, loss_ce: 0.025032\n","iteration 9118 : loss : 0.101734, loss_ce: 0.053429\n","iteration 9119 : loss : 0.083509, loss_ce: 0.040788\n","iteration 9120 : loss : 0.055758, loss_ce: 0.016281\n","iteration 9121 : loss : 0.059552, loss_ce: 0.021395\n","iteration 9122 : loss : 0.064409, loss_ce: 0.034077\n","iteration 9123 : loss : 0.064915, loss_ce: 0.021502\n","iteration 9124 : loss : 0.058343, loss_ce: 0.023310\n","iteration 9125 : loss : 0.094155, loss_ce: 0.023605\n","iteration 9126 : loss : 0.076739, loss_ce: 0.035536\n","iteration 9127 : loss : 0.129681, loss_ce: 0.010650\n","iteration 9128 : loss : 0.063785, loss_ce: 0.023709\n","iteration 9129 : loss : 0.056956, loss_ce: 0.026889\n","iteration 9130 : loss : 0.054316, loss_ce: 0.027044\n","iteration 9131 : loss : 0.067434, loss_ce: 0.028696\n","iteration 9132 : loss : 0.069684, loss_ce: 0.026348\n","iteration 9133 : loss : 0.056482, loss_ce: 0.024029\n","iteration 9134 : loss : 0.055337, loss_ce: 0.017006\n","iteration 9135 : loss : 0.077060, loss_ce: 0.022166\n","iteration 9136 : loss : 0.052560, loss_ce: 0.025956\n","iteration 9137 : loss : 0.081577, loss_ce: 0.042133\n","iteration 9138 : loss : 0.062532, loss_ce: 0.023610\n","iteration 9139 : loss : 0.059575, loss_ce: 0.020200\n","iteration 9140 : loss : 0.054595, loss_ce: 0.026851\n","iteration 9141 : loss : 0.064997, loss_ce: 0.037214\n","iteration 9142 : loss : 0.048660, loss_ce: 0.024950\n","iteration 9143 : loss : 0.059883, loss_ce: 0.030106\n","iteration 9144 : loss : 0.048954, loss_ce: 0.019020\n","iteration 9145 : loss : 0.068205, loss_ce: 0.027778\n","iteration 9146 : loss : 0.059544, loss_ce: 0.030409\n","iteration 9147 : loss : 0.093177, loss_ce: 0.010865\n","iteration 9148 : loss : 0.048037, loss_ce: 0.010740\n","iteration 9149 : loss : 0.050739, loss_ce: 0.015162\n","iteration 9150 : loss : 0.051648, loss_ce: 0.013097\n","iteration 9151 : loss : 0.053502, loss_ce: 0.028988\n","iteration 9152 : loss : 0.129869, loss_ce: 0.027276\n","iteration 9153 : loss : 0.062306, loss_ce: 0.022255\n","iteration 9154 : loss : 0.052165, loss_ce: 0.018621\n","iteration 9155 : loss : 0.065184, loss_ce: 0.018316\n","iteration 9156 : loss : 0.166364, loss_ce: 0.018051\n","iteration 9157 : loss : 0.052022, loss_ce: 0.028524\n","iteration 9158 : loss : 0.112337, loss_ce: 0.019589\n","iteration 9159 : loss : 0.051233, loss_ce: 0.015336\n","iteration 9160 : loss : 0.051188, loss_ce: 0.028028\n","iteration 9161 : loss : 0.055801, loss_ce: 0.023337\n","iteration 9162 : loss : 0.049203, loss_ce: 0.016940\n","iteration 9163 : loss : 0.059130, loss_ce: 0.024975\n","iteration 9164 : loss : 0.053617, loss_ce: 0.022422\n","iteration 9165 : loss : 0.049357, loss_ce: 0.022013\n","iteration 9166 : loss : 0.099834, loss_ce: 0.006732\n","iteration 9167 : loss : 0.056930, loss_ce: 0.016369\n","iteration 9168 : loss : 0.048359, loss_ce: 0.015326\n","iteration 9169 : loss : 0.052598, loss_ce: 0.018106\n","iteration 9170 : loss : 0.049556, loss_ce: 0.016404\n","iteration 9171 : loss : 0.048329, loss_ce: 0.015532\n","iteration 9172 : loss : 0.056244, loss_ce: 0.016920\n","iteration 9173 : loss : 0.126393, loss_ce: 0.019729\n","iteration 9174 : loss : 0.049745, loss_ce: 0.018825\n","iteration 9175 : loss : 0.041269, loss_ce: 0.013613\n","iteration 9176 : loss : 0.060118, loss_ce: 0.023152\n","iteration 9177 : loss : 0.069956, loss_ce: 0.015239\n","iteration 9178 : loss : 0.047575, loss_ce: 0.017549\n","iteration 9179 : loss : 0.053112, loss_ce: 0.016153\n","iteration 9180 : loss : 0.109076, loss_ce: 0.016456\n","iteration 9181 : loss : 0.045897, loss_ce: 0.016089\n","iteration 9182 : loss : 0.054249, loss_ce: 0.013088\n","iteration 9183 : loss : 0.054696, loss_ce: 0.016054\n","iteration 9184 : loss : 0.044900, loss_ce: 0.012734\n","iteration 9185 : loss : 0.056472, loss_ce: 0.010631\n","iteration 9186 : loss : 0.053459, loss_ce: 0.022646\n","iteration 9187 : loss : 0.049747, loss_ce: 0.017030\n","iteration 9188 : loss : 0.105375, loss_ce: 0.012673\n","iteration 9189 : loss : 0.057301, loss_ce: 0.010496\n","iteration 9190 : loss : 0.044977, loss_ce: 0.013972\n","iteration 9191 : loss : 0.046839, loss_ce: 0.012524\n","iteration 9192 : loss : 0.043790, loss_ce: 0.008696\n","iteration 9193 : loss : 0.047937, loss_ce: 0.014726\n","iteration 9194 : loss : 0.053690, loss_ce: 0.021284\n","iteration 9195 : loss : 0.049454, loss_ce: 0.018790\n","iteration 9196 : loss : 0.051679, loss_ce: 0.014442\n","iteration 9197 : loss : 0.061884, loss_ce: 0.015079\n","iteration 9198 : loss : 0.099858, loss_ce: 0.007624\n","iteration 9199 : loss : 0.111028, loss_ce: 0.014240\n","iteration 9200 : loss : 0.056914, loss_ce: 0.021920\n","iteration 9201 : loss : 0.057539, loss_ce: 0.011843\n","iteration 9202 : loss : 0.079531, loss_ce: 0.013851\n","iteration 9203 : loss : 0.058468, loss_ce: 0.013765\n","iteration 9204 : loss : 0.053025, loss_ce: 0.016630\n","iteration 9205 : loss : 0.042659, loss_ce: 0.016084\n","iteration 9206 : loss : 0.047042, loss_ce: 0.010198\n","iteration 9207 : loss : 0.160777, loss_ce: 0.000064\n"," 66%|███████████████████▊          | 99/150 [1:56:35<59:40, 70.20s/it]iteration 9208 : loss : 0.041902, loss_ce: 0.010883\n","iteration 9209 : loss : 0.052490, loss_ce: 0.022794\n","iteration 9210 : loss : 0.046139, loss_ce: 0.017842\n","iteration 9211 : loss : 0.040299, loss_ce: 0.007198\n","iteration 9212 : loss : 0.106809, loss_ce: 0.013863\n","iteration 9213 : loss : 0.048360, loss_ce: 0.013579\n","iteration 9214 : loss : 0.051847, loss_ce: 0.015254\n","iteration 9215 : loss : 0.054439, loss_ce: 0.020967\n","iteration 9216 : loss : 0.040822, loss_ce: 0.014918\n","iteration 9217 : loss : 0.039464, loss_ce: 0.012015\n","iteration 9218 : loss : 0.042824, loss_ce: 0.016816\n","iteration 9219 : loss : 0.051381, loss_ce: 0.021940\n","iteration 9220 : loss : 0.047466, loss_ce: 0.017624\n","iteration 9221 : loss : 0.048205, loss_ce: 0.018250\n","iteration 9222 : loss : 0.060978, loss_ce: 0.015884\n","iteration 9223 : loss : 0.069485, loss_ce: 0.015102\n","iteration 9224 : loss : 0.041039, loss_ce: 0.017649\n","iteration 9225 : loss : 0.048877, loss_ce: 0.016085\n","iteration 9226 : loss : 0.040975, loss_ce: 0.012519\n","iteration 9227 : loss : 0.045388, loss_ce: 0.015487\n","iteration 9228 : loss : 0.045157, loss_ce: 0.015541\n","iteration 9229 : loss : 0.047715, loss_ce: 0.016269\n","iteration 9230 : loss : 0.049804, loss_ce: 0.020311\n","iteration 9231 : loss : 0.046939, loss_ce: 0.012953\n","iteration 9232 : loss : 0.167241, loss_ce: 0.004514\n","iteration 9233 : loss : 0.046720, loss_ce: 0.014724\n","iteration 9234 : loss : 0.050541, loss_ce: 0.009176\n","iteration 9235 : loss : 0.036266, loss_ce: 0.011755\n","iteration 9236 : loss : 0.045053, loss_ce: 0.016422\n","iteration 9237 : loss : 0.055742, loss_ce: 0.025972\n","iteration 9238 : loss : 0.048894, loss_ce: 0.015783\n","iteration 9239 : loss : 0.109420, loss_ce: 0.011939\n","iteration 9240 : loss : 0.048538, loss_ce: 0.011529\n","iteration 9241 : loss : 0.049799, loss_ce: 0.014075\n","iteration 9242 : loss : 0.058575, loss_ce: 0.020631\n","iteration 9243 : loss : 0.052894, loss_ce: 0.011775\n","iteration 9244 : loss : 0.113121, loss_ce: 0.016255\n","iteration 9245 : loss : 0.056376, loss_ce: 0.014834\n","iteration 9246 : loss : 0.103302, loss_ce: 0.008624\n","iteration 9247 : loss : 0.040897, loss_ce: 0.014762\n","iteration 9248 : loss : 0.045915, loss_ce: 0.011590\n","iteration 9249 : loss : 0.102238, loss_ce: 0.015099\n","iteration 9250 : loss : 0.054316, loss_ce: 0.015512\n","iteration 9251 : loss : 0.064252, loss_ce: 0.012952\n","iteration 9252 : loss : 0.068627, loss_ce: 0.011850\n","iteration 9253 : loss : 0.038211, loss_ce: 0.013418\n","iteration 9254 : loss : 0.176249, loss_ce: 0.006037\n","iteration 9255 : loss : 0.040529, loss_ce: 0.010537\n","iteration 9256 : loss : 0.046492, loss_ce: 0.017392\n","iteration 9257 : loss : 0.047944, loss_ce: 0.022034\n","iteration 9258 : loss : 0.041259, loss_ce: 0.015904\n","iteration 9259 : loss : 0.107541, loss_ce: 0.009810\n","iteration 9260 : loss : 0.062177, loss_ce: 0.013479\n","iteration 9261 : loss : 0.036919, loss_ce: 0.013609\n","iteration 9262 : loss : 0.046667, loss_ce: 0.018728\n","iteration 9263 : loss : 0.050375, loss_ce: 0.019134\n","iteration 9264 : loss : 0.057360, loss_ce: 0.015646\n","iteration 9265 : loss : 0.039034, loss_ce: 0.012257\n","iteration 9266 : loss : 0.045496, loss_ce: 0.011919\n","iteration 9267 : loss : 0.047625, loss_ce: 0.014583\n","iteration 9268 : loss : 0.047486, loss_ce: 0.010164\n","iteration 9269 : loss : 0.038306, loss_ce: 0.011113\n","iteration 9270 : loss : 0.055510, loss_ce: 0.013298\n","iteration 9271 : loss : 0.055541, loss_ce: 0.011598\n","iteration 9272 : loss : 0.042505, loss_ce: 0.014266\n","iteration 9273 : loss : 0.041671, loss_ce: 0.016441\n","iteration 9274 : loss : 0.050060, loss_ce: 0.019555\n","iteration 9275 : loss : 0.044907, loss_ce: 0.011353\n","iteration 9276 : loss : 0.049697, loss_ce: 0.016852\n","iteration 9277 : loss : 0.045109, loss_ce: 0.017415\n","iteration 9278 : loss : 0.042261, loss_ce: 0.017289\n","iteration 9279 : loss : 0.037909, loss_ce: 0.015717\n","iteration 9280 : loss : 0.054193, loss_ce: 0.012267\n","iteration 9281 : loss : 0.045985, loss_ce: 0.017819\n","iteration 9282 : loss : 0.041260, loss_ce: 0.011501\n","iteration 9283 : loss : 0.049537, loss_ce: 0.012987\n","iteration 9284 : loss : 0.048160, loss_ce: 0.021657\n","iteration 9285 : loss : 0.115232, loss_ce: 0.014983\n","iteration 9286 : loss : 0.051261, loss_ce: 0.012508\n","iteration 9287 : loss : 0.038804, loss_ce: 0.013322\n","iteration 9288 : loss : 0.042198, loss_ce: 0.014636\n","iteration 9289 : loss : 0.038552, loss_ce: 0.006014\n","iteration 9290 : loss : 0.038571, loss_ce: 0.012118\n","iteration 9291 : loss : 0.047760, loss_ce: 0.009305\n","iteration 9292 : loss : 0.070131, loss_ce: 0.009482\n","iteration 9293 : loss : 0.043979, loss_ce: 0.015135\n","iteration 9294 : loss : 0.044388, loss_ce: 0.014959\n","iteration 9295 : loss : 0.037215, loss_ce: 0.015881\n","iteration 9296 : loss : 0.049561, loss_ce: 0.019940\n","iteration 9297 : loss : 0.036837, loss_ce: 0.018392\n","iteration 9298 : loss : 0.042295, loss_ce: 0.017672\n","iteration 9299 : loss : 0.045734, loss_ce: 0.014335\n","iteration 9300 : loss : 0.342715, loss_ce: 0.002787\n","save model to output/epoch_99.pth\n"," 67%|███████████████████▎         | 100/150 [1:57:46<58:38, 70.37s/it]iteration 9301 : loss : 0.044368, loss_ce: 0.012727\n","iteration 9302 : loss : 0.040784, loss_ce: 0.012795\n","iteration 9303 : loss : 0.052491, loss_ce: 0.018169\n","iteration 9304 : loss : 0.052688, loss_ce: 0.012586\n","iteration 9305 : loss : 0.049829, loss_ce: 0.023827\n","iteration 9306 : loss : 0.047861, loss_ce: 0.016629\n","iteration 9307 : loss : 0.051850, loss_ce: 0.011615\n","iteration 9308 : loss : 0.044808, loss_ce: 0.010194\n","iteration 9309 : loss : 0.063404, loss_ce: 0.020095\n","iteration 9310 : loss : 0.045988, loss_ce: 0.018335\n","iteration 9311 : loss : 0.091909, loss_ce: 0.008629\n","iteration 9312 : loss : 0.041579, loss_ce: 0.014926\n","iteration 9313 : loss : 0.052182, loss_ce: 0.010045\n","iteration 9314 : loss : 0.047367, loss_ce: 0.014481\n","iteration 9315 : loss : 0.111479, loss_ce: 0.016253\n","iteration 9316 : loss : 0.044027, loss_ce: 0.017151\n","iteration 9317 : loss : 0.042107, loss_ce: 0.009508\n","iteration 9318 : loss : 0.043216, loss_ce: 0.013877\n","iteration 9319 : loss : 0.043849, loss_ce: 0.012727\n","iteration 9320 : loss : 0.046749, loss_ce: 0.013630\n","iteration 9321 : loss : 0.048852, loss_ce: 0.014339\n","iteration 9322 : loss : 0.041416, loss_ce: 0.016670\n","iteration 9323 : loss : 0.044517, loss_ce: 0.014487\n","iteration 9324 : loss : 0.041365, loss_ce: 0.015496\n","iteration 9325 : loss : 0.036434, loss_ce: 0.012915\n","iteration 9326 : loss : 0.038944, loss_ce: 0.014000\n","iteration 9327 : loss : 0.047543, loss_ce: 0.023741\n","iteration 9328 : loss : 0.054365, loss_ce: 0.011961\n","iteration 9329 : loss : 0.104049, loss_ce: 0.008204\n","iteration 9330 : loss : 0.044717, loss_ce: 0.011174\n","iteration 9331 : loss : 0.037398, loss_ce: 0.006609\n","iteration 9332 : loss : 0.039984, loss_ce: 0.011736\n","iteration 9333 : loss : 0.046037, loss_ce: 0.017529\n","iteration 9334 : loss : 0.050739, loss_ce: 0.018339\n","iteration 9335 : loss : 0.036365, loss_ce: 0.012796\n","iteration 9336 : loss : 0.051827, loss_ce: 0.011141\n","iteration 9337 : loss : 0.048009, loss_ce: 0.019322\n","iteration 9338 : loss : 0.102154, loss_ce: 0.007787\n","iteration 9339 : loss : 0.038309, loss_ce: 0.013064\n","iteration 9340 : loss : 0.041857, loss_ce: 0.012135\n","iteration 9341 : loss : 0.052486, loss_ce: 0.011274\n","iteration 9342 : loss : 0.038409, loss_ce: 0.014213\n","iteration 9343 : loss : 0.051523, loss_ce: 0.013223\n","iteration 9344 : loss : 0.037535, loss_ce: 0.013214\n","iteration 9345 : loss : 0.055105, loss_ce: 0.015454\n","iteration 9346 : loss : 0.051845, loss_ce: 0.010232\n","iteration 9347 : loss : 0.042820, loss_ce: 0.018136\n","iteration 9348 : loss : 0.036250, loss_ce: 0.013286\n","iteration 9349 : loss : 0.040617, loss_ce: 0.011506\n","iteration 9350 : loss : 0.046865, loss_ce: 0.017642\n","iteration 9351 : loss : 0.039621, loss_ce: 0.008528\n","iteration 9352 : loss : 0.041734, loss_ce: 0.015602\n","iteration 9353 : loss : 0.049399, loss_ce: 0.012286\n","iteration 9354 : loss : 0.045332, loss_ce: 0.016075\n","iteration 9355 : loss : 0.044508, loss_ce: 0.012719\n","iteration 9356 : loss : 0.052673, loss_ce: 0.028652\n","iteration 9357 : loss : 0.034009, loss_ce: 0.009336\n","iteration 9358 : loss : 0.043692, loss_ce: 0.016486\n","iteration 9359 : loss : 0.048451, loss_ce: 0.014377\n","iteration 9360 : loss : 0.039989, loss_ce: 0.014103\n","iteration 9361 : loss : 0.042959, loss_ce: 0.012612\n","iteration 9362 : loss : 0.040107, loss_ce: 0.016768\n","iteration 9363 : loss : 0.041739, loss_ce: 0.015502\n","iteration 9364 : loss : 0.049857, loss_ce: 0.013485\n","iteration 9365 : loss : 0.049596, loss_ce: 0.013539\n","iteration 9366 : loss : 0.050264, loss_ce: 0.013739\n","iteration 9367 : loss : 0.039423, loss_ce: 0.009475\n","iteration 9368 : loss : 0.099112, loss_ce: 0.008963\n","iteration 9369 : loss : 0.048356, loss_ce: 0.014752\n","iteration 9370 : loss : 0.039667, loss_ce: 0.013733\n","iteration 9371 : loss : 0.121044, loss_ce: 0.007762\n","iteration 9372 : loss : 0.037080, loss_ce: 0.012995\n","iteration 9373 : loss : 0.046519, loss_ce: 0.017103\n","iteration 9374 : loss : 0.051833, loss_ce: 0.013987\n","iteration 9375 : loss : 0.033661, loss_ce: 0.007840\n","iteration 9376 : loss : 0.045227, loss_ce: 0.014094\n","iteration 9377 : loss : 0.039390, loss_ce: 0.015281\n","iteration 9378 : loss : 0.038665, loss_ce: 0.013882\n","iteration 9379 : loss : 0.055465, loss_ce: 0.018830\n","iteration 9380 : loss : 0.048113, loss_ce: 0.013446\n","iteration 9381 : loss : 0.042070, loss_ce: 0.019079\n","iteration 9382 : loss : 0.052330, loss_ce: 0.017183\n","iteration 9383 : loss : 0.049632, loss_ce: 0.007968\n","iteration 9384 : loss : 0.157439, loss_ce: 0.009333\n","iteration 9385 : loss : 0.056126, loss_ce: 0.014797\n","iteration 9386 : loss : 0.038844, loss_ce: 0.011225\n","iteration 9387 : loss : 0.046163, loss_ce: 0.017530\n","iteration 9388 : loss : 0.051222, loss_ce: 0.012631\n","iteration 9389 : loss : 0.041968, loss_ce: 0.014703\n","iteration 9390 : loss : 0.043347, loss_ce: 0.008687\n","iteration 9391 : loss : 0.103442, loss_ce: 0.013349\n","iteration 9392 : loss : 0.085868, loss_ce: 0.010462\n","iteration 9393 : loss : 0.231893, loss_ce: 0.033265\n"," 67%|███████████████████▌         | 101/150 [1:58:56<57:34, 70.50s/it]iteration 9394 : loss : 0.049197, loss_ce: 0.012601\n","iteration 9395 : loss : 0.038315, loss_ce: 0.012908\n","iteration 9396 : loss : 0.042122, loss_ce: 0.009214\n","iteration 9397 : loss : 0.038184, loss_ce: 0.010116\n","iteration 9398 : loss : 0.047922, loss_ce: 0.014274\n","iteration 9399 : loss : 0.042121, loss_ce: 0.010698\n","iteration 9400 : loss : 0.045189, loss_ce: 0.013658\n","iteration 9401 : loss : 0.046327, loss_ce: 0.014740\n","iteration 9402 : loss : 0.046756, loss_ce: 0.015496\n","iteration 9403 : loss : 0.095743, loss_ce: 0.009438\n","iteration 9404 : loss : 0.100164, loss_ce: 0.018015\n","iteration 9405 : loss : 0.046876, loss_ce: 0.008141\n","iteration 9406 : loss : 0.049131, loss_ce: 0.015527\n","iteration 9407 : loss : 0.043146, loss_ce: 0.017243\n","iteration 9408 : loss : 0.040117, loss_ce: 0.011557\n","iteration 9409 : loss : 0.052255, loss_ce: 0.017124\n","iteration 9410 : loss : 0.048258, loss_ce: 0.012787\n","iteration 9411 : loss : 0.038658, loss_ce: 0.011868\n","iteration 9412 : loss : 0.066938, loss_ce: 0.014813\n","iteration 9413 : loss : 0.094838, loss_ce: 0.005493\n","iteration 9414 : loss : 0.039963, loss_ce: 0.010834\n","iteration 9415 : loss : 0.033210, loss_ce: 0.012435\n","iteration 9416 : loss : 0.043637, loss_ce: 0.013274\n","iteration 9417 : loss : 0.041833, loss_ce: 0.013274\n","iteration 9418 : loss : 0.051649, loss_ce: 0.015052\n","iteration 9419 : loss : 0.043259, loss_ce: 0.012639\n","iteration 9420 : loss : 0.039092, loss_ce: 0.012557\n","iteration 9421 : loss : 0.052676, loss_ce: 0.015896\n","iteration 9422 : loss : 0.056504, loss_ce: 0.007687\n","iteration 9423 : loss : 0.046844, loss_ce: 0.012547\n","iteration 9424 : loss : 0.052259, loss_ce: 0.015781\n","iteration 9425 : loss : 0.040833, loss_ce: 0.018938\n","iteration 9426 : loss : 0.052565, loss_ce: 0.014994\n","iteration 9427 : loss : 0.039279, loss_ce: 0.012565\n","iteration 9428 : loss : 0.039576, loss_ce: 0.014282\n","iteration 9429 : loss : 0.057444, loss_ce: 0.021184\n","iteration 9430 : loss : 0.043788, loss_ce: 0.012569\n","iteration 9431 : loss : 0.055391, loss_ce: 0.021101\n","iteration 9432 : loss : 0.041034, loss_ce: 0.013005\n","iteration 9433 : loss : 0.039882, loss_ce: 0.012777\n","iteration 9434 : loss : 0.038869, loss_ce: 0.010967\n","iteration 9435 : loss : 0.041407, loss_ce: 0.013637\n","iteration 9436 : loss : 0.037910, loss_ce: 0.008962\n","iteration 9437 : loss : 0.044224, loss_ce: 0.016256\n","iteration 9438 : loss : 0.108772, loss_ce: 0.011717\n","iteration 9439 : loss : 0.045707, loss_ce: 0.017453\n","iteration 9440 : loss : 0.049516, loss_ce: 0.012740\n","iteration 9441 : loss : 0.049771, loss_ce: 0.008401\n","iteration 9442 : loss : 0.041444, loss_ce: 0.010104\n","iteration 9443 : loss : 0.044639, loss_ce: 0.018773\n","iteration 9444 : loss : 0.048941, loss_ce: 0.008145\n","iteration 9445 : loss : 0.042501, loss_ce: 0.008971\n","iteration 9446 : loss : 0.066291, loss_ce: 0.011173\n","iteration 9447 : loss : 0.033482, loss_ce: 0.010939\n","iteration 9448 : loss : 0.032945, loss_ce: 0.010968\n","iteration 9449 : loss : 0.043839, loss_ce: 0.015783\n","iteration 9450 : loss : 0.047750, loss_ce: 0.017631\n","iteration 9451 : loss : 0.049117, loss_ce: 0.009229\n","iteration 9452 : loss : 0.047671, loss_ce: 0.011122\n","iteration 9453 : loss : 0.042542, loss_ce: 0.015080\n","iteration 9454 : loss : 0.057498, loss_ce: 0.013544\n","iteration 9455 : loss : 0.036015, loss_ce: 0.012383\n","iteration 9456 : loss : 0.056718, loss_ce: 0.025870\n","iteration 9457 : loss : 0.108403, loss_ce: 0.014512\n","iteration 9458 : loss : 0.046848, loss_ce: 0.019267\n","iteration 9459 : loss : 0.043554, loss_ce: 0.014670\n","iteration 9460 : loss : 0.044765, loss_ce: 0.020506\n","iteration 9461 : loss : 0.044878, loss_ce: 0.014443\n","iteration 9462 : loss : 0.053850, loss_ce: 0.016352\n","iteration 9463 : loss : 0.158486, loss_ce: 0.008949\n","iteration 9464 : loss : 0.043438, loss_ce: 0.011876\n","iteration 9465 : loss : 0.033118, loss_ce: 0.009656\n","iteration 9466 : loss : 0.102328, loss_ce: 0.004417\n","iteration 9467 : loss : 0.053680, loss_ce: 0.011257\n","iteration 9468 : loss : 0.075036, loss_ce: 0.009667\n","iteration 9469 : loss : 0.102887, loss_ce: 0.010701\n","iteration 9470 : loss : 0.035417, loss_ce: 0.013132\n","iteration 9471 : loss : 0.044146, loss_ce: 0.014235\n","iteration 9472 : loss : 0.042034, loss_ce: 0.015233\n","iteration 9473 : loss : 0.040993, loss_ce: 0.016363\n","iteration 9474 : loss : 0.052952, loss_ce: 0.017674\n","iteration 9475 : loss : 0.045372, loss_ce: 0.012400\n","iteration 9476 : loss : 0.099708, loss_ce: 0.016275\n","iteration 9477 : loss : 0.042610, loss_ce: 0.014565\n","iteration 9478 : loss : 0.051157, loss_ce: 0.017028\n","iteration 9479 : loss : 0.047102, loss_ce: 0.012530\n","iteration 9480 : loss : 0.046961, loss_ce: 0.014748\n","iteration 9481 : loss : 0.043557, loss_ce: 0.014831\n","iteration 9482 : loss : 0.044919, loss_ce: 0.012163\n","iteration 9483 : loss : 0.084842, loss_ce: 0.018040\n","iteration 9484 : loss : 0.048114, loss_ce: 0.018370\n","iteration 9485 : loss : 0.048459, loss_ce: 0.010857\n","iteration 9486 : loss : 0.197635, loss_ce: 0.003491\n"," 68%|███████████████████▋         | 102/150 [2:00:07<56:20, 70.43s/it]iteration 9487 : loss : 0.053788, loss_ce: 0.010370\n","iteration 9488 : loss : 0.054161, loss_ce: 0.012166\n","iteration 9489 : loss : 0.066130, loss_ce: 0.012337\n","iteration 9490 : loss : 0.063213, loss_ce: 0.024345\n","iteration 9491 : loss : 0.064243, loss_ce: 0.020874\n","iteration 9492 : loss : 0.064944, loss_ce: 0.019393\n","iteration 9493 : loss : 0.055228, loss_ce: 0.014881\n","iteration 9494 : loss : 0.051391, loss_ce: 0.020110\n","iteration 9495 : loss : 0.050390, loss_ce: 0.019756\n","iteration 9496 : loss : 0.044582, loss_ce: 0.019097\n","iteration 9497 : loss : 0.064709, loss_ce: 0.018641\n","iteration 9498 : loss : 0.058473, loss_ce: 0.018638\n","iteration 9499 : loss : 0.051764, loss_ce: 0.022027\n","iteration 9500 : loss : 0.053377, loss_ce: 0.017303\n","iteration 9501 : loss : 0.058968, loss_ce: 0.016445\n","iteration 9502 : loss : 0.058388, loss_ce: 0.017811\n","iteration 9503 : loss : 0.047716, loss_ce: 0.015207\n","iteration 9504 : loss : 0.046538, loss_ce: 0.015020\n","iteration 9505 : loss : 0.057001, loss_ce: 0.017828\n","iteration 9506 : loss : 0.062710, loss_ce: 0.016084\n","iteration 9507 : loss : 0.052721, loss_ce: 0.014599\n","iteration 9508 : loss : 0.048191, loss_ce: 0.026085\n","iteration 9509 : loss : 0.061000, loss_ce: 0.012194\n","iteration 9510 : loss : 0.046368, loss_ce: 0.016102\n","iteration 9511 : loss : 0.048731, loss_ce: 0.022018\n","iteration 9512 : loss : 0.045396, loss_ce: 0.014825\n","iteration 9513 : loss : 0.048931, loss_ce: 0.013846\n","iteration 9514 : loss : 0.046099, loss_ce: 0.016207\n","iteration 9515 : loss : 0.045740, loss_ce: 0.017858\n","iteration 9516 : loss : 0.054610, loss_ce: 0.025222\n","iteration 9517 : loss : 0.056567, loss_ce: 0.014605\n","iteration 9518 : loss : 0.042854, loss_ce: 0.017231\n","iteration 9519 : loss : 0.051800, loss_ce: 0.013631\n","iteration 9520 : loss : 0.053891, loss_ce: 0.011669\n","iteration 9521 : loss : 0.113956, loss_ce: 0.008133\n","iteration 9522 : loss : 0.047808, loss_ce: 0.013133\n","iteration 9523 : loss : 0.047551, loss_ce: 0.017150\n","iteration 9524 : loss : 0.058146, loss_ce: 0.019247\n","iteration 9525 : loss : 0.043956, loss_ce: 0.011883\n","iteration 9526 : loss : 0.048477, loss_ce: 0.015966\n","iteration 9527 : loss : 0.041133, loss_ce: 0.016975\n","iteration 9528 : loss : 0.045088, loss_ce: 0.018161\n","iteration 9529 : loss : 0.050023, loss_ce: 0.014735\n","iteration 9530 : loss : 0.039273, loss_ce: 0.018594\n","iteration 9531 : loss : 0.045885, loss_ce: 0.014491\n","iteration 9532 : loss : 0.049229, loss_ce: 0.018530\n","iteration 9533 : loss : 0.041688, loss_ce: 0.014625\n","iteration 9534 : loss : 0.042676, loss_ce: 0.006326\n","iteration 9535 : loss : 0.040676, loss_ce: 0.016619\n","iteration 9536 : loss : 0.049814, loss_ce: 0.010352\n","iteration 9537 : loss : 0.080102, loss_ce: 0.009069\n","iteration 9538 : loss : 0.050217, loss_ce: 0.016860\n","iteration 9539 : loss : 0.066471, loss_ce: 0.018173\n","iteration 9540 : loss : 0.043549, loss_ce: 0.013958\n","iteration 9541 : loss : 0.043521, loss_ce: 0.018406\n","iteration 9542 : loss : 0.040266, loss_ce: 0.009822\n","iteration 9543 : loss : 0.050499, loss_ce: 0.013911\n","iteration 9544 : loss : 0.052458, loss_ce: 0.012434\n","iteration 9545 : loss : 0.071051, loss_ce: 0.012722\n","iteration 9546 : loss : 0.042508, loss_ce: 0.014380\n","iteration 9547 : loss : 0.035493, loss_ce: 0.006240\n","iteration 9548 : loss : 0.057773, loss_ce: 0.013579\n","iteration 9549 : loss : 0.049155, loss_ce: 0.018772\n","iteration 9550 : loss : 0.043767, loss_ce: 0.016527\n","iteration 9551 : loss : 0.106174, loss_ce: 0.008390\n","iteration 9552 : loss : 0.048976, loss_ce: 0.011099\n","iteration 9553 : loss : 0.044543, loss_ce: 0.015230\n","iteration 9554 : loss : 0.055181, loss_ce: 0.013743\n","iteration 9555 : loss : 0.048095, loss_ce: 0.014227\n","iteration 9556 : loss : 0.108136, loss_ce: 0.010927\n","iteration 9557 : loss : 0.039131, loss_ce: 0.013565\n","iteration 9558 : loss : 0.050916, loss_ce: 0.015642\n","iteration 9559 : loss : 0.044806, loss_ce: 0.015098\n","iteration 9560 : loss : 0.049721, loss_ce: 0.016741\n","iteration 9561 : loss : 0.054032, loss_ce: 0.010973\n","iteration 9562 : loss : 0.049817, loss_ce: 0.015693\n","iteration 9563 : loss : 0.041737, loss_ce: 0.011423\n","iteration 9564 : loss : 0.054523, loss_ce: 0.015472\n","iteration 9565 : loss : 0.046370, loss_ce: 0.011170\n","iteration 9566 : loss : 0.043562, loss_ce: 0.012287\n","iteration 9567 : loss : 0.041337, loss_ce: 0.015328\n","iteration 9568 : loss : 0.038549, loss_ce: 0.012384\n","iteration 9569 : loss : 0.052555, loss_ce: 0.018959\n","iteration 9570 : loss : 0.046039, loss_ce: 0.022069\n","iteration 9571 : loss : 0.105257, loss_ce: 0.014230\n","iteration 9572 : loss : 0.051153, loss_ce: 0.011703\n","iteration 9573 : loss : 0.042808, loss_ce: 0.021427\n","iteration 9574 : loss : 0.047798, loss_ce: 0.005142\n","iteration 9575 : loss : 0.043416, loss_ce: 0.013575\n","iteration 9576 : loss : 0.045215, loss_ce: 0.012430\n","iteration 9577 : loss : 0.049963, loss_ce: 0.014813\n","iteration 9578 : loss : 0.067555, loss_ce: 0.015060\n","iteration 9579 : loss : 0.242993, loss_ce: 0.018977\n"," 69%|███████████████████▉         | 103/150 [2:01:17<55:04, 70.30s/it]iteration 9580 : loss : 0.102451, loss_ce: 0.013813\n","iteration 9581 : loss : 0.043253, loss_ce: 0.013597\n","iteration 9582 : loss : 0.100549, loss_ce: 0.009716\n","iteration 9583 : loss : 0.061919, loss_ce: 0.014014\n","iteration 9584 : loss : 0.043702, loss_ce: 0.017432\n","iteration 9585 : loss : 0.046287, loss_ce: 0.015924\n","iteration 9586 : loss : 0.067532, loss_ce: 0.012633\n","iteration 9587 : loss : 0.050071, loss_ce: 0.009548\n","iteration 9588 : loss : 0.038453, loss_ce: 0.010754\n","iteration 9589 : loss : 0.054454, loss_ce: 0.019937\n","iteration 9590 : loss : 0.046795, loss_ce: 0.014947\n","iteration 9591 : loss : 0.041002, loss_ce: 0.010318\n","iteration 9592 : loss : 0.054034, loss_ce: 0.015267\n","iteration 9593 : loss : 0.102779, loss_ce: 0.009879\n","iteration 9594 : loss : 0.083564, loss_ce: 0.011706\n","iteration 9595 : loss : 0.053373, loss_ce: 0.008201\n","iteration 9596 : loss : 0.044219, loss_ce: 0.012864\n","iteration 9597 : loss : 0.042186, loss_ce: 0.015989\n","iteration 9598 : loss : 0.041423, loss_ce: 0.012247\n","iteration 9599 : loss : 0.062987, loss_ce: 0.016459\n","iteration 9600 : loss : 0.044746, loss_ce: 0.012086\n","iteration 9601 : loss : 0.041725, loss_ce: 0.018515\n","iteration 9602 : loss : 0.041063, loss_ce: 0.013639\n","iteration 9603 : loss : 0.041977, loss_ce: 0.017628\n","iteration 9604 : loss : 0.037919, loss_ce: 0.009063\n","iteration 9605 : loss : 0.098080, loss_ce: 0.006553\n","iteration 9606 : loss : 0.048030, loss_ce: 0.009454\n","iteration 9607 : loss : 0.046354, loss_ce: 0.013354\n","iteration 9608 : loss : 0.048780, loss_ce: 0.011059\n","iteration 9609 : loss : 0.042205, loss_ce: 0.010368\n","iteration 9610 : loss : 0.039785, loss_ce: 0.015087\n","iteration 9611 : loss : 0.051419, loss_ce: 0.014706\n","iteration 9612 : loss : 0.038894, loss_ce: 0.017769\n","iteration 9613 : loss : 0.050784, loss_ce: 0.017150\n","iteration 9614 : loss : 0.042175, loss_ce: 0.016998\n","iteration 9615 : loss : 0.060616, loss_ce: 0.012132\n","iteration 9616 : loss : 0.057016, loss_ce: 0.011144\n","iteration 9617 : loss : 0.061983, loss_ce: 0.021393\n","iteration 9618 : loss : 0.045252, loss_ce: 0.011128\n","iteration 9619 : loss : 0.051773, loss_ce: 0.019886\n","iteration 9620 : loss : 0.050372, loss_ce: 0.018769\n","iteration 9621 : loss : 0.043105, loss_ce: 0.014186\n","iteration 9622 : loss : 0.048650, loss_ce: 0.011031\n","iteration 9623 : loss : 0.058731, loss_ce: 0.027633\n","iteration 9624 : loss : 0.111745, loss_ce: 0.010458\n","iteration 9625 : loss : 0.053913, loss_ce: 0.013566\n","iteration 9626 : loss : 0.057168, loss_ce: 0.012220\n","iteration 9627 : loss : 0.040184, loss_ce: 0.009533\n","iteration 9628 : loss : 0.057769, loss_ce: 0.012302\n","iteration 9629 : loss : 0.042704, loss_ce: 0.020651\n","iteration 9630 : loss : 0.050607, loss_ce: 0.010086\n","iteration 9631 : loss : 0.041011, loss_ce: 0.012337\n","iteration 9632 : loss : 0.068187, loss_ce: 0.028088\n","iteration 9633 : loss : 0.040211, loss_ce: 0.015261\n","iteration 9634 : loss : 0.102652, loss_ce: 0.010096\n","iteration 9635 : loss : 0.052540, loss_ce: 0.021371\n","iteration 9636 : loss : 0.039513, loss_ce: 0.011260\n","iteration 9637 : loss : 0.042872, loss_ce: 0.016397\n","iteration 9638 : loss : 0.084477, loss_ce: 0.020434\n","iteration 9639 : loss : 0.043567, loss_ce: 0.013248\n","iteration 9640 : loss : 0.040555, loss_ce: 0.016831\n","iteration 9641 : loss : 0.054769, loss_ce: 0.011686\n","iteration 9642 : loss : 0.050204, loss_ce: 0.017255\n","iteration 9643 : loss : 0.104673, loss_ce: 0.017095\n","iteration 9644 : loss : 0.048014, loss_ce: 0.013109\n","iteration 9645 : loss : 0.048741, loss_ce: 0.022861\n","iteration 9646 : loss : 0.049579, loss_ce: 0.010995\n","iteration 9647 : loss : 0.055383, loss_ce: 0.018519\n","iteration 9648 : loss : 0.099874, loss_ce: 0.010895\n","iteration 9649 : loss : 0.088076, loss_ce: 0.012960\n","iteration 9650 : loss : 0.042296, loss_ce: 0.017395\n","iteration 9651 : loss : 0.038226, loss_ce: 0.008581\n","iteration 9652 : loss : 0.045572, loss_ce: 0.011908\n","iteration 9653 : loss : 0.044975, loss_ce: 0.013738\n","iteration 9654 : loss : 0.050594, loss_ce: 0.015236\n","iteration 9655 : loss : 0.052087, loss_ce: 0.023154\n","iteration 9656 : loss : 0.052824, loss_ce: 0.014842\n","iteration 9657 : loss : 0.038746, loss_ce: 0.007953\n","iteration 9658 : loss : 0.044104, loss_ce: 0.020450\n","iteration 9659 : loss : 0.050741, loss_ce: 0.011488\n","iteration 9660 : loss : 0.049449, loss_ce: 0.017211\n","iteration 9661 : loss : 0.052822, loss_ce: 0.010482\n","iteration 9662 : loss : 0.048589, loss_ce: 0.017714\n","iteration 9663 : loss : 0.032615, loss_ce: 0.009951\n","iteration 9664 : loss : 0.044368, loss_ce: 0.016976\n","iteration 9665 : loss : 0.046010, loss_ce: 0.014581\n","iteration 9666 : loss : 0.049544, loss_ce: 0.015499\n","iteration 9667 : loss : 0.046718, loss_ce: 0.017461\n","iteration 9668 : loss : 0.045836, loss_ce: 0.018631\n","iteration 9669 : loss : 0.054858, loss_ce: 0.013640\n","iteration 9670 : loss : 0.044266, loss_ce: 0.009554\n","iteration 9671 : loss : 0.081153, loss_ce: 0.016811\n","iteration 9672 : loss : 0.094228, loss_ce: 0.000020\n"," 69%|████████████████████         | 104/150 [2:02:27<53:54, 70.32s/it]iteration 9673 : loss : 0.035713, loss_ce: 0.013497\n","iteration 9674 : loss : 0.055846, loss_ce: 0.014803\n","iteration 9675 : loss : 0.040829, loss_ce: 0.013004\n","iteration 9676 : loss : 0.043885, loss_ce: 0.014108\n","iteration 9677 : loss : 0.047643, loss_ce: 0.016737\n","iteration 9678 : loss : 0.049198, loss_ce: 0.022615\n","iteration 9679 : loss : 0.042982, loss_ce: 0.014738\n","iteration 9680 : loss : 0.046831, loss_ce: 0.011238\n","iteration 9681 : loss : 0.046535, loss_ce: 0.029233\n","iteration 9682 : loss : 0.046719, loss_ce: 0.011121\n","iteration 9683 : loss : 0.037469, loss_ce: 0.014500\n","iteration 9684 : loss : 0.049287, loss_ce: 0.019229\n","iteration 9685 : loss : 0.050410, loss_ce: 0.015187\n","iteration 9686 : loss : 0.043797, loss_ce: 0.006398\n","iteration 9687 : loss : 0.108059, loss_ce: 0.013721\n","iteration 9688 : loss : 0.032274, loss_ce: 0.007364\n","iteration 9689 : loss : 0.052801, loss_ce: 0.011239\n","iteration 9690 : loss : 0.053532, loss_ce: 0.017420\n","iteration 9691 : loss : 0.042165, loss_ce: 0.008263\n","iteration 9692 : loss : 0.052174, loss_ce: 0.016753\n","iteration 9693 : loss : 0.047788, loss_ce: 0.020082\n","iteration 9694 : loss : 0.035419, loss_ce: 0.011899\n","iteration 9695 : loss : 0.041255, loss_ce: 0.013161\n","iteration 9696 : loss : 0.046117, loss_ce: 0.021421\n","iteration 9697 : loss : 0.045125, loss_ce: 0.014755\n","iteration 9698 : loss : 0.053468, loss_ce: 0.024669\n","iteration 9699 : loss : 0.039023, loss_ce: 0.012628\n","iteration 9700 : loss : 0.048351, loss_ce: 0.018596\n","iteration 9701 : loss : 0.040826, loss_ce: 0.009739\n","iteration 9702 : loss : 0.040454, loss_ce: 0.012709\n","iteration 9703 : loss : 0.094376, loss_ce: 0.008301\n","iteration 9704 : loss : 0.043016, loss_ce: 0.012168\n","iteration 9705 : loss : 0.100173, loss_ce: 0.011094\n","iteration 9706 : loss : 0.048741, loss_ce: 0.011418\n","iteration 9707 : loss : 0.041206, loss_ce: 0.011606\n","iteration 9708 : loss : 0.046693, loss_ce: 0.015290\n","iteration 9709 : loss : 0.036006, loss_ce: 0.009134\n","iteration 9710 : loss : 0.060805, loss_ce: 0.018725\n","iteration 9711 : loss : 0.041126, loss_ce: 0.012483\n","iteration 9712 : loss : 0.047733, loss_ce: 0.014157\n","iteration 9713 : loss : 0.047922, loss_ce: 0.022984\n","iteration 9714 : loss : 0.036977, loss_ce: 0.014956\n","iteration 9715 : loss : 0.040330, loss_ce: 0.011354\n","iteration 9716 : loss : 0.036959, loss_ce: 0.014869\n","iteration 9717 : loss : 0.038189, loss_ce: 0.012900\n","iteration 9718 : loss : 0.046883, loss_ce: 0.013818\n","iteration 9719 : loss : 0.044290, loss_ce: 0.011212\n","iteration 9720 : loss : 0.035552, loss_ce: 0.015290\n","iteration 9721 : loss : 0.057853, loss_ce: 0.013069\n","iteration 9722 : loss : 0.039341, loss_ce: 0.012544\n","iteration 9723 : loss : 0.043182, loss_ce: 0.014828\n","iteration 9724 : loss : 0.100784, loss_ce: 0.010314\n","iteration 9725 : loss : 0.035264, loss_ce: 0.009878\n","iteration 9726 : loss : 0.047861, loss_ce: 0.010144\n","iteration 9727 : loss : 0.043363, loss_ce: 0.015488\n","iteration 9728 : loss : 0.050990, loss_ce: 0.019794\n","iteration 9729 : loss : 0.041771, loss_ce: 0.010523\n","iteration 9730 : loss : 0.103536, loss_ce: 0.010107\n","iteration 9731 : loss : 0.045388, loss_ce: 0.013803\n","iteration 9732 : loss : 0.042808, loss_ce: 0.014511\n","iteration 9733 : loss : 0.045721, loss_ce: 0.011636\n","iteration 9734 : loss : 0.054038, loss_ce: 0.012498\n","iteration 9735 : loss : 0.103060, loss_ce: 0.005190\n","iteration 9736 : loss : 0.041539, loss_ce: 0.011918\n","iteration 9737 : loss : 0.045891, loss_ce: 0.015052\n","iteration 9738 : loss : 0.035957, loss_ce: 0.017573\n","iteration 9739 : loss : 0.104331, loss_ce: 0.010095\n","iteration 9740 : loss : 0.045410, loss_ce: 0.012922\n","iteration 9741 : loss : 0.047209, loss_ce: 0.016000\n","iteration 9742 : loss : 0.059140, loss_ce: 0.018600\n","iteration 9743 : loss : 0.048220, loss_ce: 0.013438\n","iteration 9744 : loss : 0.096899, loss_ce: 0.009297\n","iteration 9745 : loss : 0.042837, loss_ce: 0.012781\n","iteration 9746 : loss : 0.052716, loss_ce: 0.016621\n","iteration 9747 : loss : 0.033539, loss_ce: 0.006843\n","iteration 9748 : loss : 0.042326, loss_ce: 0.016531\n","iteration 9749 : loss : 0.038923, loss_ce: 0.013308\n","iteration 9750 : loss : 0.037878, loss_ce: 0.013351\n","iteration 9751 : loss : 0.047997, loss_ce: 0.011618\n","iteration 9752 : loss : 0.046964, loss_ce: 0.015814\n","iteration 9753 : loss : 0.048813, loss_ce: 0.012386\n","iteration 9754 : loss : 0.043394, loss_ce: 0.018897\n","iteration 9755 : loss : 0.042515, loss_ce: 0.010694\n","iteration 9756 : loss : 0.043614, loss_ce: 0.010209\n","iteration 9757 : loss : 0.040148, loss_ce: 0.012878\n","iteration 9758 : loss : 0.038209, loss_ce: 0.010271\n","iteration 9759 : loss : 0.047205, loss_ce: 0.016301\n","iteration 9760 : loss : 0.032847, loss_ce: 0.010517\n","iteration 9761 : loss : 0.040387, loss_ce: 0.010425\n","iteration 9762 : loss : 0.038918, loss_ce: 0.009663\n","iteration 9763 : loss : 0.040489, loss_ce: 0.012158\n","iteration 9764 : loss : 0.044643, loss_ce: 0.015122\n","iteration 9765 : loss : 0.098189, loss_ce: 0.015384\n"," 70%|████████████████████▎        | 105/150 [2:03:37<52:44, 70.33s/it]iteration 9766 : loss : 0.033159, loss_ce: 0.009447\n","iteration 9767 : loss : 0.041425, loss_ce: 0.016110\n","iteration 9768 : loss : 0.055109, loss_ce: 0.014530\n","iteration 9769 : loss : 0.040387, loss_ce: 0.009325\n","iteration 9770 : loss : 0.050516, loss_ce: 0.012300\n","iteration 9771 : loss : 0.042035, loss_ce: 0.020007\n","iteration 9772 : loss : 0.037540, loss_ce: 0.013081\n","iteration 9773 : loss : 0.097661, loss_ce: 0.009524\n","iteration 9774 : loss : 0.039112, loss_ce: 0.018416\n","iteration 9775 : loss : 0.039246, loss_ce: 0.010530\n","iteration 9776 : loss : 0.040136, loss_ce: 0.012851\n","iteration 9777 : loss : 0.094008, loss_ce: 0.008948\n","iteration 9778 : loss : 0.041465, loss_ce: 0.011238\n","iteration 9779 : loss : 0.039785, loss_ce: 0.013595\n","iteration 9780 : loss : 0.045997, loss_ce: 0.020054\n","iteration 9781 : loss : 0.038076, loss_ce: 0.013941\n","iteration 9782 : loss : 0.048466, loss_ce: 0.018845\n","iteration 9783 : loss : 0.039177, loss_ce: 0.011625\n","iteration 9784 : loss : 0.034078, loss_ce: 0.012671\n","iteration 9785 : loss : 0.042514, loss_ce: 0.010505\n","iteration 9786 : loss : 0.038835, loss_ce: 0.015224\n","iteration 9787 : loss : 0.040778, loss_ce: 0.006441\n","iteration 9788 : loss : 0.036573, loss_ce: 0.016830\n","iteration 9789 : loss : 0.046803, loss_ce: 0.014958\n","iteration 9790 : loss : 0.044020, loss_ce: 0.013622\n","iteration 9791 : loss : 0.048187, loss_ce: 0.013112\n","iteration 9792 : loss : 0.047393, loss_ce: 0.018380\n","iteration 9793 : loss : 0.108485, loss_ce: 0.014637\n","iteration 9794 : loss : 0.037580, loss_ce: 0.008062\n","iteration 9795 : loss : 0.042482, loss_ce: 0.015460\n","iteration 9796 : loss : 0.034089, loss_ce: 0.012220\n","iteration 9797 : loss : 0.098088, loss_ce: 0.009716\n","iteration 9798 : loss : 0.044416, loss_ce: 0.010353\n","iteration 9799 : loss : 0.043736, loss_ce: 0.010234\n","iteration 9800 : loss : 0.040896, loss_ce: 0.012813\n","iteration 9801 : loss : 0.045825, loss_ce: 0.019476\n","iteration 9802 : loss : 0.041141, loss_ce: 0.011251\n","iteration 9803 : loss : 0.043865, loss_ce: 0.011664\n","iteration 9804 : loss : 0.037973, loss_ce: 0.014293\n","iteration 9805 : loss : 0.039227, loss_ce: 0.013475\n","iteration 9806 : loss : 0.046060, loss_ce: 0.015404\n","iteration 9807 : loss : 0.040885, loss_ce: 0.014749\n","iteration 9808 : loss : 0.040248, loss_ce: 0.010447\n","iteration 9809 : loss : 0.044365, loss_ce: 0.023419\n","iteration 9810 : loss : 0.036204, loss_ce: 0.012290\n","iteration 9811 : loss : 0.040212, loss_ce: 0.012545\n","iteration 9812 : loss : 0.042028, loss_ce: 0.007632\n","iteration 9813 : loss : 0.040904, loss_ce: 0.012536\n","iteration 9814 : loss : 0.102611, loss_ce: 0.010649\n","iteration 9815 : loss : 0.042919, loss_ce: 0.017140\n","iteration 9816 : loss : 0.042362, loss_ce: 0.014255\n","iteration 9817 : loss : 0.036601, loss_ce: 0.009992\n","iteration 9818 : loss : 0.048354, loss_ce: 0.014815\n","iteration 9819 : loss : 0.047348, loss_ce: 0.013407\n","iteration 9820 : loss : 0.036404, loss_ce: 0.013835\n","iteration 9821 : loss : 0.042389, loss_ce: 0.011161\n","iteration 9822 : loss : 0.034208, loss_ce: 0.009348\n","iteration 9823 : loss : 0.043100, loss_ce: 0.016393\n","iteration 9824 : loss : 0.076557, loss_ce: 0.019365\n","iteration 9825 : loss : 0.055425, loss_ce: 0.010593\n","iteration 9826 : loss : 0.046993, loss_ce: 0.014707\n","iteration 9827 : loss : 0.103101, loss_ce: 0.009949\n","iteration 9828 : loss : 0.054148, loss_ce: 0.023788\n","iteration 9829 : loss : 0.040080, loss_ce: 0.011328\n","iteration 9830 : loss : 0.053979, loss_ce: 0.010879\n","iteration 9831 : loss : 0.043837, loss_ce: 0.009944\n","iteration 9832 : loss : 0.095591, loss_ce: 0.005291\n","iteration 9833 : loss : 0.040596, loss_ce: 0.016463\n","iteration 9834 : loss : 0.048445, loss_ce: 0.004873\n","iteration 9835 : loss : 0.057381, loss_ce: 0.009974\n","iteration 9836 : loss : 0.037156, loss_ce: 0.008916\n","iteration 9837 : loss : 0.040474, loss_ce: 0.012479\n","iteration 9838 : loss : 0.040544, loss_ce: 0.012328\n","iteration 9839 : loss : 0.044850, loss_ce: 0.012803\n","iteration 9840 : loss : 0.045934, loss_ce: 0.013873\n","iteration 9841 : loss : 0.047094, loss_ce: 0.011731\n","iteration 9842 : loss : 0.045644, loss_ce: 0.013242\n","iteration 9843 : loss : 0.035416, loss_ce: 0.010642\n","iteration 9844 : loss : 0.044213, loss_ce: 0.017976\n","iteration 9845 : loss : 0.055857, loss_ce: 0.010761\n","iteration 9846 : loss : 0.098739, loss_ce: 0.006968\n","iteration 9847 : loss : 0.049378, loss_ce: 0.009523\n","iteration 9848 : loss : 0.042852, loss_ce: 0.012098\n","iteration 9849 : loss : 0.086072, loss_ce: 0.013318\n","iteration 9850 : loss : 0.043552, loss_ce: 0.020377\n","iteration 9851 : loss : 0.044115, loss_ce: 0.011602\n","iteration 9852 : loss : 0.044378, loss_ce: 0.010495\n","iteration 9853 : loss : 0.041072, loss_ce: 0.011727\n","iteration 9854 : loss : 0.048744, loss_ce: 0.016335\n","iteration 9855 : loss : 0.042290, loss_ce: 0.013916\n","iteration 9856 : loss : 0.043944, loss_ce: 0.015109\n","iteration 9857 : loss : 0.036956, loss_ce: 0.010948\n","iteration 9858 : loss : 0.063369, loss_ce: 0.022046\n"," 71%|████████████████████▍        | 106/150 [2:04:48<51:35, 70.36s/it]iteration 9859 : loss : 0.051181, loss_ce: 0.014266\n","iteration 9860 : loss : 0.037976, loss_ce: 0.009485\n","iteration 9861 : loss : 0.038911, loss_ce: 0.014508\n","iteration 9862 : loss : 0.071606, loss_ce: 0.012519\n","iteration 9863 : loss : 0.048345, loss_ce: 0.010713\n","iteration 9864 : loss : 0.049442, loss_ce: 0.017577\n","iteration 9865 : loss : 0.047548, loss_ce: 0.020101\n","iteration 9866 : loss : 0.050074, loss_ce: 0.014518\n","iteration 9867 : loss : 0.041221, loss_ce: 0.013635\n","iteration 9868 : loss : 0.049071, loss_ce: 0.007267\n","iteration 9869 : loss : 0.068779, loss_ce: 0.015427\n","iteration 9870 : loss : 0.124190, loss_ce: 0.012467\n","iteration 9871 : loss : 0.050659, loss_ce: 0.016489\n","iteration 9872 : loss : 0.039711, loss_ce: 0.013939\n","iteration 9873 : loss : 0.044932, loss_ce: 0.017313\n","iteration 9874 : loss : 0.070575, loss_ce: 0.010019\n","iteration 9875 : loss : 0.044591, loss_ce: 0.015335\n","iteration 9876 : loss : 0.048763, loss_ce: 0.012884\n","iteration 9877 : loss : 0.043101, loss_ce: 0.015511\n","iteration 9878 : loss : 0.044512, loss_ce: 0.016338\n","iteration 9879 : loss : 0.105745, loss_ce: 0.007838\n","iteration 9880 : loss : 0.041662, loss_ce: 0.013104\n","iteration 9881 : loss : 0.045947, loss_ce: 0.019438\n","iteration 9882 : loss : 0.035175, loss_ce: 0.013346\n","iteration 9883 : loss : 0.055668, loss_ce: 0.014482\n","iteration 9884 : loss : 0.047218, loss_ce: 0.011258\n","iteration 9885 : loss : 0.033306, loss_ce: 0.008542\n","iteration 9886 : loss : 0.059001, loss_ce: 0.011875\n","iteration 9887 : loss : 0.046642, loss_ce: 0.011976\n","iteration 9888 : loss : 0.036606, loss_ce: 0.016621\n","iteration 9889 : loss : 0.048606, loss_ce: 0.015909\n","iteration 9890 : loss : 0.044554, loss_ce: 0.014293\n","iteration 9891 : loss : 0.043251, loss_ce: 0.014777\n","iteration 9892 : loss : 0.044070, loss_ce: 0.010362\n","iteration 9893 : loss : 0.045033, loss_ce: 0.015023\n","iteration 9894 : loss : 0.052151, loss_ce: 0.014233\n","iteration 9895 : loss : 0.048343, loss_ce: 0.017497\n","iteration 9896 : loss : 0.036737, loss_ce: 0.009653\n","iteration 9897 : loss : 0.041991, loss_ce: 0.015395\n","iteration 9898 : loss : 0.062547, loss_ce: 0.006905\n","iteration 9899 : loss : 0.054939, loss_ce: 0.017936\n","iteration 9900 : loss : 0.054251, loss_ce: 0.006992\n","iteration 9901 : loss : 0.055632, loss_ce: 0.014770\n","iteration 9902 : loss : 0.040387, loss_ce: 0.011194\n","iteration 9903 : loss : 0.049009, loss_ce: 0.010056\n","iteration 9904 : loss : 0.054766, loss_ce: 0.015673\n","iteration 9905 : loss : 0.042835, loss_ce: 0.011591\n","iteration 9906 : loss : 0.038249, loss_ce: 0.012253\n","iteration 9907 : loss : 0.061897, loss_ce: 0.016417\n","iteration 9908 : loss : 0.050666, loss_ce: 0.011868\n","iteration 9909 : loss : 0.050245, loss_ce: 0.015749\n","iteration 9910 : loss : 0.050085, loss_ce: 0.017892\n","iteration 9911 : loss : 0.047795, loss_ce: 0.017727\n","iteration 9912 : loss : 0.038446, loss_ce: 0.011153\n","iteration 9913 : loss : 0.040796, loss_ce: 0.008784\n","iteration 9914 : loss : 0.043328, loss_ce: 0.015250\n","iteration 9915 : loss : 0.046121, loss_ce: 0.017180\n","iteration 9916 : loss : 0.062200, loss_ce: 0.013090\n","iteration 9917 : loss : 0.046698, loss_ce: 0.011940\n","iteration 9918 : loss : 0.038666, loss_ce: 0.003931\n","iteration 9919 : loss : 0.047463, loss_ce: 0.016347\n","iteration 9920 : loss : 0.052478, loss_ce: 0.018283\n","iteration 9921 : loss : 0.047642, loss_ce: 0.011679\n","iteration 9922 : loss : 0.047205, loss_ce: 0.018603\n","iteration 9923 : loss : 0.049580, loss_ce: 0.016318\n","iteration 9924 : loss : 0.050355, loss_ce: 0.015600\n","iteration 9925 : loss : 0.052863, loss_ce: 0.013914\n","iteration 9926 : loss : 0.035106, loss_ce: 0.011811\n","iteration 9927 : loss : 0.092284, loss_ce: 0.011134\n","iteration 9928 : loss : 0.103270, loss_ce: 0.010504\n","iteration 9929 : loss : 0.098520, loss_ce: 0.009237\n","iteration 9930 : loss : 0.059391, loss_ce: 0.018439\n","iteration 9931 : loss : 0.049171, loss_ce: 0.016211\n","iteration 9932 : loss : 0.045591, loss_ce: 0.016209\n","iteration 9933 : loss : 0.070815, loss_ce: 0.022889\n","iteration 9934 : loss : 0.044223, loss_ce: 0.011522\n","iteration 9935 : loss : 0.076530, loss_ce: 0.009810\n","iteration 9936 : loss : 0.039241, loss_ce: 0.015297\n","iteration 9937 : loss : 0.049075, loss_ce: 0.015428\n","iteration 9938 : loss : 0.050900, loss_ce: 0.018085\n","iteration 9939 : loss : 0.108081, loss_ce: 0.008801\n","iteration 9940 : loss : 0.036651, loss_ce: 0.011285\n","iteration 9941 : loss : 0.041399, loss_ce: 0.013128\n","iteration 9942 : loss : 0.042774, loss_ce: 0.019180\n","iteration 9943 : loss : 0.049375, loss_ce: 0.017490\n","iteration 9944 : loss : 0.045731, loss_ce: 0.013661\n","iteration 9945 : loss : 0.048911, loss_ce: 0.018357\n","iteration 9946 : loss : 0.039900, loss_ce: 0.011623\n","iteration 9947 : loss : 0.041419, loss_ce: 0.011857\n","iteration 9948 : loss : 0.046680, loss_ce: 0.018479\n","iteration 9949 : loss : 0.046135, loss_ce: 0.004113\n","iteration 9950 : loss : 0.037413, loss_ce: 0.014397\n","iteration 9951 : loss : 0.243138, loss_ce: 0.023128\n"," 71%|████████████████████▋        | 107/150 [2:05:58<50:24, 70.35s/it]iteration 9952 : loss : 0.056275, loss_ce: 0.010903\n","iteration 9953 : loss : 0.100672, loss_ce: 0.014716\n","iteration 9954 : loss : 0.051604, loss_ce: 0.016002\n","iteration 9955 : loss : 0.042752, loss_ce: 0.014323\n","iteration 9956 : loss : 0.044663, loss_ce: 0.020718\n","iteration 9957 : loss : 0.035094, loss_ce: 0.013164\n","iteration 9958 : loss : 0.044739, loss_ce: 0.015624\n","iteration 9959 : loss : 0.038887, loss_ce: 0.020354\n","iteration 9960 : loss : 0.034619, loss_ce: 0.011543\n","iteration 9961 : loss : 0.044243, loss_ce: 0.012353\n","iteration 9962 : loss : 0.043973, loss_ce: 0.011420\n","iteration 9963 : loss : 0.052964, loss_ce: 0.014083\n","iteration 9964 : loss : 0.034613, loss_ce: 0.004217\n","iteration 9965 : loss : 0.042233, loss_ce: 0.015191\n","iteration 9966 : loss : 0.101214, loss_ce: 0.012829\n","iteration 9967 : loss : 0.048781, loss_ce: 0.013785\n","iteration 9968 : loss : 0.040485, loss_ce: 0.015791\n","iteration 9969 : loss : 0.045732, loss_ce: 0.015804\n","iteration 9970 : loss : 0.105924, loss_ce: 0.011770\n","iteration 9971 : loss : 0.043209, loss_ce: 0.007754\n","iteration 9972 : loss : 0.098699, loss_ce: 0.012567\n","iteration 9973 : loss : 0.057164, loss_ce: 0.009459\n","iteration 9974 : loss : 0.053337, loss_ce: 0.015312\n","iteration 9975 : loss : 0.041129, loss_ce: 0.008606\n","iteration 9976 : loss : 0.041399, loss_ce: 0.012990\n","iteration 9977 : loss : 0.047038, loss_ce: 0.009540\n","iteration 9978 : loss : 0.038886, loss_ce: 0.013471\n","iteration 9979 : loss : 0.042166, loss_ce: 0.019165\n","iteration 9980 : loss : 0.042554, loss_ce: 0.011413\n","iteration 9981 : loss : 0.047245, loss_ce: 0.014985\n","iteration 9982 : loss : 0.040701, loss_ce: 0.016985\n","iteration 9983 : loss : 0.094190, loss_ce: 0.008944\n","iteration 9984 : loss : 0.049097, loss_ce: 0.015582\n","iteration 9985 : loss : 0.044588, loss_ce: 0.010400\n","iteration 9986 : loss : 0.040643, loss_ce: 0.011184\n","iteration 9987 : loss : 0.040314, loss_ce: 0.013951\n","iteration 9988 : loss : 0.042009, loss_ce: 0.012468\n","iteration 9989 : loss : 0.051188, loss_ce: 0.014920\n","iteration 9990 : loss : 0.050988, loss_ce: 0.009952\n","iteration 9991 : loss : 0.037735, loss_ce: 0.012797\n","iteration 9992 : loss : 0.045606, loss_ce: 0.016987\n","iteration 9993 : loss : 0.045666, loss_ce: 0.014206\n","iteration 9994 : loss : 0.037515, loss_ce: 0.010705\n","iteration 9995 : loss : 0.050484, loss_ce: 0.011968\n","iteration 9996 : loss : 0.062570, loss_ce: 0.011335\n","iteration 9997 : loss : 0.043111, loss_ce: 0.012304\n","iteration 9998 : loss : 0.050979, loss_ce: 0.015098\n","iteration 9999 : loss : 0.052770, loss_ce: 0.012709\n","iteration 10000 : loss : 0.040525, loss_ce: 0.015311\n","iteration 10001 : loss : 0.052173, loss_ce: 0.015370\n","iteration 10002 : loss : 0.049141, loss_ce: 0.018107\n","iteration 10003 : loss : 0.040382, loss_ce: 0.013613\n","iteration 10004 : loss : 0.036468, loss_ce: 0.006330\n","iteration 10005 : loss : 0.034932, loss_ce: 0.007693\n","iteration 10006 : loss : 0.037538, loss_ce: 0.014129\n","iteration 10007 : loss : 0.038636, loss_ce: 0.011846\n","iteration 10008 : loss : 0.041289, loss_ce: 0.011594\n","iteration 10009 : loss : 0.060091, loss_ce: 0.010934\n","iteration 10010 : loss : 0.046748, loss_ce: 0.014477\n","iteration 10011 : loss : 0.050548, loss_ce: 0.011808\n","iteration 10012 : loss : 0.045291, loss_ce: 0.010285\n","iteration 10013 : loss : 0.099384, loss_ce: 0.008949\n","iteration 10014 : loss : 0.043293, loss_ce: 0.012148\n","iteration 10015 : loss : 0.114902, loss_ce: 0.010219\n","iteration 10016 : loss : 0.045412, loss_ce: 0.013265\n","iteration 10017 : loss : 0.037543, loss_ce: 0.012051\n","iteration 10018 : loss : 0.050232, loss_ce: 0.016256\n","iteration 10019 : loss : 0.046810, loss_ce: 0.014707\n","iteration 10020 : loss : 0.040432, loss_ce: 0.009278\n","iteration 10021 : loss : 0.040952, loss_ce: 0.013736\n","iteration 10022 : loss : 0.037937, loss_ce: 0.008851\n","iteration 10023 : loss : 0.039314, loss_ce: 0.014424\n","iteration 10024 : loss : 0.047859, loss_ce: 0.015799\n","iteration 10025 : loss : 0.106035, loss_ce: 0.016963\n","iteration 10026 : loss : 0.062799, loss_ce: 0.016521\n","iteration 10027 : loss : 0.099687, loss_ce: 0.007190\n","iteration 10028 : loss : 0.056873, loss_ce: 0.017039\n","iteration 10029 : loss : 0.044203, loss_ce: 0.013736\n","iteration 10030 : loss : 0.038503, loss_ce: 0.013131\n","iteration 10031 : loss : 0.039938, loss_ce: 0.013936\n","iteration 10032 : loss : 0.102121, loss_ce: 0.010334\n","iteration 10033 : loss : 0.044836, loss_ce: 0.013977\n","iteration 10034 : loss : 0.038724, loss_ce: 0.014691\n","iteration 10035 : loss : 0.059190, loss_ce: 0.015585\n","iteration 10036 : loss : 0.042894, loss_ce: 0.019508\n","iteration 10037 : loss : 0.048626, loss_ce: 0.014456\n","iteration 10038 : loss : 0.053544, loss_ce: 0.012786\n","iteration 10039 : loss : 0.040470, loss_ce: 0.013704\n","iteration 10040 : loss : 0.043300, loss_ce: 0.014391\n","iteration 10041 : loss : 0.052199, loss_ce: 0.017263\n","iteration 10042 : loss : 0.048213, loss_ce: 0.011231\n","iteration 10043 : loss : 0.034561, loss_ce: 0.005852\n","iteration 10044 : loss : 0.213273, loss_ce: 0.045304\n"," 72%|████████████████████▉        | 108/150 [2:07:08<49:13, 70.31s/it]iteration 10045 : loss : 0.042294, loss_ce: 0.014392\n","iteration 10046 : loss : 0.050392, loss_ce: 0.012147\n","iteration 10047 : loss : 0.041531, loss_ce: 0.014052\n","iteration 10048 : loss : 0.038914, loss_ce: 0.011195\n","iteration 10049 : loss : 0.052977, loss_ce: 0.010517\n","iteration 10050 : loss : 0.113089, loss_ce: 0.010443\n","iteration 10051 : loss : 0.045855, loss_ce: 0.016377\n","iteration 10052 : loss : 0.039088, loss_ce: 0.014482\n","iteration 10053 : loss : 0.056130, loss_ce: 0.014615\n","iteration 10054 : loss : 0.041239, loss_ce: 0.015753\n","iteration 10055 : loss : 0.039808, loss_ce: 0.010687\n","iteration 10056 : loss : 0.040687, loss_ce: 0.011228\n","iteration 10057 : loss : 0.057383, loss_ce: 0.017562\n","iteration 10058 : loss : 0.036092, loss_ce: 0.011006\n","iteration 10059 : loss : 0.045092, loss_ce: 0.009126\n","iteration 10060 : loss : 0.039425, loss_ce: 0.011093\n","iteration 10061 : loss : 0.046744, loss_ce: 0.012410\n","iteration 10062 : loss : 0.083867, loss_ce: 0.015546\n","iteration 10063 : loss : 0.063293, loss_ce: 0.019912\n","iteration 10064 : loss : 0.042077, loss_ce: 0.013945\n","iteration 10065 : loss : 0.052152, loss_ce: 0.012833\n","iteration 10066 : loss : 0.048223, loss_ce: 0.011257\n","iteration 10067 : loss : 0.046689, loss_ce: 0.010826\n","iteration 10068 : loss : 0.051720, loss_ce: 0.009496\n","iteration 10069 : loss : 0.047428, loss_ce: 0.010563\n","iteration 10070 : loss : 0.042028, loss_ce: 0.013378\n","iteration 10071 : loss : 0.043738, loss_ce: 0.009801\n","iteration 10072 : loss : 0.039562, loss_ce: 0.014310\n","iteration 10073 : loss : 0.039613, loss_ce: 0.012551\n","iteration 10074 : loss : 0.060517, loss_ce: 0.017660\n","iteration 10075 : loss : 0.043173, loss_ce: 0.013865\n","iteration 10076 : loss : 0.036182, loss_ce: 0.007027\n","iteration 10077 : loss : 0.046575, loss_ce: 0.014818\n","iteration 10078 : loss : 0.045489, loss_ce: 0.010871\n","iteration 10079 : loss : 0.033035, loss_ce: 0.009810\n","iteration 10080 : loss : 0.132936, loss_ce: 0.007181\n","iteration 10081 : loss : 0.035466, loss_ce: 0.014496\n","iteration 10082 : loss : 0.050822, loss_ce: 0.015465\n","iteration 10083 : loss : 0.071514, loss_ce: 0.019513\n","iteration 10084 : loss : 0.032635, loss_ce: 0.009164\n","iteration 10085 : loss : 0.047417, loss_ce: 0.012979\n","iteration 10086 : loss : 0.039158, loss_ce: 0.017878\n","iteration 10087 : loss : 0.043140, loss_ce: 0.014095\n","iteration 10088 : loss : 0.042623, loss_ce: 0.008244\n","iteration 10089 : loss : 0.040493, loss_ce: 0.010250\n","iteration 10090 : loss : 0.052987, loss_ce: 0.022364\n","iteration 10091 : loss : 0.053434, loss_ce: 0.019256\n","iteration 10092 : loss : 0.047221, loss_ce: 0.021509\n","iteration 10093 : loss : 0.159461, loss_ce: 0.007395\n","iteration 10094 : loss : 0.043007, loss_ce: 0.012869\n","iteration 10095 : loss : 0.047079, loss_ce: 0.012305\n","iteration 10096 : loss : 0.041478, loss_ce: 0.017405\n","iteration 10097 : loss : 0.047726, loss_ce: 0.018047\n","iteration 10098 : loss : 0.036387, loss_ce: 0.013312\n","iteration 10099 : loss : 0.033514, loss_ce: 0.015452\n","iteration 10100 : loss : 0.106462, loss_ce: 0.014994\n","iteration 10101 : loss : 0.113313, loss_ce: 0.008992\n","iteration 10102 : loss : 0.048900, loss_ce: 0.010240\n","iteration 10103 : loss : 0.034018, loss_ce: 0.009495\n","iteration 10104 : loss : 0.039692, loss_ce: 0.008072\n","iteration 10105 : loss : 0.033648, loss_ce: 0.007986\n","iteration 10106 : loss : 0.056724, loss_ce: 0.012007\n","iteration 10107 : loss : 0.036718, loss_ce: 0.013651\n","iteration 10108 : loss : 0.041645, loss_ce: 0.016901\n","iteration 10109 : loss : 0.048433, loss_ce: 0.015155\n","iteration 10110 : loss : 0.088847, loss_ce: 0.009394\n","iteration 10111 : loss : 0.038324, loss_ce: 0.010873\n","iteration 10112 : loss : 0.043950, loss_ce: 0.010502\n","iteration 10113 : loss : 0.045175, loss_ce: 0.012334\n","iteration 10114 : loss : 0.046518, loss_ce: 0.012736\n","iteration 10115 : loss : 0.054305, loss_ce: 0.009042\n","iteration 10116 : loss : 0.037741, loss_ce: 0.008358\n","iteration 10117 : loss : 0.044816, loss_ce: 0.012863\n","iteration 10118 : loss : 0.047709, loss_ce: 0.018594\n","iteration 10119 : loss : 0.057549, loss_ce: 0.016618\n","iteration 10120 : loss : 0.043727, loss_ce: 0.012359\n","iteration 10121 : loss : 0.038871, loss_ce: 0.013142\n","iteration 10122 : loss : 0.048653, loss_ce: 0.015314\n","iteration 10123 : loss : 0.044297, loss_ce: 0.018683\n","iteration 10124 : loss : 0.047371, loss_ce: 0.009527\n","iteration 10125 : loss : 0.034400, loss_ce: 0.008741\n","iteration 10126 : loss : 0.040467, loss_ce: 0.012467\n","iteration 10127 : loss : 0.033310, loss_ce: 0.012148\n","iteration 10128 : loss : 0.047250, loss_ce: 0.018113\n","iteration 10129 : loss : 0.048273, loss_ce: 0.017877\n","iteration 10130 : loss : 0.043535, loss_ce: 0.019907\n","iteration 10131 : loss : 0.039768, loss_ce: 0.016444\n","iteration 10132 : loss : 0.042883, loss_ce: 0.018491\n","iteration 10133 : loss : 0.041696, loss_ce: 0.015261\n","iteration 10134 : loss : 0.048836, loss_ce: 0.020255\n","iteration 10135 : loss : 0.043641, loss_ce: 0.009316\n","iteration 10136 : loss : 0.040233, loss_ce: 0.013501\n","iteration 10137 : loss : 0.305674, loss_ce: 0.004201\n"," 73%|█████████████████████        | 109/150 [2:08:19<48:00, 70.26s/it]iteration 10138 : loss : 0.038750, loss_ce: 0.010481\n","iteration 10139 : loss : 0.116005, loss_ce: 0.030764\n","iteration 10140 : loss : 0.056894, loss_ce: 0.025848\n","iteration 10141 : loss : 0.057341, loss_ce: 0.034591\n","iteration 10142 : loss : 0.055779, loss_ce: 0.027703\n","iteration 10143 : loss : 0.058607, loss_ce: 0.037529\n","iteration 10144 : loss : 0.073926, loss_ce: 0.024016\n","iteration 10145 : loss : 0.052280, loss_ce: 0.019304\n","iteration 10146 : loss : 0.107002, loss_ce: 0.018821\n","iteration 10147 : loss : 0.061670, loss_ce: 0.021119\n","iteration 10148 : loss : 0.049208, loss_ce: 0.015911\n","iteration 10149 : loss : 0.055531, loss_ce: 0.014205\n","iteration 10150 : loss : 0.045492, loss_ce: 0.012505\n","iteration 10151 : loss : 0.042016, loss_ce: 0.015853\n","iteration 10152 : loss : 0.061133, loss_ce: 0.028989\n","iteration 10153 : loss : 0.048932, loss_ce: 0.017792\n","iteration 10154 : loss : 0.042758, loss_ce: 0.014296\n","iteration 10155 : loss : 0.042016, loss_ce: 0.012426\n","iteration 10156 : loss : 0.053920, loss_ce: 0.017414\n","iteration 10157 : loss : 0.048883, loss_ce: 0.017711\n","iteration 10158 : loss : 0.066588, loss_ce: 0.014960\n","iteration 10159 : loss : 0.047077, loss_ce: 0.007044\n","iteration 10160 : loss : 0.053764, loss_ce: 0.013775\n","iteration 10161 : loss : 0.046269, loss_ce: 0.011694\n","iteration 10162 : loss : 0.049662, loss_ce: 0.017408\n","iteration 10163 : loss : 0.043917, loss_ce: 0.014281\n","iteration 10164 : loss : 0.040225, loss_ce: 0.015446\n","iteration 10165 : loss : 0.042033, loss_ce: 0.015373\n","iteration 10166 : loss : 0.043678, loss_ce: 0.009949\n","iteration 10167 : loss : 0.097950, loss_ce: 0.008112\n","iteration 10168 : loss : 0.052888, loss_ce: 0.011273\n","iteration 10169 : loss : 0.036851, loss_ce: 0.017950\n","iteration 10170 : loss : 0.048534, loss_ce: 0.014218\n","iteration 10171 : loss : 0.041649, loss_ce: 0.019237\n","iteration 10172 : loss : 0.107762, loss_ce: 0.017847\n","iteration 10173 : loss : 0.046479, loss_ce: 0.021588\n","iteration 10174 : loss : 0.042787, loss_ce: 0.013512\n","iteration 10175 : loss : 0.041663, loss_ce: 0.011256\n","iteration 10176 : loss : 0.050235, loss_ce: 0.013411\n","iteration 10177 : loss : 0.038116, loss_ce: 0.006605\n","iteration 10178 : loss : 0.103123, loss_ce: 0.017591\n","iteration 10179 : loss : 0.044422, loss_ce: 0.009228\n","iteration 10180 : loss : 0.050575, loss_ce: 0.013570\n","iteration 10181 : loss : 0.050079, loss_ce: 0.014215\n","iteration 10182 : loss : 0.046783, loss_ce: 0.011734\n","iteration 10183 : loss : 0.045492, loss_ce: 0.015575\n","iteration 10184 : loss : 0.043499, loss_ce: 0.016628\n","iteration 10185 : loss : 0.102698, loss_ce: 0.008541\n","iteration 10186 : loss : 0.038594, loss_ce: 0.012840\n","iteration 10187 : loss : 0.080005, loss_ce: 0.012611\n","iteration 10188 : loss : 0.049313, loss_ce: 0.018865\n","iteration 10189 : loss : 0.040466, loss_ce: 0.011132\n","iteration 10190 : loss : 0.046310, loss_ce: 0.016009\n","iteration 10191 : loss : 0.037521, loss_ce: 0.016459\n","iteration 10192 : loss : 0.040384, loss_ce: 0.021702\n","iteration 10193 : loss : 0.051405, loss_ce: 0.024012\n","iteration 10194 : loss : 0.041147, loss_ce: 0.011206\n","iteration 10195 : loss : 0.045807, loss_ce: 0.010176\n","iteration 10196 : loss : 0.040447, loss_ce: 0.019986\n","iteration 10197 : loss : 0.038996, loss_ce: 0.011432\n","iteration 10198 : loss : 0.043821, loss_ce: 0.015136\n","iteration 10199 : loss : 0.044895, loss_ce: 0.012325\n","iteration 10200 : loss : 0.037808, loss_ce: 0.008370\n","iteration 10201 : loss : 0.041869, loss_ce: 0.009207\n","iteration 10202 : loss : 0.034567, loss_ce: 0.009788\n","iteration 10203 : loss : 0.038825, loss_ce: 0.014815\n","iteration 10204 : loss : 0.045558, loss_ce: 0.014919\n","iteration 10205 : loss : 0.043701, loss_ce: 0.014182\n","iteration 10206 : loss : 0.106706, loss_ce: 0.010876\n","iteration 10207 : loss : 0.042993, loss_ce: 0.014007\n","iteration 10208 : loss : 0.042411, loss_ce: 0.015874\n","iteration 10209 : loss : 0.046350, loss_ce: 0.011212\n","iteration 10210 : loss : 0.100639, loss_ce: 0.012098\n","iteration 10211 : loss : 0.036472, loss_ce: 0.007168\n","iteration 10212 : loss : 0.057724, loss_ce: 0.015061\n","iteration 10213 : loss : 0.050470, loss_ce: 0.016725\n","iteration 10214 : loss : 0.054259, loss_ce: 0.010795\n","iteration 10215 : loss : 0.038614, loss_ce: 0.013222\n","iteration 10216 : loss : 0.035582, loss_ce: 0.012626\n","iteration 10217 : loss : 0.040432, loss_ce: 0.015680\n","iteration 10218 : loss : 0.040353, loss_ce: 0.012247\n","iteration 10219 : loss : 0.047416, loss_ce: 0.013866\n","iteration 10220 : loss : 0.041123, loss_ce: 0.022221\n","iteration 10221 : loss : 0.049829, loss_ce: 0.013977\n","iteration 10222 : loss : 0.041967, loss_ce: 0.013625\n","iteration 10223 : loss : 0.030343, loss_ce: 0.011552\n","iteration 10224 : loss : 0.045199, loss_ce: 0.014977\n","iteration 10225 : loss : 0.047551, loss_ce: 0.013375\n","iteration 10226 : loss : 0.043074, loss_ce: 0.014036\n","iteration 10227 : loss : 0.047056, loss_ce: 0.006912\n","iteration 10228 : loss : 0.033375, loss_ce: 0.011984\n","iteration 10229 : loss : 0.041106, loss_ce: 0.012835\n","iteration 10230 : loss : 0.173445, loss_ce: 0.018418\n"," 73%|█████████████████████▎       | 110/150 [2:09:29<46:47, 70.20s/it]iteration 10231 : loss : 0.052466, loss_ce: 0.010583\n","iteration 10232 : loss : 0.038886, loss_ce: 0.010990\n","iteration 10233 : loss : 0.033957, loss_ce: 0.008511\n","iteration 10234 : loss : 0.045033, loss_ce: 0.013152\n","iteration 10235 : loss : 0.044590, loss_ce: 0.011157\n","iteration 10236 : loss : 0.045087, loss_ce: 0.016115\n","iteration 10237 : loss : 0.040153, loss_ce: 0.012562\n","iteration 10238 : loss : 0.042105, loss_ce: 0.013190\n","iteration 10239 : loss : 0.035721, loss_ce: 0.013548\n","iteration 10240 : loss : 0.045344, loss_ce: 0.009068\n","iteration 10241 : loss : 0.043744, loss_ce: 0.012877\n","iteration 10242 : loss : 0.036541, loss_ce: 0.012852\n","iteration 10243 : loss : 0.035711, loss_ce: 0.008216\n","iteration 10244 : loss : 0.038844, loss_ce: 0.011661\n","iteration 10245 : loss : 0.044352, loss_ce: 0.017121\n","iteration 10246 : loss : 0.058590, loss_ce: 0.010916\n","iteration 10247 : loss : 0.036626, loss_ce: 0.009348\n","iteration 10248 : loss : 0.034692, loss_ce: 0.011896\n","iteration 10249 : loss : 0.036899, loss_ce: 0.016310\n","iteration 10250 : loss : 0.038250, loss_ce: 0.010775\n","iteration 10251 : loss : 0.033822, loss_ce: 0.012498\n","iteration 10252 : loss : 0.036262, loss_ce: 0.012753\n","iteration 10253 : loss : 0.040411, loss_ce: 0.008189\n","iteration 10254 : loss : 0.099216, loss_ce: 0.008991\n","iteration 10255 : loss : 0.036738, loss_ce: 0.012263\n","iteration 10256 : loss : 0.043988, loss_ce: 0.008015\n","iteration 10257 : loss : 0.040322, loss_ce: 0.015181\n","iteration 10258 : loss : 0.042583, loss_ce: 0.017794\n","iteration 10259 : loss : 0.042862, loss_ce: 0.013385\n","iteration 10260 : loss : 0.048049, loss_ce: 0.021357\n","iteration 10261 : loss : 0.037905, loss_ce: 0.010490\n","iteration 10262 : loss : 0.043416, loss_ce: 0.027707\n","iteration 10263 : loss : 0.052925, loss_ce: 0.023789\n","iteration 10264 : loss : 0.036906, loss_ce: 0.011089\n","iteration 10265 : loss : 0.038390, loss_ce: 0.013213\n","iteration 10266 : loss : 0.098577, loss_ce: 0.008238\n","iteration 10267 : loss : 0.051700, loss_ce: 0.015398\n","iteration 10268 : loss : 0.035691, loss_ce: 0.014895\n","iteration 10269 : loss : 0.037892, loss_ce: 0.013225\n","iteration 10270 : loss : 0.042194, loss_ce: 0.007477\n","iteration 10271 : loss : 0.043828, loss_ce: 0.005185\n","iteration 10272 : loss : 0.038970, loss_ce: 0.015445\n","iteration 10273 : loss : 0.038010, loss_ce: 0.009169\n","iteration 10274 : loss : 0.051197, loss_ce: 0.013616\n","iteration 10275 : loss : 0.039880, loss_ce: 0.011507\n","iteration 10276 : loss : 0.051148, loss_ce: 0.012998\n","iteration 10277 : loss : 0.038593, loss_ce: 0.009948\n","iteration 10278 : loss : 0.043357, loss_ce: 0.013254\n","iteration 10279 : loss : 0.047783, loss_ce: 0.013965\n","iteration 10280 : loss : 0.041168, loss_ce: 0.016602\n","iteration 10281 : loss : 0.039643, loss_ce: 0.011334\n","iteration 10282 : loss : 0.037861, loss_ce: 0.011912\n","iteration 10283 : loss : 0.042445, loss_ce: 0.014235\n","iteration 10284 : loss : 0.045568, loss_ce: 0.012301\n","iteration 10285 : loss : 0.040406, loss_ce: 0.010729\n","iteration 10286 : loss : 0.042166, loss_ce: 0.017951\n","iteration 10287 : loss : 0.035963, loss_ce: 0.013099\n","iteration 10288 : loss : 0.099122, loss_ce: 0.011634\n","iteration 10289 : loss : 0.104814, loss_ce: 0.010112\n","iteration 10290 : loss : 0.033648, loss_ce: 0.009641\n","iteration 10291 : loss : 0.036412, loss_ce: 0.013487\n","iteration 10292 : loss : 0.039761, loss_ce: 0.017086\n","iteration 10293 : loss : 0.033194, loss_ce: 0.014986\n","iteration 10294 : loss : 0.035499, loss_ce: 0.011217\n","iteration 10295 : loss : 0.042315, loss_ce: 0.008002\n","iteration 10296 : loss : 0.037099, loss_ce: 0.010747\n","iteration 10297 : loss : 0.041163, loss_ce: 0.009487\n","iteration 10298 : loss : 0.044153, loss_ce: 0.011628\n","iteration 10299 : loss : 0.047474, loss_ce: 0.018660\n","iteration 10300 : loss : 0.040723, loss_ce: 0.017291\n","iteration 10301 : loss : 0.039218, loss_ce: 0.014993\n","iteration 10302 : loss : 0.036786, loss_ce: 0.009945\n","iteration 10303 : loss : 0.098039, loss_ce: 0.007292\n","iteration 10304 : loss : 0.047015, loss_ce: 0.010298\n","iteration 10305 : loss : 0.039461, loss_ce: 0.013031\n","iteration 10306 : loss : 0.033165, loss_ce: 0.011231\n","iteration 10307 : loss : 0.054201, loss_ce: 0.011322\n","iteration 10308 : loss : 0.039570, loss_ce: 0.009888\n","iteration 10309 : loss : 0.033066, loss_ce: 0.012413\n","iteration 10310 : loss : 0.098464, loss_ce: 0.008554\n","iteration 10311 : loss : 0.153835, loss_ce: 0.007099\n","iteration 10312 : loss : 0.101158, loss_ce: 0.012135\n","iteration 10313 : loss : 0.055598, loss_ce: 0.022897\n","iteration 10314 : loss : 0.039528, loss_ce: 0.006590\n","iteration 10315 : loss : 0.038698, loss_ce: 0.006710\n","iteration 10316 : loss : 0.039581, loss_ce: 0.007851\n","iteration 10317 : loss : 0.037017, loss_ce: 0.014081\n","iteration 10318 : loss : 0.038238, loss_ce: 0.008915\n","iteration 10319 : loss : 0.042562, loss_ce: 0.016686\n","iteration 10320 : loss : 0.045274, loss_ce: 0.022279\n","iteration 10321 : loss : 0.050022, loss_ce: 0.011669\n","iteration 10322 : loss : 0.045489, loss_ce: 0.015504\n","iteration 10323 : loss : 0.268307, loss_ce: 0.006679\n"," 74%|█████████████████████▍       | 111/150 [2:10:39<45:35, 70.15s/it]iteration 10324 : loss : 0.036758, loss_ce: 0.013365\n","iteration 10325 : loss : 0.038564, loss_ce: 0.012001\n","iteration 10326 : loss : 0.037251, loss_ce: 0.008208\n","iteration 10327 : loss : 0.048412, loss_ce: 0.022407\n","iteration 10328 : loss : 0.050308, loss_ce: 0.017056\n","iteration 10329 : loss : 0.104489, loss_ce: 0.011561\n","iteration 10330 : loss : 0.113407, loss_ce: 0.012249\n","iteration 10331 : loss : 0.044134, loss_ce: 0.017959\n","iteration 10332 : loss : 0.038223, loss_ce: 0.014823\n","iteration 10333 : loss : 0.039231, loss_ce: 0.013321\n","iteration 10334 : loss : 0.035336, loss_ce: 0.010165\n","iteration 10335 : loss : 0.041586, loss_ce: 0.013889\n","iteration 10336 : loss : 0.041677, loss_ce: 0.006869\n","iteration 10337 : loss : 0.041564, loss_ce: 0.013022\n","iteration 10338 : loss : 0.052220, loss_ce: 0.012873\n","iteration 10339 : loss : 0.088383, loss_ce: 0.009993\n","iteration 10340 : loss : 0.038278, loss_ce: 0.014149\n","iteration 10341 : loss : 0.043386, loss_ce: 0.012312\n","iteration 10342 : loss : 0.041129, loss_ce: 0.015078\n","iteration 10343 : loss : 0.036677, loss_ce: 0.016538\n","iteration 10344 : loss : 0.039952, loss_ce: 0.020326\n","iteration 10345 : loss : 0.044990, loss_ce: 0.016321\n","iteration 10346 : loss : 0.050108, loss_ce: 0.012284\n","iteration 10347 : loss : 0.042121, loss_ce: 0.013292\n","iteration 10348 : loss : 0.041513, loss_ce: 0.010066\n","iteration 10349 : loss : 0.041190, loss_ce: 0.014752\n","iteration 10350 : loss : 0.036593, loss_ce: 0.012320\n","iteration 10351 : loss : 0.044499, loss_ce: 0.014942\n","iteration 10352 : loss : 0.046705, loss_ce: 0.018632\n","iteration 10353 : loss : 0.042262, loss_ce: 0.013467\n","iteration 10354 : loss : 0.104955, loss_ce: 0.008440\n","iteration 10355 : loss : 0.045341, loss_ce: 0.010448\n","iteration 10356 : loss : 0.042339, loss_ce: 0.007637\n","iteration 10357 : loss : 0.043009, loss_ce: 0.013516\n","iteration 10358 : loss : 0.048267, loss_ce: 0.009823\n","iteration 10359 : loss : 0.050020, loss_ce: 0.017148\n","iteration 10360 : loss : 0.037514, loss_ce: 0.009685\n","iteration 10361 : loss : 0.048872, loss_ce: 0.015662\n","iteration 10362 : loss : 0.047576, loss_ce: 0.010672\n","iteration 10363 : loss : 0.045325, loss_ce: 0.008758\n","iteration 10364 : loss : 0.040986, loss_ce: 0.006802\n","iteration 10365 : loss : 0.043854, loss_ce: 0.014401\n","iteration 10366 : loss : 0.047086, loss_ce: 0.016754\n","iteration 10367 : loss : 0.103461, loss_ce: 0.011926\n","iteration 10368 : loss : 0.040418, loss_ce: 0.010293\n","iteration 10369 : loss : 0.045377, loss_ce: 0.011126\n","iteration 10370 : loss : 0.064034, loss_ce: 0.009164\n","iteration 10371 : loss : 0.034689, loss_ce: 0.009578\n","iteration 10372 : loss : 0.037499, loss_ce: 0.010947\n","iteration 10373 : loss : 0.039326, loss_ce: 0.012629\n","iteration 10374 : loss : 0.036122, loss_ce: 0.010688\n","iteration 10375 : loss : 0.038777, loss_ce: 0.017309\n","iteration 10376 : loss : 0.043495, loss_ce: 0.014858\n","iteration 10377 : loss : 0.045714, loss_ce: 0.015852\n","iteration 10378 : loss : 0.037221, loss_ce: 0.015108\n","iteration 10379 : loss : 0.038229, loss_ce: 0.008800\n","iteration 10380 : loss : 0.030590, loss_ce: 0.014166\n","iteration 10381 : loss : 0.045709, loss_ce: 0.009788\n","iteration 10382 : loss : 0.034778, loss_ce: 0.013491\n","iteration 10383 : loss : 0.215445, loss_ce: 0.008109\n","iteration 10384 : loss : 0.054111, loss_ce: 0.014690\n","iteration 10385 : loss : 0.060378, loss_ce: 0.014677\n","iteration 10386 : loss : 0.045886, loss_ce: 0.019240\n","iteration 10387 : loss : 0.038945, loss_ce: 0.017407\n","iteration 10388 : loss : 0.036277, loss_ce: 0.010316\n","iteration 10389 : loss : 0.049512, loss_ce: 0.011693\n","iteration 10390 : loss : 0.054244, loss_ce: 0.017231\n","iteration 10391 : loss : 0.051621, loss_ce: 0.021716\n","iteration 10392 : loss : 0.041635, loss_ce: 0.015770\n","iteration 10393 : loss : 0.039699, loss_ce: 0.014189\n","iteration 10394 : loss : 0.044333, loss_ce: 0.013485\n","iteration 10395 : loss : 0.041579, loss_ce: 0.014341\n","iteration 10396 : loss : 0.041980, loss_ce: 0.008682\n","iteration 10397 : loss : 0.046630, loss_ce: 0.009753\n","iteration 10398 : loss : 0.043071, loss_ce: 0.011916\n","iteration 10399 : loss : 0.035414, loss_ce: 0.009421\n","iteration 10400 : loss : 0.098401, loss_ce: 0.009010\n","iteration 10401 : loss : 0.040674, loss_ce: 0.019934\n","iteration 10402 : loss : 0.039423, loss_ce: 0.018204\n","iteration 10403 : loss : 0.046185, loss_ce: 0.013636\n","iteration 10404 : loss : 0.108086, loss_ce: 0.009245\n","iteration 10405 : loss : 0.051916, loss_ce: 0.010336\n","iteration 10406 : loss : 0.035233, loss_ce: 0.009447\n","iteration 10407 : loss : 0.069574, loss_ce: 0.014103\n","iteration 10408 : loss : 0.036968, loss_ce: 0.010524\n","iteration 10409 : loss : 0.035528, loss_ce: 0.010984\n","iteration 10410 : loss : 0.041741, loss_ce: 0.008417\n","iteration 10411 : loss : 0.041273, loss_ce: 0.015839\n","iteration 10412 : loss : 0.045080, loss_ce: 0.012095\n","iteration 10413 : loss : 0.048712, loss_ce: 0.009274\n","iteration 10414 : loss : 0.039484, loss_ce: 0.014902\n","iteration 10415 : loss : 0.038460, loss_ce: 0.011712\n","iteration 10416 : loss : 0.165904, loss_ce: 0.014937\n"," 75%|█████████████████████▋       | 112/150 [2:11:49<44:23, 70.10s/it]iteration 10417 : loss : 0.042691, loss_ce: 0.015038\n","iteration 10418 : loss : 0.042025, loss_ce: 0.015781\n","iteration 10419 : loss : 0.045975, loss_ce: 0.011185\n","iteration 10420 : loss : 0.047602, loss_ce: 0.010711\n","iteration 10421 : loss : 0.043311, loss_ce: 0.008399\n","iteration 10422 : loss : 0.047782, loss_ce: 0.009022\n","iteration 10423 : loss : 0.040489, loss_ce: 0.011959\n","iteration 10424 : loss : 0.041126, loss_ce: 0.009599\n","iteration 10425 : loss : 0.043372, loss_ce: 0.011273\n","iteration 10426 : loss : 0.042596, loss_ce: 0.011455\n","iteration 10427 : loss : 0.045532, loss_ce: 0.012099\n","iteration 10428 : loss : 0.046832, loss_ce: 0.017125\n","iteration 10429 : loss : 0.041312, loss_ce: 0.017303\n","iteration 10430 : loss : 0.042549, loss_ce: 0.013061\n","iteration 10431 : loss : 0.037217, loss_ce: 0.011391\n","iteration 10432 : loss : 0.037885, loss_ce: 0.012600\n","iteration 10433 : loss : 0.038497, loss_ce: 0.010410\n","iteration 10434 : loss : 0.045709, loss_ce: 0.012187\n","iteration 10435 : loss : 0.040429, loss_ce: 0.009343\n","iteration 10436 : loss : 0.034668, loss_ce: 0.008358\n","iteration 10437 : loss : 0.047040, loss_ce: 0.017013\n","iteration 10438 : loss : 0.038028, loss_ce: 0.014284\n","iteration 10439 : loss : 0.042656, loss_ce: 0.006668\n","iteration 10440 : loss : 0.041600, loss_ce: 0.017537\n","iteration 10441 : loss : 0.040588, loss_ce: 0.013630\n","iteration 10442 : loss : 0.037614, loss_ce: 0.009903\n","iteration 10443 : loss : 0.039460, loss_ce: 0.012843\n","iteration 10444 : loss : 0.031939, loss_ce: 0.011551\n","iteration 10445 : loss : 0.044011, loss_ce: 0.015314\n","iteration 10446 : loss : 0.100859, loss_ce: 0.011754\n","iteration 10447 : loss : 0.102207, loss_ce: 0.010551\n","iteration 10448 : loss : 0.060067, loss_ce: 0.014051\n","iteration 10449 : loss : 0.093455, loss_ce: 0.008800\n","iteration 10450 : loss : 0.051150, loss_ce: 0.011999\n","iteration 10451 : loss : 0.098553, loss_ce: 0.013883\n","iteration 10452 : loss : 0.039117, loss_ce: 0.010103\n","iteration 10453 : loss : 0.121432, loss_ce: 0.006326\n","iteration 10454 : loss : 0.047185, loss_ce: 0.020073\n","iteration 10455 : loss : 0.037423, loss_ce: 0.017168\n","iteration 10456 : loss : 0.038739, loss_ce: 0.013895\n","iteration 10457 : loss : 0.045311, loss_ce: 0.020406\n","iteration 10458 : loss : 0.043128, loss_ce: 0.009173\n","iteration 10459 : loss : 0.037543, loss_ce: 0.010398\n","iteration 10460 : loss : 0.040863, loss_ce: 0.014518\n","iteration 10461 : loss : 0.034238, loss_ce: 0.009533\n","iteration 10462 : loss : 0.047068, loss_ce: 0.015742\n","iteration 10463 : loss : 0.036252, loss_ce: 0.013175\n","iteration 10464 : loss : 0.067716, loss_ce: 0.015271\n","iteration 10465 : loss : 0.042881, loss_ce: 0.015224\n","iteration 10466 : loss : 0.043013, loss_ce: 0.016661\n","iteration 10467 : loss : 0.037303, loss_ce: 0.010464\n","iteration 10468 : loss : 0.043524, loss_ce: 0.009781\n","iteration 10469 : loss : 0.038017, loss_ce: 0.011498\n","iteration 10470 : loss : 0.038857, loss_ce: 0.018029\n","iteration 10471 : loss : 0.104606, loss_ce: 0.013364\n","iteration 10472 : loss : 0.051591, loss_ce: 0.009620\n","iteration 10473 : loss : 0.039360, loss_ce: 0.013333\n","iteration 10474 : loss : 0.040180, loss_ce: 0.014915\n","iteration 10475 : loss : 0.043870, loss_ce: 0.011947\n","iteration 10476 : loss : 0.037121, loss_ce: 0.009453\n","iteration 10477 : loss : 0.046982, loss_ce: 0.009527\n","iteration 10478 : loss : 0.048262, loss_ce: 0.013398\n","iteration 10479 : loss : 0.045747, loss_ce: 0.011254\n","iteration 10480 : loss : 0.041077, loss_ce: 0.018093\n","iteration 10481 : loss : 0.047054, loss_ce: 0.013315\n","iteration 10482 : loss : 0.035891, loss_ce: 0.017670\n","iteration 10483 : loss : 0.040968, loss_ce: 0.015856\n","iteration 10484 : loss : 0.045450, loss_ce: 0.015566\n","iteration 10485 : loss : 0.040195, loss_ce: 0.010486\n","iteration 10486 : loss : 0.039539, loss_ce: 0.013340\n","iteration 10487 : loss : 0.034919, loss_ce: 0.011724\n","iteration 10488 : loss : 0.035087, loss_ce: 0.007486\n","iteration 10489 : loss : 0.048738, loss_ce: 0.014959\n","iteration 10490 : loss : 0.104656, loss_ce: 0.010966\n","iteration 10491 : loss : 0.044380, loss_ce: 0.018951\n","iteration 10492 : loss : 0.057172, loss_ce: 0.012136\n","iteration 10493 : loss : 0.033755, loss_ce: 0.009198\n","iteration 10494 : loss : 0.037335, loss_ce: 0.010339\n","iteration 10495 : loss : 0.032998, loss_ce: 0.012098\n","iteration 10496 : loss : 0.030926, loss_ce: 0.006644\n","iteration 10497 : loss : 0.031688, loss_ce: 0.007389\n","iteration 10498 : loss : 0.040075, loss_ce: 0.008117\n","iteration 10499 : loss : 0.048260, loss_ce: 0.016483\n","iteration 10500 : loss : 0.034659, loss_ce: 0.009599\n","iteration 10501 : loss : 0.033509, loss_ce: 0.009489\n","iteration 10502 : loss : 0.041750, loss_ce: 0.016252\n","iteration 10503 : loss : 0.045781, loss_ce: 0.021412\n","iteration 10504 : loss : 0.056456, loss_ce: 0.015368\n","iteration 10505 : loss : 0.058044, loss_ce: 0.012591\n","iteration 10506 : loss : 0.036982, loss_ce: 0.010598\n","iteration 10507 : loss : 0.035520, loss_ce: 0.005501\n","iteration 10508 : loss : 0.063913, loss_ce: 0.012707\n","iteration 10509 : loss : 0.273086, loss_ce: 0.006070\n"," 75%|█████████████████████▊       | 113/150 [2:12:59<43:13, 70.08s/it]iteration 10510 : loss : 0.059183, loss_ce: 0.009473\n","iteration 10511 : loss : 0.043438, loss_ce: 0.012127\n","iteration 10512 : loss : 0.038666, loss_ce: 0.009688\n","iteration 10513 : loss : 0.040495, loss_ce: 0.010845\n","iteration 10514 : loss : 0.043712, loss_ce: 0.014154\n","iteration 10515 : loss : 0.030612, loss_ce: 0.008812\n","iteration 10516 : loss : 0.039094, loss_ce: 0.013814\n","iteration 10517 : loss : 0.034328, loss_ce: 0.013959\n","iteration 10518 : loss : 0.036206, loss_ce: 0.012427\n","iteration 10519 : loss : 0.040451, loss_ce: 0.007534\n","iteration 10520 : loss : 0.040052, loss_ce: 0.014583\n","iteration 10521 : loss : 0.041893, loss_ce: 0.010452\n","iteration 10522 : loss : 0.051495, loss_ce: 0.019406\n","iteration 10523 : loss : 0.044779, loss_ce: 0.010054\n","iteration 10524 : loss : 0.038664, loss_ce: 0.013086\n","iteration 10525 : loss : 0.042468, loss_ce: 0.009051\n","iteration 10526 : loss : 0.042265, loss_ce: 0.012030\n","iteration 10527 : loss : 0.038993, loss_ce: 0.018438\n","iteration 10528 : loss : 0.038181, loss_ce: 0.007593\n","iteration 10529 : loss : 0.033847, loss_ce: 0.012215\n","iteration 10530 : loss : 0.042881, loss_ce: 0.014109\n","iteration 10531 : loss : 0.044569, loss_ce: 0.007917\n","iteration 10532 : loss : 0.041520, loss_ce: 0.018508\n","iteration 10533 : loss : 0.039965, loss_ce: 0.014544\n","iteration 10534 : loss : 0.046974, loss_ce: 0.011607\n","iteration 10535 : loss : 0.043548, loss_ce: 0.015138\n","iteration 10536 : loss : 0.045376, loss_ce: 0.008826\n","iteration 10537 : loss : 0.051400, loss_ce: 0.012122\n","iteration 10538 : loss : 0.037048, loss_ce: 0.013983\n","iteration 10539 : loss : 0.049975, loss_ce: 0.015427\n","iteration 10540 : loss : 0.041387, loss_ce: 0.013280\n","iteration 10541 : loss : 0.034751, loss_ce: 0.009209\n","iteration 10542 : loss : 0.043188, loss_ce: 0.014239\n","iteration 10543 : loss : 0.038610, loss_ce: 0.010254\n","iteration 10544 : loss : 0.037703, loss_ce: 0.008024\n","iteration 10545 : loss : 0.038325, loss_ce: 0.012350\n","iteration 10546 : loss : 0.038660, loss_ce: 0.016755\n","iteration 10547 : loss : 0.053404, loss_ce: 0.014815\n","iteration 10548 : loss : 0.103856, loss_ce: 0.014637\n","iteration 10549 : loss : 0.050865, loss_ce: 0.011510\n","iteration 10550 : loss : 0.038323, loss_ce: 0.009082\n","iteration 10551 : loss : 0.055259, loss_ce: 0.013273\n","iteration 10552 : loss : 0.048747, loss_ce: 0.016006\n","iteration 10553 : loss : 0.040118, loss_ce: 0.010397\n","iteration 10554 : loss : 0.040509, loss_ce: 0.014382\n","iteration 10555 : loss : 0.034854, loss_ce: 0.014579\n","iteration 10556 : loss : 0.051899, loss_ce: 0.008192\n","iteration 10557 : loss : 0.038214, loss_ce: 0.011580\n","iteration 10558 : loss : 0.056173, loss_ce: 0.007932\n","iteration 10559 : loss : 0.045935, loss_ce: 0.011574\n","iteration 10560 : loss : 0.041398, loss_ce: 0.014458\n","iteration 10561 : loss : 0.042229, loss_ce: 0.009736\n","iteration 10562 : loss : 0.100546, loss_ce: 0.011962\n","iteration 10563 : loss : 0.037931, loss_ce: 0.016767\n","iteration 10564 : loss : 0.040608, loss_ce: 0.010051\n","iteration 10565 : loss : 0.043843, loss_ce: 0.016234\n","iteration 10566 : loss : 0.041022, loss_ce: 0.017788\n","iteration 10567 : loss : 0.093985, loss_ce: 0.006274\n","iteration 10568 : loss : 0.037025, loss_ce: 0.009611\n","iteration 10569 : loss : 0.048831, loss_ce: 0.010379\n","iteration 10570 : loss : 0.052021, loss_ce: 0.012627\n","iteration 10571 : loss : 0.037647, loss_ce: 0.016497\n","iteration 10572 : loss : 0.040689, loss_ce: 0.013847\n","iteration 10573 : loss : 0.099448, loss_ce: 0.011066\n","iteration 10574 : loss : 0.031621, loss_ce: 0.014289\n","iteration 10575 : loss : 0.051870, loss_ce: 0.014806\n","iteration 10576 : loss : 0.043075, loss_ce: 0.012167\n","iteration 10577 : loss : 0.043235, loss_ce: 0.012908\n","iteration 10578 : loss : 0.040782, loss_ce: 0.016171\n","iteration 10579 : loss : 0.040594, loss_ce: 0.012441\n","iteration 10580 : loss : 0.034862, loss_ce: 0.008937\n","iteration 10581 : loss : 0.036740, loss_ce: 0.014116\n","iteration 10582 : loss : 0.038499, loss_ce: 0.010820\n","iteration 10583 : loss : 0.031134, loss_ce: 0.006982\n","iteration 10584 : loss : 0.033982, loss_ce: 0.008617\n","iteration 10585 : loss : 0.041985, loss_ce: 0.009370\n","iteration 10586 : loss : 0.096700, loss_ce: 0.011514\n","iteration 10587 : loss : 0.158418, loss_ce: 0.005737\n","iteration 10588 : loss : 0.040368, loss_ce: 0.012905\n","iteration 10589 : loss : 0.098568, loss_ce: 0.008802\n","iteration 10590 : loss : 0.095004, loss_ce: 0.007348\n","iteration 10591 : loss : 0.045775, loss_ce: 0.011235\n","iteration 10592 : loss : 0.050238, loss_ce: 0.016347\n","iteration 10593 : loss : 0.046558, loss_ce: 0.016664\n","iteration 10594 : loss : 0.048848, loss_ce: 0.020889\n","iteration 10595 : loss : 0.045787, loss_ce: 0.015289\n","iteration 10596 : loss : 0.035558, loss_ce: 0.015620\n","iteration 10597 : loss : 0.042659, loss_ce: 0.014665\n","iteration 10598 : loss : 0.062975, loss_ce: 0.014370\n","iteration 10599 : loss : 0.036802, loss_ce: 0.014072\n","iteration 10600 : loss : 0.031305, loss_ce: 0.012205\n","iteration 10601 : loss : 0.047852, loss_ce: 0.009897\n","iteration 10602 : loss : 0.278042, loss_ce: 0.002777\n"," 76%|██████████████████████       | 114/150 [2:14:09<42:01, 70.04s/it]iteration 10603 : loss : 0.106144, loss_ce: 0.010274\n","iteration 10604 : loss : 0.043116, loss_ce: 0.012063\n","iteration 10605 : loss : 0.100802, loss_ce: 0.007495\n","iteration 10606 : loss : 0.033644, loss_ce: 0.009347\n","iteration 10607 : loss : 0.044898, loss_ce: 0.010348\n","iteration 10608 : loss : 0.037517, loss_ce: 0.007259\n","iteration 10609 : loss : 0.069999, loss_ce: 0.010107\n","iteration 10610 : loss : 0.042088, loss_ce: 0.010548\n","iteration 10611 : loss : 0.038914, loss_ce: 0.012851\n","iteration 10612 : loss : 0.041331, loss_ce: 0.009956\n","iteration 10613 : loss : 0.038402, loss_ce: 0.012103\n","iteration 10614 : loss : 0.065441, loss_ce: 0.011859\n","iteration 10615 : loss : 0.099933, loss_ce: 0.011354\n","iteration 10616 : loss : 0.036681, loss_ce: 0.010518\n","iteration 10617 : loss : 0.034506, loss_ce: 0.012464\n","iteration 10618 : loss : 0.037081, loss_ce: 0.009834\n","iteration 10619 : loss : 0.044421, loss_ce: 0.015810\n","iteration 10620 : loss : 0.041972, loss_ce: 0.011916\n","iteration 10621 : loss : 0.039939, loss_ce: 0.014933\n","iteration 10622 : loss : 0.029997, loss_ce: 0.008941\n","iteration 10623 : loss : 0.097974, loss_ce: 0.013584\n","iteration 10624 : loss : 0.075210, loss_ce: 0.004544\n","iteration 10625 : loss : 0.046647, loss_ce: 0.017770\n","iteration 10626 : loss : 0.042179, loss_ce: 0.016970\n","iteration 10627 : loss : 0.095608, loss_ce: 0.013561\n","iteration 10628 : loss : 0.049820, loss_ce: 0.016542\n","iteration 10629 : loss : 0.044869, loss_ce: 0.018047\n","iteration 10630 : loss : 0.043262, loss_ce: 0.011089\n","iteration 10631 : loss : 0.046570, loss_ce: 0.013857\n","iteration 10632 : loss : 0.051470, loss_ce: 0.017598\n","iteration 10633 : loss : 0.036223, loss_ce: 0.009002\n","iteration 10634 : loss : 0.043821, loss_ce: 0.021176\n","iteration 10635 : loss : 0.049531, loss_ce: 0.008361\n","iteration 10636 : loss : 0.108536, loss_ce: 0.013369\n","iteration 10637 : loss : 0.038591, loss_ce: 0.016316\n","iteration 10638 : loss : 0.040549, loss_ce: 0.009296\n","iteration 10639 : loss : 0.039135, loss_ce: 0.012497\n","iteration 10640 : loss : 0.044216, loss_ce: 0.014056\n","iteration 10641 : loss : 0.042995, loss_ce: 0.013847\n","iteration 10642 : loss : 0.035406, loss_ce: 0.010907\n","iteration 10643 : loss : 0.033778, loss_ce: 0.009253\n","iteration 10644 : loss : 0.038733, loss_ce: 0.015846\n","iteration 10645 : loss : 0.047281, loss_ce: 0.016253\n","iteration 10646 : loss : 0.040055, loss_ce: 0.012392\n","iteration 10647 : loss : 0.037573, loss_ce: 0.017025\n","iteration 10648 : loss : 0.106488, loss_ce: 0.009821\n","iteration 10649 : loss : 0.038173, loss_ce: 0.006743\n","iteration 10650 : loss : 0.043567, loss_ce: 0.015096\n","iteration 10651 : loss : 0.040797, loss_ce: 0.013331\n","iteration 10652 : loss : 0.109151, loss_ce: 0.009113\n","iteration 10653 : loss : 0.044519, loss_ce: 0.011850\n","iteration 10654 : loss : 0.033139, loss_ce: 0.011672\n","iteration 10655 : loss : 0.042477, loss_ce: 0.010677\n","iteration 10656 : loss : 0.037307, loss_ce: 0.010047\n","iteration 10657 : loss : 0.095737, loss_ce: 0.006946\n","iteration 10658 : loss : 0.034116, loss_ce: 0.009055\n","iteration 10659 : loss : 0.059133, loss_ce: 0.015456\n","iteration 10660 : loss : 0.042694, loss_ce: 0.007533\n","iteration 10661 : loss : 0.031599, loss_ce: 0.011690\n","iteration 10662 : loss : 0.040769, loss_ce: 0.009515\n","iteration 10663 : loss : 0.040728, loss_ce: 0.016874\n","iteration 10664 : loss : 0.043958, loss_ce: 0.016717\n","iteration 10665 : loss : 0.050818, loss_ce: 0.018947\n","iteration 10666 : loss : 0.045036, loss_ce: 0.009373\n","iteration 10667 : loss : 0.055438, loss_ce: 0.012479\n","iteration 10668 : loss : 0.043322, loss_ce: 0.017292\n","iteration 10669 : loss : 0.039101, loss_ce: 0.015371\n","iteration 10670 : loss : 0.043017, loss_ce: 0.010394\n","iteration 10671 : loss : 0.056745, loss_ce: 0.008089\n","iteration 10672 : loss : 0.047449, loss_ce: 0.017886\n","iteration 10673 : loss : 0.031738, loss_ce: 0.010999\n","iteration 10674 : loss : 0.038826, loss_ce: 0.008728\n","iteration 10675 : loss : 0.037120, loss_ce: 0.014478\n","iteration 10676 : loss : 0.034013, loss_ce: 0.010085\n","iteration 10677 : loss : 0.040913, loss_ce: 0.009925\n","iteration 10678 : loss : 0.034781, loss_ce: 0.013076\n","iteration 10679 : loss : 0.054320, loss_ce: 0.018712\n","iteration 10680 : loss : 0.039101, loss_ce: 0.014655\n","iteration 10681 : loss : 0.038759, loss_ce: 0.010021\n","iteration 10682 : loss : 0.039407, loss_ce: 0.014929\n","iteration 10683 : loss : 0.044124, loss_ce: 0.014701\n","iteration 10684 : loss : 0.043307, loss_ce: 0.011011\n","iteration 10685 : loss : 0.044531, loss_ce: 0.015895\n","iteration 10686 : loss : 0.043102, loss_ce: 0.017861\n","iteration 10687 : loss : 0.046550, loss_ce: 0.016548\n","iteration 10688 : loss : 0.039986, loss_ce: 0.013934\n","iteration 10689 : loss : 0.042792, loss_ce: 0.008934\n","iteration 10690 : loss : 0.036815, loss_ce: 0.008498\n","iteration 10691 : loss : 0.041309, loss_ce: 0.014193\n","iteration 10692 : loss : 0.084728, loss_ce: 0.012050\n","iteration 10693 : loss : 0.042689, loss_ce: 0.008204\n","iteration 10694 : loss : 0.043941, loss_ce: 0.015479\n","iteration 10695 : loss : 0.230814, loss_ce: 0.028921\n"," 77%|██████████████████████▏      | 115/150 [2:15:19<40:50, 70.02s/it]iteration 10696 : loss : 0.155842, loss_ce: 0.006528\n","iteration 10697 : loss : 0.046365, loss_ce: 0.011323\n","iteration 10698 : loss : 0.049172, loss_ce: 0.009590\n","iteration 10699 : loss : 0.093837, loss_ce: 0.009772\n","iteration 10700 : loss : 0.038589, loss_ce: 0.012382\n","iteration 10701 : loss : 0.043210, loss_ce: 0.011286\n","iteration 10702 : loss : 0.104830, loss_ce: 0.012880\n","iteration 10703 : loss : 0.048657, loss_ce: 0.015424\n","iteration 10704 : loss : 0.041819, loss_ce: 0.013019\n","iteration 10705 : loss : 0.042189, loss_ce: 0.010695\n","iteration 10706 : loss : 0.035848, loss_ce: 0.015053\n","iteration 10707 : loss : 0.107429, loss_ce: 0.012564\n","iteration 10708 : loss : 0.039736, loss_ce: 0.017938\n","iteration 10709 : loss : 0.041852, loss_ce: 0.011400\n","iteration 10710 : loss : 0.042664, loss_ce: 0.013263\n","iteration 10711 : loss : 0.036601, loss_ce: 0.011764\n","iteration 10712 : loss : 0.121186, loss_ce: 0.013133\n","iteration 10713 : loss : 0.043258, loss_ce: 0.016071\n","iteration 10714 : loss : 0.076387, loss_ce: 0.014919\n","iteration 10715 : loss : 0.041271, loss_ce: 0.011151\n","iteration 10716 : loss : 0.041408, loss_ce: 0.018569\n","iteration 10717 : loss : 0.041169, loss_ce: 0.009779\n","iteration 10718 : loss : 0.038941, loss_ce: 0.012779\n","iteration 10719 : loss : 0.059166, loss_ce: 0.009842\n","iteration 10720 : loss : 0.041045, loss_ce: 0.008406\n","iteration 10721 : loss : 0.041127, loss_ce: 0.011222\n","iteration 10722 : loss : 0.042805, loss_ce: 0.016603\n","iteration 10723 : loss : 0.044051, loss_ce: 0.014029\n","iteration 10724 : loss : 0.034252, loss_ce: 0.011691\n","iteration 10725 : loss : 0.047443, loss_ce: 0.011217\n","iteration 10726 : loss : 0.044467, loss_ce: 0.012833\n","iteration 10727 : loss : 0.034232, loss_ce: 0.012420\n","iteration 10728 : loss : 0.036217, loss_ce: 0.003828\n","iteration 10729 : loss : 0.214980, loss_ce: 0.003788\n","iteration 10730 : loss : 0.049014, loss_ce: 0.011397\n","iteration 10731 : loss : 0.038621, loss_ce: 0.015874\n","iteration 10732 : loss : 0.099948, loss_ce: 0.010542\n","iteration 10733 : loss : 0.046579, loss_ce: 0.007069\n","iteration 10734 : loss : 0.051035, loss_ce: 0.007157\n","iteration 10735 : loss : 0.046576, loss_ce: 0.014514\n","iteration 10736 : loss : 0.042415, loss_ce: 0.016884\n","iteration 10737 : loss : 0.035934, loss_ce: 0.013573\n","iteration 10738 : loss : 0.045696, loss_ce: 0.009063\n","iteration 10739 : loss : 0.059705, loss_ce: 0.010675\n","iteration 10740 : loss : 0.037645, loss_ce: 0.010564\n","iteration 10741 : loss : 0.046521, loss_ce: 0.013879\n","iteration 10742 : loss : 0.042908, loss_ce: 0.013661\n","iteration 10743 : loss : 0.038254, loss_ce: 0.009192\n","iteration 10744 : loss : 0.044014, loss_ce: 0.014013\n","iteration 10745 : loss : 0.035858, loss_ce: 0.012923\n","iteration 10746 : loss : 0.045247, loss_ce: 0.015343\n","iteration 10747 : loss : 0.035014, loss_ce: 0.007642\n","iteration 10748 : loss : 0.043402, loss_ce: 0.014104\n","iteration 10749 : loss : 0.045378, loss_ce: 0.014454\n","iteration 10750 : loss : 0.043300, loss_ce: 0.014737\n","iteration 10751 : loss : 0.058708, loss_ce: 0.010236\n","iteration 10752 : loss : 0.038718, loss_ce: 0.009225\n","iteration 10753 : loss : 0.035543, loss_ce: 0.013128\n","iteration 10754 : loss : 0.043625, loss_ce: 0.012173\n","iteration 10755 : loss : 0.040481, loss_ce: 0.017450\n","iteration 10756 : loss : 0.052109, loss_ce: 0.008867\n","iteration 10757 : loss : 0.038152, loss_ce: 0.005110\n","iteration 10758 : loss : 0.038615, loss_ce: 0.010920\n","iteration 10759 : loss : 0.038266, loss_ce: 0.012970\n","iteration 10760 : loss : 0.043351, loss_ce: 0.013576\n","iteration 10761 : loss : 0.044706, loss_ce: 0.011494\n","iteration 10762 : loss : 0.041209, loss_ce: 0.010792\n","iteration 10763 : loss : 0.097434, loss_ce: 0.010668\n","iteration 10764 : loss : 0.046653, loss_ce: 0.012409\n","iteration 10765 : loss : 0.040893, loss_ce: 0.018696\n","iteration 10766 : loss : 0.038444, loss_ce: 0.014146\n","iteration 10767 : loss : 0.029903, loss_ce: 0.008294\n","iteration 10768 : loss : 0.040780, loss_ce: 0.012259\n","iteration 10769 : loss : 0.035218, loss_ce: 0.013232\n","iteration 10770 : loss : 0.047757, loss_ce: 0.018769\n","iteration 10771 : loss : 0.039242, loss_ce: 0.015202\n","iteration 10772 : loss : 0.035374, loss_ce: 0.012517\n","iteration 10773 : loss : 0.071366, loss_ce: 0.010077\n","iteration 10774 : loss : 0.036501, loss_ce: 0.015373\n","iteration 10775 : loss : 0.045899, loss_ce: 0.015828\n","iteration 10776 : loss : 0.096353, loss_ce: 0.007536\n","iteration 10777 : loss : 0.051269, loss_ce: 0.009680\n","iteration 10778 : loss : 0.038418, loss_ce: 0.011551\n","iteration 10779 : loss : 0.043820, loss_ce: 0.011498\n","iteration 10780 : loss : 0.042253, loss_ce: 0.011700\n","iteration 10781 : loss : 0.041914, loss_ce: 0.017164\n","iteration 10782 : loss : 0.045442, loss_ce: 0.013929\n","iteration 10783 : loss : 0.037102, loss_ce: 0.011824\n","iteration 10784 : loss : 0.050569, loss_ce: 0.022041\n","iteration 10785 : loss : 0.034770, loss_ce: 0.008466\n","iteration 10786 : loss : 0.049766, loss_ce: 0.023714\n","iteration 10787 : loss : 0.032315, loss_ce: 0.009449\n","iteration 10788 : loss : 0.114577, loss_ce: 0.012822\n"," 77%|██████████████████████▍      | 116/150 [2:16:29<39:41, 70.04s/it]iteration 10789 : loss : 0.048850, loss_ce: 0.013174\n","iteration 10790 : loss : 0.045479, loss_ce: 0.010211\n","iteration 10791 : loss : 0.060345, loss_ce: 0.012124\n","iteration 10792 : loss : 0.031274, loss_ce: 0.012974\n","iteration 10793 : loss : 0.097281, loss_ce: 0.010143\n","iteration 10794 : loss : 0.102793, loss_ce: 0.010928\n","iteration 10795 : loss : 0.039689, loss_ce: 0.007853\n","iteration 10796 : loss : 0.046375, loss_ce: 0.011767\n","iteration 10797 : loss : 0.055332, loss_ce: 0.011031\n","iteration 10798 : loss : 0.053334, loss_ce: 0.022466\n","iteration 10799 : loss : 0.042991, loss_ce: 0.014027\n","iteration 10800 : loss : 0.039046, loss_ce: 0.015370\n","iteration 10801 : loss : 0.048696, loss_ce: 0.021217\n","iteration 10802 : loss : 0.038044, loss_ce: 0.008943\n","iteration 10803 : loss : 0.039466, loss_ce: 0.013117\n","iteration 10804 : loss : 0.043093, loss_ce: 0.012678\n","iteration 10805 : loss : 0.031640, loss_ce: 0.005131\n","iteration 10806 : loss : 0.047973, loss_ce: 0.008266\n","iteration 10807 : loss : 0.098845, loss_ce: 0.008898\n","iteration 10808 : loss : 0.051925, loss_ce: 0.016528\n","iteration 10809 : loss : 0.044802, loss_ce: 0.013610\n","iteration 10810 : loss : 0.111504, loss_ce: 0.007087\n","iteration 10811 : loss : 0.054321, loss_ce: 0.010203\n","iteration 10812 : loss : 0.098052, loss_ce: 0.008972\n","iteration 10813 : loss : 0.038036, loss_ce: 0.012790\n","iteration 10814 : loss : 0.040610, loss_ce: 0.017198\n","iteration 10815 : loss : 0.219744, loss_ce: 0.009567\n","iteration 10816 : loss : 0.051038, loss_ce: 0.018227\n","iteration 10817 : loss : 0.038610, loss_ce: 0.016926\n","iteration 10818 : loss : 0.042993, loss_ce: 0.011301\n","iteration 10819 : loss : 0.101949, loss_ce: 0.010391\n","iteration 10820 : loss : 0.030679, loss_ce: 0.005870\n","iteration 10821 : loss : 0.035414, loss_ce: 0.011441\n","iteration 10822 : loss : 0.087975, loss_ce: 0.010685\n","iteration 10823 : loss : 0.047463, loss_ce: 0.017090\n","iteration 10824 : loss : 0.041066, loss_ce: 0.011603\n","iteration 10825 : loss : 0.039763, loss_ce: 0.009299\n","iteration 10826 : loss : 0.034992, loss_ce: 0.005779\n","iteration 10827 : loss : 0.031900, loss_ce: 0.013850\n","iteration 10828 : loss : 0.035711, loss_ce: 0.010860\n","iteration 10829 : loss : 0.048265, loss_ce: 0.017741\n","iteration 10830 : loss : 0.038154, loss_ce: 0.012368\n","iteration 10831 : loss : 0.046910, loss_ce: 0.009935\n","iteration 10832 : loss : 0.042615, loss_ce: 0.008892\n","iteration 10833 : loss : 0.125049, loss_ce: 0.007261\n","iteration 10834 : loss : 0.042388, loss_ce: 0.010469\n","iteration 10835 : loss : 0.039322, loss_ce: 0.017368\n","iteration 10836 : loss : 0.043442, loss_ce: 0.011741\n","iteration 10837 : loss : 0.040942, loss_ce: 0.019309\n","iteration 10838 : loss : 0.045588, loss_ce: 0.010665\n","iteration 10839 : loss : 0.047127, loss_ce: 0.020817\n","iteration 10840 : loss : 0.045075, loss_ce: 0.009059\n","iteration 10841 : loss : 0.044768, loss_ce: 0.010492\n","iteration 10842 : loss : 0.049614, loss_ce: 0.011470\n","iteration 10843 : loss : 0.039811, loss_ce: 0.010879\n","iteration 10844 : loss : 0.036392, loss_ce: 0.008631\n","iteration 10845 : loss : 0.041520, loss_ce: 0.012839\n","iteration 10846 : loss : 0.045501, loss_ce: 0.011743\n","iteration 10847 : loss : 0.039650, loss_ce: 0.011106\n","iteration 10848 : loss : 0.045115, loss_ce: 0.016857\n","iteration 10849 : loss : 0.041107, loss_ce: 0.011852\n","iteration 10850 : loss : 0.044564, loss_ce: 0.008490\n","iteration 10851 : loss : 0.039428, loss_ce: 0.011738\n","iteration 10852 : loss : 0.104501, loss_ce: 0.003986\n","iteration 10853 : loss : 0.072827, loss_ce: 0.012434\n","iteration 10854 : loss : 0.044559, loss_ce: 0.023344\n","iteration 10855 : loss : 0.050272, loss_ce: 0.012774\n","iteration 10856 : loss : 0.044040, loss_ce: 0.006816\n","iteration 10857 : loss : 0.052781, loss_ce: 0.017223\n","iteration 10858 : loss : 0.052946, loss_ce: 0.023782\n","iteration 10859 : loss : 0.044562, loss_ce: 0.013670\n","iteration 10860 : loss : 0.041190, loss_ce: 0.021293\n","iteration 10861 : loss : 0.035599, loss_ce: 0.010308\n","iteration 10862 : loss : 0.039744, loss_ce: 0.014903\n","iteration 10863 : loss : 0.038683, loss_ce: 0.012696\n","iteration 10864 : loss : 0.043972, loss_ce: 0.014379\n","iteration 10865 : loss : 0.042528, loss_ce: 0.016526\n","iteration 10866 : loss : 0.036564, loss_ce: 0.009399\n","iteration 10867 : loss : 0.044543, loss_ce: 0.015783\n","iteration 10868 : loss : 0.039173, loss_ce: 0.012700\n","iteration 10869 : loss : 0.046312, loss_ce: 0.014868\n","iteration 10870 : loss : 0.105822, loss_ce: 0.011567\n","iteration 10871 : loss : 0.036259, loss_ce: 0.006567\n","iteration 10872 : loss : 0.098947, loss_ce: 0.008249\n","iteration 10873 : loss : 0.042769, loss_ce: 0.012928\n","iteration 10874 : loss : 0.044352, loss_ce: 0.021132\n","iteration 10875 : loss : 0.041323, loss_ce: 0.014100\n","iteration 10876 : loss : 0.038605, loss_ce: 0.016877\n","iteration 10877 : loss : 0.037919, loss_ce: 0.013763\n","iteration 10878 : loss : 0.040056, loss_ce: 0.013859\n","iteration 10879 : loss : 0.029351, loss_ce: 0.005291\n","iteration 10880 : loss : 0.098844, loss_ce: 0.006700\n","iteration 10881 : loss : 0.117515, loss_ce: 0.018357\n"," 78%|██████████████████████▌      | 117/150 [2:17:39<38:32, 70.07s/it]iteration 10882 : loss : 0.040565, loss_ce: 0.008300\n","iteration 10883 : loss : 0.053427, loss_ce: 0.011443\n","iteration 10884 : loss : 0.044211, loss_ce: 0.011753\n","iteration 10885 : loss : 0.048190, loss_ce: 0.010966\n","iteration 10886 : loss : 0.047602, loss_ce: 0.010950\n","iteration 10887 : loss : 0.041899, loss_ce: 0.012439\n","iteration 10888 : loss : 0.038035, loss_ce: 0.015254\n","iteration 10889 : loss : 0.038786, loss_ce: 0.014498\n","iteration 10890 : loss : 0.040659, loss_ce: 0.010194\n","iteration 10891 : loss : 0.037287, loss_ce: 0.012488\n","iteration 10892 : loss : 0.040983, loss_ce: 0.016119\n","iteration 10893 : loss : 0.042813, loss_ce: 0.017032\n","iteration 10894 : loss : 0.045759, loss_ce: 0.014050\n","iteration 10895 : loss : 0.049908, loss_ce: 0.004631\n","iteration 10896 : loss : 0.041519, loss_ce: 0.012222\n","iteration 10897 : loss : 0.050995, loss_ce: 0.019953\n","iteration 10898 : loss : 0.040744, loss_ce: 0.009617\n","iteration 10899 : loss : 0.103879, loss_ce: 0.007477\n","iteration 10900 : loss : 0.032781, loss_ce: 0.010565\n","iteration 10901 : loss : 0.092043, loss_ce: 0.006899\n","iteration 10902 : loss : 0.030375, loss_ce: 0.007360\n","iteration 10903 : loss : 0.037526, loss_ce: 0.012620\n","iteration 10904 : loss : 0.051493, loss_ce: 0.017425\n","iteration 10905 : loss : 0.037041, loss_ce: 0.014140\n","iteration 10906 : loss : 0.048935, loss_ce: 0.016591\n","iteration 10907 : loss : 0.045548, loss_ce: 0.015413\n","iteration 10908 : loss : 0.037615, loss_ce: 0.010888\n","iteration 10909 : loss : 0.045007, loss_ce: 0.018959\n","iteration 10910 : loss : 0.041535, loss_ce: 0.006016\n","iteration 10911 : loss : 0.038927, loss_ce: 0.014484\n","iteration 10912 : loss : 0.036056, loss_ce: 0.010447\n","iteration 10913 : loss : 0.029310, loss_ce: 0.011002\n","iteration 10914 : loss : 0.041489, loss_ce: 0.012144\n","iteration 10915 : loss : 0.037712, loss_ce: 0.013895\n","iteration 10916 : loss : 0.036408, loss_ce: 0.012698\n","iteration 10917 : loss : 0.037634, loss_ce: 0.008288\n","iteration 10918 : loss : 0.042971, loss_ce: 0.010110\n","iteration 10919 : loss : 0.050031, loss_ce: 0.008658\n","iteration 10920 : loss : 0.109197, loss_ce: 0.011741\n","iteration 10921 : loss : 0.035768, loss_ce: 0.011494\n","iteration 10922 : loss : 0.034995, loss_ce: 0.009021\n","iteration 10923 : loss : 0.042002, loss_ce: 0.015908\n","iteration 10924 : loss : 0.039021, loss_ce: 0.015234\n","iteration 10925 : loss : 0.049492, loss_ce: 0.016091\n","iteration 10926 : loss : 0.033034, loss_ce: 0.009737\n","iteration 10927 : loss : 0.038640, loss_ce: 0.014802\n","iteration 10928 : loss : 0.037238, loss_ce: 0.012385\n","iteration 10929 : loss : 0.050938, loss_ce: 0.015521\n","iteration 10930 : loss : 0.041742, loss_ce: 0.011809\n","iteration 10931 : loss : 0.040906, loss_ce: 0.013611\n","iteration 10932 : loss : 0.039047, loss_ce: 0.009422\n","iteration 10933 : loss : 0.036301, loss_ce: 0.014301\n","iteration 10934 : loss : 0.033387, loss_ce: 0.011377\n","iteration 10935 : loss : 0.037538, loss_ce: 0.011897\n","iteration 10936 : loss : 0.031676, loss_ce: 0.011164\n","iteration 10937 : loss : 0.036414, loss_ce: 0.006615\n","iteration 10938 : loss : 0.050847, loss_ce: 0.011451\n","iteration 10939 : loss : 0.032417, loss_ce: 0.010910\n","iteration 10940 : loss : 0.036837, loss_ce: 0.011107\n","iteration 10941 : loss : 0.036544, loss_ce: 0.011823\n","iteration 10942 : loss : 0.096151, loss_ce: 0.005658\n","iteration 10943 : loss : 0.049052, loss_ce: 0.013164\n","iteration 10944 : loss : 0.092560, loss_ce: 0.006649\n","iteration 10945 : loss : 0.035895, loss_ce: 0.012829\n","iteration 10946 : loss : 0.044456, loss_ce: 0.015628\n","iteration 10947 : loss : 0.102686, loss_ce: 0.010761\n","iteration 10948 : loss : 0.044893, loss_ce: 0.010510\n","iteration 10949 : loss : 0.034834, loss_ce: 0.012595\n","iteration 10950 : loss : 0.045025, loss_ce: 0.013995\n","iteration 10951 : loss : 0.041354, loss_ce: 0.014673\n","iteration 10952 : loss : 0.040472, loss_ce: 0.014466\n","iteration 10953 : loss : 0.037554, loss_ce: 0.005728\n","iteration 10954 : loss : 0.042050, loss_ce: 0.017206\n","iteration 10955 : loss : 0.049848, loss_ce: 0.021243\n","iteration 10956 : loss : 0.041268, loss_ce: 0.014780\n","iteration 10957 : loss : 0.043430, loss_ce: 0.014490\n","iteration 10958 : loss : 0.037020, loss_ce: 0.013345\n","iteration 10959 : loss : 0.051701, loss_ce: 0.020174\n","iteration 10960 : loss : 0.038737, loss_ce: 0.011818\n","iteration 10961 : loss : 0.038704, loss_ce: 0.008232\n","iteration 10962 : loss : 0.083245, loss_ce: 0.014091\n","iteration 10963 : loss : 0.099318, loss_ce: 0.007166\n","iteration 10964 : loss : 0.032076, loss_ce: 0.009092\n","iteration 10965 : loss : 0.045230, loss_ce: 0.013372\n","iteration 10966 : loss : 0.049938, loss_ce: 0.013937\n","iteration 10967 : loss : 0.038598, loss_ce: 0.012086\n","iteration 10968 : loss : 0.035141, loss_ce: 0.013396\n","iteration 10969 : loss : 0.043970, loss_ce: 0.011713\n","iteration 10970 : loss : 0.099701, loss_ce: 0.011880\n","iteration 10971 : loss : 0.064512, loss_ce: 0.011992\n","iteration 10972 : loss : 0.040724, loss_ce: 0.017647\n","iteration 10973 : loss : 0.038086, loss_ce: 0.012147\n","iteration 10974 : loss : 0.395339, loss_ce: 0.001575\n"," 79%|██████████████████████▊      | 118/150 [2:18:49<37:22, 70.07s/it]iteration 10975 : loss : 0.052029, loss_ce: 0.016913\n","iteration 10976 : loss : 0.061059, loss_ce: 0.012493\n","iteration 10977 : loss : 0.077573, loss_ce: 0.025168\n","iteration 10978 : loss : 0.081781, loss_ce: 0.025292\n","iteration 10979 : loss : 0.071004, loss_ce: 0.018932\n","iteration 10980 : loss : 0.060650, loss_ce: 0.018163\n","iteration 10981 : loss : 0.082157, loss_ce: 0.021097\n","iteration 10982 : loss : 0.055326, loss_ce: 0.012642\n","iteration 10983 : loss : 0.067043, loss_ce: 0.016607\n","iteration 10984 : loss : 0.044827, loss_ce: 0.018792\n","iteration 10985 : loss : 0.048463, loss_ce: 0.020342\n","iteration 10986 : loss : 0.051444, loss_ce: 0.021114\n","iteration 10987 : loss : 0.079882, loss_ce: 0.031486\n","iteration 10988 : loss : 0.106826, loss_ce: 0.019606\n","iteration 10989 : loss : 0.067455, loss_ce: 0.026189\n","iteration 10990 : loss : 0.056156, loss_ce: 0.016069\n","iteration 10991 : loss : 0.065508, loss_ce: 0.015230\n","iteration 10992 : loss : 0.062725, loss_ce: 0.020661\n","iteration 10993 : loss : 0.054539, loss_ce: 0.012146\n","iteration 10994 : loss : 0.044073, loss_ce: 0.008494\n","iteration 10995 : loss : 0.067424, loss_ce: 0.013193\n","iteration 10996 : loss : 0.073428, loss_ce: 0.012801\n","iteration 10997 : loss : 0.048575, loss_ce: 0.010722\n","iteration 10998 : loss : 0.054114, loss_ce: 0.021384\n","iteration 10999 : loss : 0.055188, loss_ce: 0.021228\n","iteration 11000 : loss : 0.037840, loss_ce: 0.008720\n","iteration 11001 : loss : 0.116562, loss_ce: 0.016765\n","iteration 11002 : loss : 0.054802, loss_ce: 0.022824\n","iteration 11003 : loss : 0.052661, loss_ce: 0.015186\n","iteration 11004 : loss : 0.045761, loss_ce: 0.013653\n","iteration 11005 : loss : 0.048077, loss_ce: 0.010950\n","iteration 11006 : loss : 0.048200, loss_ce: 0.008778\n","iteration 11007 : loss : 0.103324, loss_ce: 0.020207\n","iteration 11008 : loss : 0.054419, loss_ce: 0.016347\n","iteration 11009 : loss : 0.112815, loss_ce: 0.009282\n","iteration 11010 : loss : 0.095796, loss_ce: 0.017565\n","iteration 11011 : loss : 0.053258, loss_ce: 0.013955\n","iteration 11012 : loss : 0.037885, loss_ce: 0.007241\n","iteration 11013 : loss : 0.050889, loss_ce: 0.014346\n","iteration 11014 : loss : 0.114300, loss_ce: 0.015153\n","iteration 11015 : loss : 0.048716, loss_ce: 0.012138\n","iteration 11016 : loss : 0.102985, loss_ce: 0.012076\n","iteration 11017 : loss : 0.038059, loss_ce: 0.009873\n","iteration 11018 : loss : 0.044900, loss_ce: 0.024070\n","iteration 11019 : loss : 0.045320, loss_ce: 0.015302\n","iteration 11020 : loss : 0.044387, loss_ce: 0.015505\n","iteration 11021 : loss : 0.043050, loss_ce: 0.013420\n","iteration 11022 : loss : 0.042517, loss_ce: 0.011132\n","iteration 11023 : loss : 0.048436, loss_ce: 0.022011\n","iteration 11024 : loss : 0.048449, loss_ce: 0.017587\n","iteration 11025 : loss : 0.060442, loss_ce: 0.015632\n","iteration 11026 : loss : 0.052287, loss_ce: 0.006890\n","iteration 11027 : loss : 0.045289, loss_ce: 0.017598\n","iteration 11028 : loss : 0.039592, loss_ce: 0.011503\n","iteration 11029 : loss : 0.049932, loss_ce: 0.014438\n","iteration 11030 : loss : 0.049604, loss_ce: 0.018928\n","iteration 11031 : loss : 0.064202, loss_ce: 0.017181\n","iteration 11032 : loss : 0.039578, loss_ce: 0.011822\n","iteration 11033 : loss : 0.051891, loss_ce: 0.013782\n","iteration 11034 : loss : 0.049711, loss_ce: 0.013241\n","iteration 11035 : loss : 0.041562, loss_ce: 0.011934\n","iteration 11036 : loss : 0.044717, loss_ce: 0.008363\n","iteration 11037 : loss : 0.041129, loss_ce: 0.012927\n","iteration 11038 : loss : 0.068165, loss_ce: 0.016143\n","iteration 11039 : loss : 0.043551, loss_ce: 0.015033\n","iteration 11040 : loss : 0.045296, loss_ce: 0.013596\n","iteration 11041 : loss : 0.038292, loss_ce: 0.012344\n","iteration 11042 : loss : 0.051199, loss_ce: 0.024211\n","iteration 11043 : loss : 0.041814, loss_ce: 0.014006\n","iteration 11044 : loss : 0.041729, loss_ce: 0.014380\n","iteration 11045 : loss : 0.039962, loss_ce: 0.017186\n","iteration 11046 : loss : 0.034273, loss_ce: 0.012557\n","iteration 11047 : loss : 0.041081, loss_ce: 0.009260\n","iteration 11048 : loss : 0.054606, loss_ce: 0.015008\n","iteration 11049 : loss : 0.043953, loss_ce: 0.014627\n","iteration 11050 : loss : 0.046165, loss_ce: 0.020394\n","iteration 11051 : loss : 0.043498, loss_ce: 0.011694\n","iteration 11052 : loss : 0.047952, loss_ce: 0.018467\n","iteration 11053 : loss : 0.035908, loss_ce: 0.010807\n","iteration 11054 : loss : 0.044722, loss_ce: 0.010292\n","iteration 11055 : loss : 0.046310, loss_ce: 0.020435\n","iteration 11056 : loss : 0.046458, loss_ce: 0.019663\n","iteration 11057 : loss : 0.040801, loss_ce: 0.009971\n","iteration 11058 : loss : 0.041518, loss_ce: 0.017739\n","iteration 11059 : loss : 0.049170, loss_ce: 0.010387\n","iteration 11060 : loss : 0.047963, loss_ce: 0.008657\n","iteration 11061 : loss : 0.038651, loss_ce: 0.011574\n","iteration 11062 : loss : 0.101090, loss_ce: 0.009566\n","iteration 11063 : loss : 0.038248, loss_ce: 0.010751\n","iteration 11064 : loss : 0.046796, loss_ce: 0.013844\n","iteration 11065 : loss : 0.036118, loss_ce: 0.012796\n","iteration 11066 : loss : 0.059809, loss_ce: 0.011590\n","iteration 11067 : loss : 0.269462, loss_ce: 0.004914\n"," 79%|███████████████████████      | 119/150 [2:19:59<36:15, 70.18s/it]iteration 11068 : loss : 0.040809, loss_ce: 0.015111\n","iteration 11069 : loss : 0.042359, loss_ce: 0.017179\n","iteration 11070 : loss : 0.045629, loss_ce: 0.016911\n","iteration 11071 : loss : 0.041048, loss_ce: 0.010671\n","iteration 11072 : loss : 0.043700, loss_ce: 0.013924\n","iteration 11073 : loss : 0.047224, loss_ce: 0.015137\n","iteration 11074 : loss : 0.050768, loss_ce: 0.016282\n","iteration 11075 : loss : 0.043782, loss_ce: 0.012345\n","iteration 11076 : loss : 0.038537, loss_ce: 0.007993\n","iteration 11077 : loss : 0.038787, loss_ce: 0.006862\n","iteration 11078 : loss : 0.041681, loss_ce: 0.006664\n","iteration 11079 : loss : 0.045212, loss_ce: 0.023126\n","iteration 11080 : loss : 0.042844, loss_ce: 0.016662\n","iteration 11081 : loss : 0.045037, loss_ce: 0.010661\n","iteration 11082 : loss : 0.053203, loss_ce: 0.014972\n","iteration 11083 : loss : 0.035779, loss_ce: 0.013803\n","iteration 11084 : loss : 0.044666, loss_ce: 0.008941\n","iteration 11085 : loss : 0.039580, loss_ce: 0.011806\n","iteration 11086 : loss : 0.042374, loss_ce: 0.015982\n","iteration 11087 : loss : 0.051945, loss_ce: 0.018376\n","iteration 11088 : loss : 0.050575, loss_ce: 0.014044\n","iteration 11089 : loss : 0.052103, loss_ce: 0.008278\n","iteration 11090 : loss : 0.055033, loss_ce: 0.012742\n","iteration 11091 : loss : 0.041110, loss_ce: 0.015577\n","iteration 11092 : loss : 0.045260, loss_ce: 0.011948\n","iteration 11093 : loss : 0.041116, loss_ce: 0.009070\n","iteration 11094 : loss : 0.041811, loss_ce: 0.011078\n","iteration 11095 : loss : 0.033576, loss_ce: 0.013438\n","iteration 11096 : loss : 0.038772, loss_ce: 0.019472\n","iteration 11097 : loss : 0.040044, loss_ce: 0.006034\n","iteration 11098 : loss : 0.034343, loss_ce: 0.010953\n","iteration 11099 : loss : 0.100849, loss_ce: 0.017497\n","iteration 11100 : loss : 0.038742, loss_ce: 0.013005\n","iteration 11101 : loss : 0.038302, loss_ce: 0.010734\n","iteration 11102 : loss : 0.041043, loss_ce: 0.008834\n","iteration 11103 : loss : 0.057092, loss_ce: 0.013442\n","iteration 11104 : loss : 0.045606, loss_ce: 0.014157\n","iteration 11105 : loss : 0.106107, loss_ce: 0.007310\n","iteration 11106 : loss : 0.059828, loss_ce: 0.011587\n","iteration 11107 : loss : 0.063211, loss_ce: 0.009126\n","iteration 11108 : loss : 0.040883, loss_ce: 0.014051\n","iteration 11109 : loss : 0.040945, loss_ce: 0.011308\n","iteration 11110 : loss : 0.059036, loss_ce: 0.012356\n","iteration 11111 : loss : 0.048718, loss_ce: 0.014102\n","iteration 11112 : loss : 0.036861, loss_ce: 0.016084\n","iteration 11113 : loss : 0.037797, loss_ce: 0.011617\n","iteration 11114 : loss : 0.050341, loss_ce: 0.014055\n","iteration 11115 : loss : 0.033285, loss_ce: 0.014891\n","iteration 11116 : loss : 0.048745, loss_ce: 0.014286\n","iteration 11117 : loss : 0.040923, loss_ce: 0.013084\n","iteration 11118 : loss : 0.053226, loss_ce: 0.016350\n","iteration 11119 : loss : 0.044871, loss_ce: 0.011822\n","iteration 11120 : loss : 0.044166, loss_ce: 0.015780\n","iteration 11121 : loss : 0.049484, loss_ce: 0.008120\n","iteration 11122 : loss : 0.039249, loss_ce: 0.013206\n","iteration 11123 : loss : 0.042364, loss_ce: 0.016000\n","iteration 11124 : loss : 0.038133, loss_ce: 0.013044\n","iteration 11125 : loss : 0.043998, loss_ce: 0.013050\n","iteration 11126 : loss : 0.046703, loss_ce: 0.016812\n","iteration 11127 : loss : 0.049256, loss_ce: 0.012431\n","iteration 11128 : loss : 0.045881, loss_ce: 0.016919\n","iteration 11129 : loss : 0.048879, loss_ce: 0.013906\n","iteration 11130 : loss : 0.040466, loss_ce: 0.013530\n","iteration 11131 : loss : 0.043307, loss_ce: 0.012492\n","iteration 11132 : loss : 0.044057, loss_ce: 0.010083\n","iteration 11133 : loss : 0.047133, loss_ce: 0.015720\n","iteration 11134 : loss : 0.038201, loss_ce: 0.012953\n","iteration 11135 : loss : 0.157425, loss_ce: 0.011508\n","iteration 11136 : loss : 0.043967, loss_ce: 0.009356\n","iteration 11137 : loss : 0.047187, loss_ce: 0.015405\n","iteration 11138 : loss : 0.043112, loss_ce: 0.011476\n","iteration 11139 : loss : 0.034695, loss_ce: 0.008879\n","iteration 11140 : loss : 0.038443, loss_ce: 0.011172\n","iteration 11141 : loss : 0.112617, loss_ce: 0.014239\n","iteration 11142 : loss : 0.039155, loss_ce: 0.008715\n","iteration 11143 : loss : 0.054323, loss_ce: 0.007387\n","iteration 11144 : loss : 0.037080, loss_ce: 0.014886\n","iteration 11145 : loss : 0.040713, loss_ce: 0.009975\n","iteration 11146 : loss : 0.031940, loss_ce: 0.013298\n","iteration 11147 : loss : 0.037568, loss_ce: 0.012139\n","iteration 11148 : loss : 0.037776, loss_ce: 0.013133\n","iteration 11149 : loss : 0.052008, loss_ce: 0.014734\n","iteration 11150 : loss : 0.038922, loss_ce: 0.016921\n","iteration 11151 : loss : 0.044974, loss_ce: 0.017078\n","iteration 11152 : loss : 0.107650, loss_ce: 0.013628\n","iteration 11153 : loss : 0.093625, loss_ce: 0.011482\n","iteration 11154 : loss : 0.039723, loss_ce: 0.013613\n","iteration 11155 : loss : 0.035800, loss_ce: 0.011666\n","iteration 11156 : loss : 0.036065, loss_ce: 0.010839\n","iteration 11157 : loss : 0.039692, loss_ce: 0.009945\n","iteration 11158 : loss : 0.040239, loss_ce: 0.011957\n","iteration 11159 : loss : 0.039131, loss_ce: 0.012329\n","iteration 11160 : loss : 0.109769, loss_ce: 0.012256\n"," 80%|███████████████████████▏     | 120/150 [2:21:09<35:04, 70.14s/it]iteration 11161 : loss : 0.053665, loss_ce: 0.015823\n","iteration 11162 : loss : 0.050310, loss_ce: 0.013258\n","iteration 11163 : loss : 0.037308, loss_ce: 0.008693\n","iteration 11164 : loss : 0.045749, loss_ce: 0.022177\n","iteration 11165 : loss : 0.043028, loss_ce: 0.011859\n","iteration 11166 : loss : 0.042713, loss_ce: 0.011419\n","iteration 11167 : loss : 0.046030, loss_ce: 0.005983\n","iteration 11168 : loss : 0.046108, loss_ce: 0.008971\n","iteration 11169 : loss : 0.048085, loss_ce: 0.011093\n","iteration 11170 : loss : 0.049771, loss_ce: 0.010661\n","iteration 11171 : loss : 0.099218, loss_ce: 0.011161\n","iteration 11172 : loss : 0.037914, loss_ce: 0.017494\n","iteration 11173 : loss : 0.047639, loss_ce: 0.013879\n","iteration 11174 : loss : 0.041267, loss_ce: 0.007850\n","iteration 11175 : loss : 0.032358, loss_ce: 0.013781\n","iteration 11176 : loss : 0.054951, loss_ce: 0.016239\n","iteration 11177 : loss : 0.044365, loss_ce: 0.008195\n","iteration 11178 : loss : 0.034715, loss_ce: 0.005504\n","iteration 11179 : loss : 0.043805, loss_ce: 0.016152\n","iteration 11180 : loss : 0.036381, loss_ce: 0.015148\n","iteration 11181 : loss : 0.097676, loss_ce: 0.007459\n","iteration 11182 : loss : 0.039153, loss_ce: 0.009654\n","iteration 11183 : loss : 0.051323, loss_ce: 0.013355\n","iteration 11184 : loss : 0.039553, loss_ce: 0.012199\n","iteration 11185 : loss : 0.035918, loss_ce: 0.012496\n","iteration 11186 : loss : 0.041306, loss_ce: 0.011720\n","iteration 11187 : loss : 0.042724, loss_ce: 0.014998\n","iteration 11188 : loss : 0.098266, loss_ce: 0.004635\n","iteration 11189 : loss : 0.035935, loss_ce: 0.012044\n","iteration 11190 : loss : 0.035967, loss_ce: 0.008309\n","iteration 11191 : loss : 0.038303, loss_ce: 0.010787\n","iteration 11192 : loss : 0.095953, loss_ce: 0.012609\n","iteration 11193 : loss : 0.033196, loss_ce: 0.008898\n","iteration 11194 : loss : 0.101722, loss_ce: 0.007916\n","iteration 11195 : loss : 0.034096, loss_ce: 0.011273\n","iteration 11196 : loss : 0.051035, loss_ce: 0.011514\n","iteration 11197 : loss : 0.052063, loss_ce: 0.013832\n","iteration 11198 : loss : 0.044387, loss_ce: 0.008305\n","iteration 11199 : loss : 0.052429, loss_ce: 0.013808\n","iteration 11200 : loss : 0.041884, loss_ce: 0.012486\n","iteration 11201 : loss : 0.047977, loss_ce: 0.017148\n","iteration 11202 : loss : 0.034431, loss_ce: 0.008456\n","iteration 11203 : loss : 0.035275, loss_ce: 0.011558\n","iteration 11204 : loss : 0.052690, loss_ce: 0.019636\n","iteration 11205 : loss : 0.042701, loss_ce: 0.020647\n","iteration 11206 : loss : 0.047688, loss_ce: 0.014923\n","iteration 11207 : loss : 0.045168, loss_ce: 0.019602\n","iteration 11208 : loss : 0.035282, loss_ce: 0.012273\n","iteration 11209 : loss : 0.046086, loss_ce: 0.014344\n","iteration 11210 : loss : 0.050976, loss_ce: 0.018582\n","iteration 11211 : loss : 0.040364, loss_ce: 0.007617\n","iteration 11212 : loss : 0.036615, loss_ce: 0.011511\n","iteration 11213 : loss : 0.097685, loss_ce: 0.012643\n","iteration 11214 : loss : 0.037468, loss_ce: 0.006641\n","iteration 11215 : loss : 0.098294, loss_ce: 0.011479\n","iteration 11216 : loss : 0.046534, loss_ce: 0.011744\n","iteration 11217 : loss : 0.041582, loss_ce: 0.014719\n","iteration 11218 : loss : 0.098042, loss_ce: 0.008342\n","iteration 11219 : loss : 0.042991, loss_ce: 0.011450\n","iteration 11220 : loss : 0.049849, loss_ce: 0.014590\n","iteration 11221 : loss : 0.048082, loss_ce: 0.014185\n","iteration 11222 : loss : 0.045555, loss_ce: 0.009284\n","iteration 11223 : loss : 0.044114, loss_ce: 0.015155\n","iteration 11224 : loss : 0.041114, loss_ce: 0.014947\n","iteration 11225 : loss : 0.044077, loss_ce: 0.012305\n","iteration 11226 : loss : 0.052917, loss_ce: 0.017693\n","iteration 11227 : loss : 0.043667, loss_ce: 0.014715\n","iteration 11228 : loss : 0.037601, loss_ce: 0.009370\n","iteration 11229 : loss : 0.036451, loss_ce: 0.012921\n","iteration 11230 : loss : 0.097686, loss_ce: 0.006223\n","iteration 11231 : loss : 0.034137, loss_ce: 0.013556\n","iteration 11232 : loss : 0.041431, loss_ce: 0.020843\n","iteration 11233 : loss : 0.035873, loss_ce: 0.013471\n","iteration 11234 : loss : 0.037883, loss_ce: 0.011180\n","iteration 11235 : loss : 0.048754, loss_ce: 0.010081\n","iteration 11236 : loss : 0.043193, loss_ce: 0.014885\n","iteration 11237 : loss : 0.033207, loss_ce: 0.009261\n","iteration 11238 : loss : 0.043111, loss_ce: 0.014589\n","iteration 11239 : loss : 0.099579, loss_ce: 0.012578\n","iteration 11240 : loss : 0.037331, loss_ce: 0.011045\n","iteration 11241 : loss : 0.099970, loss_ce: 0.010950\n","iteration 11242 : loss : 0.046057, loss_ce: 0.018727\n","iteration 11243 : loss : 0.040673, loss_ce: 0.009567\n","iteration 11244 : loss : 0.041302, loss_ce: 0.019986\n","iteration 11245 : loss : 0.043692, loss_ce: 0.007636\n","iteration 11246 : loss : 0.055197, loss_ce: 0.015523\n","iteration 11247 : loss : 0.037867, loss_ce: 0.007342\n","iteration 11248 : loss : 0.039328, loss_ce: 0.013949\n","iteration 11249 : loss : 0.101214, loss_ce: 0.006981\n","iteration 11250 : loss : 0.041707, loss_ce: 0.019688\n","iteration 11251 : loss : 0.038284, loss_ce: 0.014706\n","iteration 11252 : loss : 0.047016, loss_ce: 0.011130\n","iteration 11253 : loss : 0.216089, loss_ce: 0.004850\n"," 81%|███████████████████████▍     | 121/150 [2:22:19<33:52, 70.10s/it]iteration 11254 : loss : 0.066803, loss_ce: 0.009168\n","iteration 11255 : loss : 0.099531, loss_ce: 0.012267\n","iteration 11256 : loss : 0.038259, loss_ce: 0.013275\n","iteration 11257 : loss : 0.044165, loss_ce: 0.011980\n","iteration 11258 : loss : 0.042570, loss_ce: 0.012552\n","iteration 11259 : loss : 0.028982, loss_ce: 0.009428\n","iteration 11260 : loss : 0.043999, loss_ce: 0.015862\n","iteration 11261 : loss : 0.048776, loss_ce: 0.015142\n","iteration 11262 : loss : 0.033975, loss_ce: 0.010081\n","iteration 11263 : loss : 0.031792, loss_ce: 0.007698\n","iteration 11264 : loss : 0.038939, loss_ce: 0.013763\n","iteration 11265 : loss : 0.035555, loss_ce: 0.013035\n","iteration 11266 : loss : 0.031086, loss_ce: 0.009479\n","iteration 11267 : loss : 0.040030, loss_ce: 0.011554\n","iteration 11268 : loss : 0.042794, loss_ce: 0.011855\n","iteration 11269 : loss : 0.045682, loss_ce: 0.015321\n","iteration 11270 : loss : 0.034966, loss_ce: 0.010117\n","iteration 11271 : loss : 0.041001, loss_ce: 0.020040\n","iteration 11272 : loss : 0.035114, loss_ce: 0.006890\n","iteration 11273 : loss : 0.031914, loss_ce: 0.013438\n","iteration 11274 : loss : 0.036538, loss_ce: 0.013741\n","iteration 11275 : loss : 0.034292, loss_ce: 0.010379\n","iteration 11276 : loss : 0.044017, loss_ce: 0.011216\n","iteration 11277 : loss : 0.036808, loss_ce: 0.007890\n","iteration 11278 : loss : 0.040356, loss_ce: 0.010722\n","iteration 11279 : loss : 0.037027, loss_ce: 0.013925\n","iteration 11280 : loss : 0.035972, loss_ce: 0.012941\n","iteration 11281 : loss : 0.044873, loss_ce: 0.015618\n","iteration 11282 : loss : 0.038373, loss_ce: 0.012162\n","iteration 11283 : loss : 0.043760, loss_ce: 0.013194\n","iteration 11284 : loss : 0.033168, loss_ce: 0.012930\n","iteration 11285 : loss : 0.049439, loss_ce: 0.013301\n","iteration 11286 : loss : 0.045483, loss_ce: 0.014604\n","iteration 11287 : loss : 0.098627, loss_ce: 0.008668\n","iteration 11288 : loss : 0.037039, loss_ce: 0.010602\n","iteration 11289 : loss : 0.039240, loss_ce: 0.018040\n","iteration 11290 : loss : 0.033817, loss_ce: 0.010369\n","iteration 11291 : loss : 0.045141, loss_ce: 0.015851\n","iteration 11292 : loss : 0.050463, loss_ce: 0.020782\n","iteration 11293 : loss : 0.035890, loss_ce: 0.011772\n","iteration 11294 : loss : 0.029516, loss_ce: 0.010657\n","iteration 11295 : loss : 0.046012, loss_ce: 0.015654\n","iteration 11296 : loss : 0.049056, loss_ce: 0.019548\n","iteration 11297 : loss : 0.060242, loss_ce: 0.009050\n","iteration 11298 : loss : 0.052229, loss_ce: 0.012782\n","iteration 11299 : loss : 0.043091, loss_ce: 0.012565\n","iteration 11300 : loss : 0.047762, loss_ce: 0.008872\n","iteration 11301 : loss : 0.039334, loss_ce: 0.012738\n","iteration 11302 : loss : 0.041561, loss_ce: 0.017256\n","iteration 11303 : loss : 0.036471, loss_ce: 0.014143\n","iteration 11304 : loss : 0.102067, loss_ce: 0.009605\n","iteration 11305 : loss : 0.037987, loss_ce: 0.013015\n","iteration 11306 : loss : 0.036394, loss_ce: 0.009763\n","iteration 11307 : loss : 0.040645, loss_ce: 0.009608\n","iteration 11308 : loss : 0.094803, loss_ce: 0.008437\n","iteration 11309 : loss : 0.039225, loss_ce: 0.008868\n","iteration 11310 : loss : 0.044454, loss_ce: 0.012126\n","iteration 11311 : loss : 0.057021, loss_ce: 0.010758\n","iteration 11312 : loss : 0.108722, loss_ce: 0.006714\n","iteration 11313 : loss : 0.043378, loss_ce: 0.011979\n","iteration 11314 : loss : 0.040333, loss_ce: 0.010469\n","iteration 11315 : loss : 0.036106, loss_ce: 0.009751\n","iteration 11316 : loss : 0.035297, loss_ce: 0.014440\n","iteration 11317 : loss : 0.094465, loss_ce: 0.006574\n","iteration 11318 : loss : 0.042180, loss_ce: 0.009818\n","iteration 11319 : loss : 0.040702, loss_ce: 0.018640\n","iteration 11320 : loss : 0.044075, loss_ce: 0.013976\n","iteration 11321 : loss : 0.040680, loss_ce: 0.019669\n","iteration 11322 : loss : 0.041637, loss_ce: 0.012466\n","iteration 11323 : loss : 0.050403, loss_ce: 0.010485\n","iteration 11324 : loss : 0.055799, loss_ce: 0.007437\n","iteration 11325 : loss : 0.038449, loss_ce: 0.009267\n","iteration 11326 : loss : 0.033480, loss_ce: 0.009880\n","iteration 11327 : loss : 0.039212, loss_ce: 0.018631\n","iteration 11328 : loss : 0.043234, loss_ce: 0.009496\n","iteration 11329 : loss : 0.154137, loss_ce: 0.012473\n","iteration 11330 : loss : 0.046622, loss_ce: 0.011317\n","iteration 11331 : loss : 0.046672, loss_ce: 0.013261\n","iteration 11332 : loss : 0.056353, loss_ce: 0.014552\n","iteration 11333 : loss : 0.045175, loss_ce: 0.010662\n","iteration 11334 : loss : 0.033272, loss_ce: 0.013042\n","iteration 11335 : loss : 0.047902, loss_ce: 0.013427\n","iteration 11336 : loss : 0.038279, loss_ce: 0.010613\n","iteration 11337 : loss : 0.043280, loss_ce: 0.015973\n","iteration 11338 : loss : 0.038610, loss_ce: 0.015963\n","iteration 11339 : loss : 0.041135, loss_ce: 0.015055\n","iteration 11340 : loss : 0.036983, loss_ce: 0.010578\n","iteration 11341 : loss : 0.027232, loss_ce: 0.007317\n","iteration 11342 : loss : 0.044013, loss_ce: 0.018006\n","iteration 11343 : loss : 0.040191, loss_ce: 0.012527\n","iteration 11344 : loss : 0.102153, loss_ce: 0.006116\n","iteration 11345 : loss : 0.033180, loss_ce: 0.007567\n","iteration 11346 : loss : 0.114677, loss_ce: 0.014384\n"," 81%|███████████████████████▌     | 122/150 [2:23:29<32:41, 70.06s/it]iteration 11347 : loss : 0.038785, loss_ce: 0.016022\n","iteration 11348 : loss : 0.040890, loss_ce: 0.011924\n","iteration 11349 : loss : 0.037529, loss_ce: 0.013581\n","iteration 11350 : loss : 0.036463, loss_ce: 0.012763\n","iteration 11351 : loss : 0.041066, loss_ce: 0.014566\n","iteration 11352 : loss : 0.042423, loss_ce: 0.014047\n","iteration 11353 : loss : 0.037749, loss_ce: 0.007520\n","iteration 11354 : loss : 0.040350, loss_ce: 0.006164\n","iteration 11355 : loss : 0.038459, loss_ce: 0.012428\n","iteration 11356 : loss : 0.097176, loss_ce: 0.010985\n","iteration 11357 : loss : 0.037793, loss_ce: 0.009392\n","iteration 11358 : loss : 0.034133, loss_ce: 0.019247\n","iteration 11359 : loss : 0.099580, loss_ce: 0.009543\n","iteration 11360 : loss : 0.038474, loss_ce: 0.012555\n","iteration 11361 : loss : 0.040175, loss_ce: 0.014986\n","iteration 11362 : loss : 0.038770, loss_ce: 0.008591\n","iteration 11363 : loss : 0.056796, loss_ce: 0.018459\n","iteration 11364 : loss : 0.040152, loss_ce: 0.014501\n","iteration 11365 : loss : 0.052793, loss_ce: 0.010870\n","iteration 11366 : loss : 0.027535, loss_ce: 0.004592\n","iteration 11367 : loss : 0.041300, loss_ce: 0.010946\n","iteration 11368 : loss : 0.037560, loss_ce: 0.013932\n","iteration 11369 : loss : 0.035797, loss_ce: 0.012352\n","iteration 11370 : loss : 0.034125, loss_ce: 0.013562\n","iteration 11371 : loss : 0.036359, loss_ce: 0.009849\n","iteration 11372 : loss : 0.040742, loss_ce: 0.012673\n","iteration 11373 : loss : 0.047130, loss_ce: 0.014569\n","iteration 11374 : loss : 0.036363, loss_ce: 0.008947\n","iteration 11375 : loss : 0.034145, loss_ce: 0.014304\n","iteration 11376 : loss : 0.041586, loss_ce: 0.011311\n","iteration 11377 : loss : 0.045512, loss_ce: 0.008929\n","iteration 11378 : loss : 0.036223, loss_ce: 0.012577\n","iteration 11379 : loss : 0.032910, loss_ce: 0.013202\n","iteration 11380 : loss : 0.041032, loss_ce: 0.015109\n","iteration 11381 : loss : 0.037230, loss_ce: 0.010682\n","iteration 11382 : loss : 0.093071, loss_ce: 0.011460\n","iteration 11383 : loss : 0.039863, loss_ce: 0.009631\n","iteration 11384 : loss : 0.047859, loss_ce: 0.011936\n","iteration 11385 : loss : 0.040951, loss_ce: 0.009385\n","iteration 11386 : loss : 0.100966, loss_ce: 0.016645\n","iteration 11387 : loss : 0.048163, loss_ce: 0.012838\n","iteration 11388 : loss : 0.036939, loss_ce: 0.015732\n","iteration 11389 : loss : 0.057491, loss_ce: 0.011762\n","iteration 11390 : loss : 0.047505, loss_ce: 0.020372\n","iteration 11391 : loss : 0.032417, loss_ce: 0.016837\n","iteration 11392 : loss : 0.042150, loss_ce: 0.012097\n","iteration 11393 : loss : 0.044705, loss_ce: 0.013199\n","iteration 11394 : loss : 0.049499, loss_ce: 0.011297\n","iteration 11395 : loss : 0.038996, loss_ce: 0.014287\n","iteration 11396 : loss : 0.034089, loss_ce: 0.007051\n","iteration 11397 : loss : 0.042277, loss_ce: 0.013746\n","iteration 11398 : loss : 0.035921, loss_ce: 0.008117\n","iteration 11399 : loss : 0.035757, loss_ce: 0.008712\n","iteration 11400 : loss : 0.036315, loss_ce: 0.009638\n","iteration 11401 : loss : 0.036278, loss_ce: 0.009280\n","iteration 11402 : loss : 0.064482, loss_ce: 0.012712\n","iteration 11403 : loss : 0.034471, loss_ce: 0.010183\n","iteration 11404 : loss : 0.037144, loss_ce: 0.012716\n","iteration 11405 : loss : 0.100273, loss_ce: 0.004928\n","iteration 11406 : loss : 0.036109, loss_ce: 0.011444\n","iteration 11407 : loss : 0.102706, loss_ce: 0.006029\n","iteration 11408 : loss : 0.033905, loss_ce: 0.011866\n","iteration 11409 : loss : 0.039780, loss_ce: 0.012360\n","iteration 11410 : loss : 0.045058, loss_ce: 0.010118\n","iteration 11411 : loss : 0.041292, loss_ce: 0.017200\n","iteration 11412 : loss : 0.044858, loss_ce: 0.016228\n","iteration 11413 : loss : 0.040447, loss_ce: 0.014820\n","iteration 11414 : loss : 0.036314, loss_ce: 0.008047\n","iteration 11415 : loss : 0.060011, loss_ce: 0.012149\n","iteration 11416 : loss : 0.040917, loss_ce: 0.010251\n","iteration 11417 : loss : 0.038726, loss_ce: 0.010802\n","iteration 11418 : loss : 0.041925, loss_ce: 0.016590\n","iteration 11419 : loss : 0.039425, loss_ce: 0.013576\n","iteration 11420 : loss : 0.050729, loss_ce: 0.014796\n","iteration 11421 : loss : 0.034439, loss_ce: 0.010860\n","iteration 11422 : loss : 0.045962, loss_ce: 0.009053\n","iteration 11423 : loss : 0.046511, loss_ce: 0.010494\n","iteration 11424 : loss : 0.036180, loss_ce: 0.011286\n","iteration 11425 : loss : 0.037766, loss_ce: 0.011577\n","iteration 11426 : loss : 0.043433, loss_ce: 0.013197\n","iteration 11427 : loss : 0.038314, loss_ce: 0.019553\n","iteration 11428 : loss : 0.044036, loss_ce: 0.015582\n","iteration 11429 : loss : 0.034132, loss_ce: 0.009439\n","iteration 11430 : loss : 0.045284, loss_ce: 0.012084\n","iteration 11431 : loss : 0.040107, loss_ce: 0.012408\n","iteration 11432 : loss : 0.044392, loss_ce: 0.011177\n","iteration 11433 : loss : 0.044075, loss_ce: 0.013955\n","iteration 11434 : loss : 0.041938, loss_ce: 0.016289\n","iteration 11435 : loss : 0.040889, loss_ce: 0.012108\n","iteration 11436 : loss : 0.036874, loss_ce: 0.012952\n","iteration 11437 : loss : 0.103609, loss_ce: 0.007856\n","iteration 11438 : loss : 0.040188, loss_ce: 0.010117\n","iteration 11439 : loss : 0.228221, loss_ce: 0.004817\n"," 82%|███████████████████████▊     | 123/150 [2:24:39<31:30, 70.03s/it]iteration 11440 : loss : 0.040733, loss_ce: 0.011816\n","iteration 11441 : loss : 0.041367, loss_ce: 0.007223\n","iteration 11442 : loss : 0.056596, loss_ce: 0.014183\n","iteration 11443 : loss : 0.046847, loss_ce: 0.015303\n","iteration 11444 : loss : 0.035161, loss_ce: 0.014769\n","iteration 11445 : loss : 0.042819, loss_ce: 0.015160\n","iteration 11446 : loss : 0.042579, loss_ce: 0.008977\n","iteration 11447 : loss : 0.093827, loss_ce: 0.005846\n","iteration 11448 : loss : 0.058753, loss_ce: 0.012723\n","iteration 11449 : loss : 0.043296, loss_ce: 0.015631\n","iteration 11450 : loss : 0.048390, loss_ce: 0.013213\n","iteration 11451 : loss : 0.043750, loss_ce: 0.010995\n","iteration 11452 : loss : 0.044468, loss_ce: 0.013159\n","iteration 11453 : loss : 0.034865, loss_ce: 0.008005\n","iteration 11454 : loss : 0.039965, loss_ce: 0.011508\n","iteration 11455 : loss : 0.042273, loss_ce: 0.014657\n","iteration 11456 : loss : 0.034867, loss_ce: 0.009021\n","iteration 11457 : loss : 0.043904, loss_ce: 0.012061\n","iteration 11458 : loss : 0.043146, loss_ce: 0.019106\n","iteration 11459 : loss : 0.108618, loss_ce: 0.008540\n","iteration 11460 : loss : 0.037386, loss_ce: 0.011527\n","iteration 11461 : loss : 0.041585, loss_ce: 0.014861\n","iteration 11462 : loss : 0.046484, loss_ce: 0.013418\n","iteration 11463 : loss : 0.039251, loss_ce: 0.014393\n","iteration 11464 : loss : 0.036418, loss_ce: 0.014489\n","iteration 11465 : loss : 0.033855, loss_ce: 0.009467\n","iteration 11466 : loss : 0.036232, loss_ce: 0.011322\n","iteration 11467 : loss : 0.044129, loss_ce: 0.012963\n","iteration 11468 : loss : 0.038179, loss_ce: 0.015474\n","iteration 11469 : loss : 0.068210, loss_ce: 0.008191\n","iteration 11470 : loss : 0.035247, loss_ce: 0.006032\n","iteration 11471 : loss : 0.041738, loss_ce: 0.019303\n","iteration 11472 : loss : 0.037166, loss_ce: 0.014002\n","iteration 11473 : loss : 0.042118, loss_ce: 0.016107\n","iteration 11474 : loss : 0.042655, loss_ce: 0.018702\n","iteration 11475 : loss : 0.043934, loss_ce: 0.010889\n","iteration 11476 : loss : 0.042852, loss_ce: 0.014265\n","iteration 11477 : loss : 0.042270, loss_ce: 0.016926\n","iteration 11478 : loss : 0.038640, loss_ce: 0.009341\n","iteration 11479 : loss : 0.035866, loss_ce: 0.011626\n","iteration 11480 : loss : 0.053255, loss_ce: 0.005095\n","iteration 11481 : loss : 0.030798, loss_ce: 0.012718\n","iteration 11482 : loss : 0.047040, loss_ce: 0.014091\n","iteration 11483 : loss : 0.035779, loss_ce: 0.016556\n","iteration 11484 : loss : 0.041233, loss_ce: 0.012576\n","iteration 11485 : loss : 0.033280, loss_ce: 0.006550\n","iteration 11486 : loss : 0.036010, loss_ce: 0.013706\n","iteration 11487 : loss : 0.035582, loss_ce: 0.011056\n","iteration 11488 : loss : 0.096718, loss_ce: 0.008801\n","iteration 11489 : loss : 0.038802, loss_ce: 0.007388\n","iteration 11490 : loss : 0.049353, loss_ce: 0.008033\n","iteration 11491 : loss : 0.037531, loss_ce: 0.014435\n","iteration 11492 : loss : 0.078019, loss_ce: 0.008253\n","iteration 11493 : loss : 0.042364, loss_ce: 0.011821\n","iteration 11494 : loss : 0.035114, loss_ce: 0.012656\n","iteration 11495 : loss : 0.043544, loss_ce: 0.016115\n","iteration 11496 : loss : 0.038303, loss_ce: 0.016459\n","iteration 11497 : loss : 0.039785, loss_ce: 0.010848\n","iteration 11498 : loss : 0.042333, loss_ce: 0.015459\n","iteration 11499 : loss : 0.043149, loss_ce: 0.015415\n","iteration 11500 : loss : 0.036953, loss_ce: 0.013674\n","iteration 11501 : loss : 0.046827, loss_ce: 0.020552\n","iteration 11502 : loss : 0.048523, loss_ce: 0.013429\n","iteration 11503 : loss : 0.031698, loss_ce: 0.010685\n","iteration 11504 : loss : 0.047403, loss_ce: 0.009906\n","iteration 11505 : loss : 0.099795, loss_ce: 0.010696\n","iteration 11506 : loss : 0.043579, loss_ce: 0.012914\n","iteration 11507 : loss : 0.036022, loss_ce: 0.011919\n","iteration 11508 : loss : 0.045862, loss_ce: 0.009907\n","iteration 11509 : loss : 0.037672, loss_ce: 0.007518\n","iteration 11510 : loss : 0.036205, loss_ce: 0.014695\n","iteration 11511 : loss : 0.056586, loss_ce: 0.010652\n","iteration 11512 : loss : 0.042076, loss_ce: 0.012545\n","iteration 11513 : loss : 0.095023, loss_ce: 0.006080\n","iteration 11514 : loss : 0.038540, loss_ce: 0.011226\n","iteration 11515 : loss : 0.040617, loss_ce: 0.008723\n","iteration 11516 : loss : 0.038841, loss_ce: 0.016084\n","iteration 11517 : loss : 0.034795, loss_ce: 0.007918\n","iteration 11518 : loss : 0.052769, loss_ce: 0.011201\n","iteration 11519 : loss : 0.043986, loss_ce: 0.011269\n","iteration 11520 : loss : 0.050386, loss_ce: 0.015439\n","iteration 11521 : loss : 0.041708, loss_ce: 0.013632\n","iteration 11522 : loss : 0.038875, loss_ce: 0.008406\n","iteration 11523 : loss : 0.036385, loss_ce: 0.008675\n","iteration 11524 : loss : 0.099264, loss_ce: 0.008939\n","iteration 11525 : loss : 0.036909, loss_ce: 0.011022\n","iteration 11526 : loss : 0.037937, loss_ce: 0.014151\n","iteration 11527 : loss : 0.028853, loss_ce: 0.010308\n","iteration 11528 : loss : 0.036840, loss_ce: 0.015190\n","iteration 11529 : loss : 0.044954, loss_ce: 0.013301\n","iteration 11530 : loss : 0.041028, loss_ce: 0.010469\n","iteration 11531 : loss : 0.033395, loss_ce: 0.012877\n","iteration 11532 : loss : 0.053365, loss_ce: 0.028292\n"," 83%|███████████████████████▉     | 124/150 [2:25:50<30:22, 70.09s/it]iteration 11533 : loss : 0.051926, loss_ce: 0.011156\n","iteration 11534 : loss : 0.035715, loss_ce: 0.012038\n","iteration 11535 : loss : 0.048105, loss_ce: 0.012213\n","iteration 11536 : loss : 0.035459, loss_ce: 0.011022\n","iteration 11537 : loss : 0.037186, loss_ce: 0.009658\n","iteration 11538 : loss : 0.057700, loss_ce: 0.018342\n","iteration 11539 : loss : 0.040109, loss_ce: 0.010563\n","iteration 11540 : loss : 0.034164, loss_ce: 0.010333\n","iteration 11541 : loss : 0.044616, loss_ce: 0.017073\n","iteration 11542 : loss : 0.091059, loss_ce: 0.014701\n","iteration 11543 : loss : 0.103117, loss_ce: 0.007286\n","iteration 11544 : loss : 0.036834, loss_ce: 0.013144\n","iteration 11545 : loss : 0.042875, loss_ce: 0.014272\n","iteration 11546 : loss : 0.033248, loss_ce: 0.013656\n","iteration 11547 : loss : 0.046090, loss_ce: 0.007612\n","iteration 11548 : loss : 0.041824, loss_ce: 0.016754\n","iteration 11549 : loss : 0.041233, loss_ce: 0.010248\n","iteration 11550 : loss : 0.043872, loss_ce: 0.014355\n","iteration 11551 : loss : 0.037109, loss_ce: 0.015651\n","iteration 11552 : loss : 0.040258, loss_ce: 0.014613\n","iteration 11553 : loss : 0.038633, loss_ce: 0.009729\n","iteration 11554 : loss : 0.041889, loss_ce: 0.014856\n","iteration 11555 : loss : 0.036046, loss_ce: 0.008204\n","iteration 11556 : loss : 0.045256, loss_ce: 0.010762\n","iteration 11557 : loss : 0.042397, loss_ce: 0.014795\n","iteration 11558 : loss : 0.051472, loss_ce: 0.009054\n","iteration 11559 : loss : 0.036221, loss_ce: 0.007725\n","iteration 11560 : loss : 0.037087, loss_ce: 0.009114\n","iteration 11561 : loss : 0.042149, loss_ce: 0.020266\n","iteration 11562 : loss : 0.027607, loss_ce: 0.008521\n","iteration 11563 : loss : 0.050924, loss_ce: 0.013217\n","iteration 11564 : loss : 0.041660, loss_ce: 0.012826\n","iteration 11565 : loss : 0.039356, loss_ce: 0.015490\n","iteration 11566 : loss : 0.040730, loss_ce: 0.013006\n","iteration 11567 : loss : 0.033040, loss_ce: 0.012402\n","iteration 11568 : loss : 0.046428, loss_ce: 0.014844\n","iteration 11569 : loss : 0.033476, loss_ce: 0.009315\n","iteration 11570 : loss : 0.048768, loss_ce: 0.010119\n","iteration 11571 : loss : 0.042173, loss_ce: 0.017141\n","iteration 11572 : loss : 0.047804, loss_ce: 0.013667\n","iteration 11573 : loss : 0.044610, loss_ce: 0.010413\n","iteration 11574 : loss : 0.040373, loss_ce: 0.011000\n","iteration 11575 : loss : 0.038276, loss_ce: 0.010164\n","iteration 11576 : loss : 0.038622, loss_ce: 0.014350\n","iteration 11577 : loss : 0.030804, loss_ce: 0.013551\n","iteration 11578 : loss : 0.037286, loss_ce: 0.007125\n","iteration 11579 : loss : 0.037254, loss_ce: 0.017492\n","iteration 11580 : loss : 0.035403, loss_ce: 0.007603\n","iteration 11581 : loss : 0.039667, loss_ce: 0.012531\n","iteration 11582 : loss : 0.041072, loss_ce: 0.015087\n","iteration 11583 : loss : 0.041292, loss_ce: 0.013289\n","iteration 11584 : loss : 0.041400, loss_ce: 0.014121\n","iteration 11585 : loss : 0.045759, loss_ce: 0.011335\n","iteration 11586 : loss : 0.039360, loss_ce: 0.009584\n","iteration 11587 : loss : 0.102000, loss_ce: 0.008055\n","iteration 11588 : loss : 0.047347, loss_ce: 0.009806\n","iteration 11589 : loss : 0.037553, loss_ce: 0.015107\n","iteration 11590 : loss : 0.044441, loss_ce: 0.013546\n","iteration 11591 : loss : 0.045847, loss_ce: 0.012376\n","iteration 11592 : loss : 0.035031, loss_ce: 0.010371\n","iteration 11593 : loss : 0.036490, loss_ce: 0.016332\n","iteration 11594 : loss : 0.042660, loss_ce: 0.010544\n","iteration 11595 : loss : 0.029781, loss_ce: 0.011949\n","iteration 11596 : loss : 0.151547, loss_ce: 0.005344\n","iteration 11597 : loss : 0.053466, loss_ce: 0.012854\n","iteration 11598 : loss : 0.040459, loss_ce: 0.014078\n","iteration 11599 : loss : 0.040307, loss_ce: 0.010309\n","iteration 11600 : loss : 0.039097, loss_ce: 0.013309\n","iteration 11601 : loss : 0.036134, loss_ce: 0.014401\n","iteration 11602 : loss : 0.040598, loss_ce: 0.007086\n","iteration 11603 : loss : 0.040156, loss_ce: 0.012522\n","iteration 11604 : loss : 0.039535, loss_ce: 0.009301\n","iteration 11605 : loss : 0.044063, loss_ce: 0.013086\n","iteration 11606 : loss : 0.031802, loss_ce: 0.011651\n","iteration 11607 : loss : 0.039482, loss_ce: 0.011341\n","iteration 11608 : loss : 0.035026, loss_ce: 0.008693\n","iteration 11609 : loss : 0.044033, loss_ce: 0.009753\n","iteration 11610 : loss : 0.048843, loss_ce: 0.011901\n","iteration 11611 : loss : 0.038107, loss_ce: 0.017923\n","iteration 11612 : loss : 0.040254, loss_ce: 0.010482\n","iteration 11613 : loss : 0.038151, loss_ce: 0.012199\n","iteration 11614 : loss : 0.045621, loss_ce: 0.014156\n","iteration 11615 : loss : 0.045816, loss_ce: 0.016015\n","iteration 11616 : loss : 0.034892, loss_ce: 0.010328\n","iteration 11617 : loss : 0.043676, loss_ce: 0.020080\n","iteration 11618 : loss : 0.043690, loss_ce: 0.012971\n","iteration 11619 : loss : 0.041058, loss_ce: 0.012376\n","iteration 11620 : loss : 0.037615, loss_ce: 0.008871\n","iteration 11621 : loss : 0.035145, loss_ce: 0.010652\n","iteration 11622 : loss : 0.033956, loss_ce: 0.010539\n","iteration 11623 : loss : 0.036210, loss_ce: 0.008618\n","iteration 11624 : loss : 0.046571, loss_ce: 0.009616\n","iteration 11625 : loss : 0.166113, loss_ce: 0.019504\n"," 83%|████████████████████████▏    | 125/150 [2:27:00<29:13, 70.16s/it]iteration 11626 : loss : 0.097087, loss_ce: 0.009444\n","iteration 11627 : loss : 0.056863, loss_ce: 0.013101\n","iteration 11628 : loss : 0.036922, loss_ce: 0.015181\n","iteration 11629 : loss : 0.040569, loss_ce: 0.007293\n","iteration 11630 : loss : 0.038613, loss_ce: 0.011175\n","iteration 11631 : loss : 0.038087, loss_ce: 0.009282\n","iteration 11632 : loss : 0.057463, loss_ce: 0.011960\n","iteration 11633 : loss : 0.041865, loss_ce: 0.016310\n","iteration 11634 : loss : 0.031178, loss_ce: 0.009028\n","iteration 11635 : loss : 0.045917, loss_ce: 0.008080\n","iteration 11636 : loss : 0.051501, loss_ce: 0.010386\n","iteration 11637 : loss : 0.036710, loss_ce: 0.016303\n","iteration 11638 : loss : 0.035194, loss_ce: 0.016954\n","iteration 11639 : loss : 0.035306, loss_ce: 0.012273\n","iteration 11640 : loss : 0.040566, loss_ce: 0.014089\n","iteration 11641 : loss : 0.053990, loss_ce: 0.012261\n","iteration 11642 : loss : 0.033698, loss_ce: 0.015632\n","iteration 11643 : loss : 0.041912, loss_ce: 0.014237\n","iteration 11644 : loss : 0.038186, loss_ce: 0.015939\n","iteration 11645 : loss : 0.039878, loss_ce: 0.006446\n","iteration 11646 : loss : 0.041096, loss_ce: 0.014349\n","iteration 11647 : loss : 0.037004, loss_ce: 0.007923\n","iteration 11648 : loss : 0.037756, loss_ce: 0.011177\n","iteration 11649 : loss : 0.047923, loss_ce: 0.011850\n","iteration 11650 : loss : 0.037205, loss_ce: 0.014599\n","iteration 11651 : loss : 0.111410, loss_ce: 0.006290\n","iteration 11652 : loss : 0.036610, loss_ce: 0.009621\n","iteration 11653 : loss : 0.087549, loss_ce: 0.004086\n","iteration 11654 : loss : 0.046510, loss_ce: 0.012410\n","iteration 11655 : loss : 0.040655, loss_ce: 0.014021\n","iteration 11656 : loss : 0.047639, loss_ce: 0.017940\n","iteration 11657 : loss : 0.035744, loss_ce: 0.011669\n","iteration 11658 : loss : 0.038490, loss_ce: 0.011596\n","iteration 11659 : loss : 0.106247, loss_ce: 0.008969\n","iteration 11660 : loss : 0.032460, loss_ce: 0.013161\n","iteration 11661 : loss : 0.043591, loss_ce: 0.019081\n","iteration 11662 : loss : 0.037611, loss_ce: 0.007263\n","iteration 11663 : loss : 0.054906, loss_ce: 0.016522\n","iteration 11664 : loss : 0.053236, loss_ce: 0.008806\n","iteration 11665 : loss : 0.039555, loss_ce: 0.015880\n","iteration 11666 : loss : 0.027860, loss_ce: 0.003401\n","iteration 11667 : loss : 0.044685, loss_ce: 0.012409\n","iteration 11668 : loss : 0.038620, loss_ce: 0.013953\n","iteration 11669 : loss : 0.039112, loss_ce: 0.010445\n","iteration 11670 : loss : 0.043996, loss_ce: 0.014381\n","iteration 11671 : loss : 0.039880, loss_ce: 0.009692\n","iteration 11672 : loss : 0.040259, loss_ce: 0.013533\n","iteration 11673 : loss : 0.034428, loss_ce: 0.015196\n","iteration 11674 : loss : 0.043888, loss_ce: 0.013098\n","iteration 11675 : loss : 0.042906, loss_ce: 0.014048\n","iteration 11676 : loss : 0.035508, loss_ce: 0.014706\n","iteration 11677 : loss : 0.033937, loss_ce: 0.009214\n","iteration 11678 : loss : 0.038963, loss_ce: 0.012586\n","iteration 11679 : loss : 0.034496, loss_ce: 0.011390\n","iteration 11680 : loss : 0.088051, loss_ce: 0.005117\n","iteration 11681 : loss : 0.032194, loss_ce: 0.010706\n","iteration 11682 : loss : 0.043059, loss_ce: 0.016264\n","iteration 11683 : loss : 0.073959, loss_ce: 0.015744\n","iteration 11684 : loss : 0.039031, loss_ce: 0.008680\n","iteration 11685 : loss : 0.034723, loss_ce: 0.009870\n","iteration 11686 : loss : 0.035377, loss_ce: 0.010469\n","iteration 11687 : loss : 0.042293, loss_ce: 0.017922\n","iteration 11688 : loss : 0.040037, loss_ce: 0.009974\n","iteration 11689 : loss : 0.038807, loss_ce: 0.015030\n","iteration 11690 : loss : 0.035669, loss_ce: 0.008607\n","iteration 11691 : loss : 0.032739, loss_ce: 0.008169\n","iteration 11692 : loss : 0.102951, loss_ce: 0.016303\n","iteration 11693 : loss : 0.042082, loss_ce: 0.012489\n","iteration 11694 : loss : 0.037953, loss_ce: 0.011093\n","iteration 11695 : loss : 0.040544, loss_ce: 0.017917\n","iteration 11696 : loss : 0.044640, loss_ce: 0.017039\n","iteration 11697 : loss : 0.247966, loss_ce: 0.003861\n","iteration 11698 : loss : 0.042704, loss_ce: 0.014099\n","iteration 11699 : loss : 0.038106, loss_ce: 0.008739\n","iteration 11700 : loss : 0.036861, loss_ce: 0.016037\n","iteration 11701 : loss : 0.039474, loss_ce: 0.011091\n","iteration 11702 : loss : 0.043467, loss_ce: 0.013404\n","iteration 11703 : loss : 0.058602, loss_ce: 0.014874\n","iteration 11704 : loss : 0.103812, loss_ce: 0.007819\n","iteration 11705 : loss : 0.041867, loss_ce: 0.021033\n","iteration 11706 : loss : 0.103059, loss_ce: 0.014405\n","iteration 11707 : loss : 0.041318, loss_ce: 0.012912\n","iteration 11708 : loss : 0.037402, loss_ce: 0.012607\n","iteration 11709 : loss : 0.045320, loss_ce: 0.018756\n","iteration 11710 : loss : 0.042418, loss_ce: 0.015983\n","iteration 11711 : loss : 0.045115, loss_ce: 0.011760\n","iteration 11712 : loss : 0.100789, loss_ce: 0.009147\n","iteration 11713 : loss : 0.036627, loss_ce: 0.010319\n","iteration 11714 : loss : 0.032289, loss_ce: 0.009617\n","iteration 11715 : loss : 0.040145, loss_ce: 0.010323\n","iteration 11716 : loss : 0.042709, loss_ce: 0.012559\n","iteration 11717 : loss : 0.093350, loss_ce: 0.008823\n","iteration 11718 : loss : 0.361053, loss_ce: 0.003280\n"," 84%|████████████████████████▎    | 126/150 [2:28:10<28:03, 70.15s/it]iteration 11719 : loss : 0.036514, loss_ce: 0.010808\n","iteration 11720 : loss : 0.039841, loss_ce: 0.010839\n","iteration 11721 : loss : 0.035809, loss_ce: 0.010956\n","iteration 11722 : loss : 0.098986, loss_ce: 0.008841\n","iteration 11723 : loss : 0.044232, loss_ce: 0.013158\n","iteration 11724 : loss : 0.106280, loss_ce: 0.014752\n","iteration 11725 : loss : 0.050663, loss_ce: 0.010163\n","iteration 11726 : loss : 0.035806, loss_ce: 0.011628\n","iteration 11727 : loss : 0.105237, loss_ce: 0.011484\n","iteration 11728 : loss : 0.038500, loss_ce: 0.012322\n","iteration 11729 : loss : 0.040011, loss_ce: 0.013200\n","iteration 11730 : loss : 0.031855, loss_ce: 0.012052\n","iteration 11731 : loss : 0.093866, loss_ce: 0.008258\n","iteration 11732 : loss : 0.035727, loss_ce: 0.012597\n","iteration 11733 : loss : 0.033410, loss_ce: 0.009737\n","iteration 11734 : loss : 0.039599, loss_ce: 0.010822\n","iteration 11735 : loss : 0.038078, loss_ce: 0.014859\n","iteration 11736 : loss : 0.043325, loss_ce: 0.011245\n","iteration 11737 : loss : 0.073645, loss_ce: 0.012865\n","iteration 11738 : loss : 0.041099, loss_ce: 0.010908\n","iteration 11739 : loss : 0.040304, loss_ce: 0.014533\n","iteration 11740 : loss : 0.044167, loss_ce: 0.015245\n","iteration 11741 : loss : 0.037788, loss_ce: 0.009847\n","iteration 11742 : loss : 0.039909, loss_ce: 0.013989\n","iteration 11743 : loss : 0.041028, loss_ce: 0.010651\n","iteration 11744 : loss : 0.047306, loss_ce: 0.013445\n","iteration 11745 : loss : 0.035099, loss_ce: 0.010272\n","iteration 11746 : loss : 0.055245, loss_ce: 0.012156\n","iteration 11747 : loss : 0.039437, loss_ce: 0.015119\n","iteration 11748 : loss : 0.038747, loss_ce: 0.009187\n","iteration 11749 : loss : 0.036116, loss_ce: 0.006333\n","iteration 11750 : loss : 0.039768, loss_ce: 0.016320\n","iteration 11751 : loss : 0.039600, loss_ce: 0.016148\n","iteration 11752 : loss : 0.038467, loss_ce: 0.010623\n","iteration 11753 : loss : 0.037688, loss_ce: 0.010303\n","iteration 11754 : loss : 0.039559, loss_ce: 0.013345\n","iteration 11755 : loss : 0.040532, loss_ce: 0.010919\n","iteration 11756 : loss : 0.033313, loss_ce: 0.008909\n","iteration 11757 : loss : 0.037564, loss_ce: 0.007889\n","iteration 11758 : loss : 0.040013, loss_ce: 0.015182\n","iteration 11759 : loss : 0.053449, loss_ce: 0.008191\n","iteration 11760 : loss : 0.034367, loss_ce: 0.015870\n","iteration 11761 : loss : 0.037451, loss_ce: 0.016791\n","iteration 11762 : loss : 0.045368, loss_ce: 0.013779\n","iteration 11763 : loss : 0.040337, loss_ce: 0.012028\n","iteration 11764 : loss : 0.033375, loss_ce: 0.011576\n","iteration 11765 : loss : 0.036892, loss_ce: 0.010883\n","iteration 11766 : loss : 0.042094, loss_ce: 0.018968\n","iteration 11767 : loss : 0.055502, loss_ce: 0.011647\n","iteration 11768 : loss : 0.040445, loss_ce: 0.015661\n","iteration 11769 : loss : 0.099119, loss_ce: 0.012846\n","iteration 11770 : loss : 0.050491, loss_ce: 0.008001\n","iteration 11771 : loss : 0.041531, loss_ce: 0.013552\n","iteration 11772 : loss : 0.036773, loss_ce: 0.009139\n","iteration 11773 : loss : 0.040046, loss_ce: 0.013669\n","iteration 11774 : loss : 0.042789, loss_ce: 0.008830\n","iteration 11775 : loss : 0.034399, loss_ce: 0.009181\n","iteration 11776 : loss : 0.041076, loss_ce: 0.011086\n","iteration 11777 : loss : 0.033728, loss_ce: 0.013550\n","iteration 11778 : loss : 0.038189, loss_ce: 0.012364\n","iteration 11779 : loss : 0.049634, loss_ce: 0.017338\n","iteration 11780 : loss : 0.051590, loss_ce: 0.014945\n","iteration 11781 : loss : 0.037019, loss_ce: 0.009774\n","iteration 11782 : loss : 0.030194, loss_ce: 0.005222\n","iteration 11783 : loss : 0.040775, loss_ce: 0.015506\n","iteration 11784 : loss : 0.057133, loss_ce: 0.010990\n","iteration 11785 : loss : 0.041223, loss_ce: 0.010273\n","iteration 11786 : loss : 0.040712, loss_ce: 0.011269\n","iteration 11787 : loss : 0.103289, loss_ce: 0.013753\n","iteration 11788 : loss : 0.036200, loss_ce: 0.011359\n","iteration 11789 : loss : 0.038576, loss_ce: 0.019688\n","iteration 11790 : loss : 0.035306, loss_ce: 0.012287\n","iteration 11791 : loss : 0.102661, loss_ce: 0.008389\n","iteration 11792 : loss : 0.035350, loss_ce: 0.013289\n","iteration 11793 : loss : 0.095375, loss_ce: 0.014013\n","iteration 11794 : loss : 0.033181, loss_ce: 0.010244\n","iteration 11795 : loss : 0.039960, loss_ce: 0.010570\n","iteration 11796 : loss : 0.040950, loss_ce: 0.012480\n","iteration 11797 : loss : 0.042451, loss_ce: 0.017360\n","iteration 11798 : loss : 0.038504, loss_ce: 0.009360\n","iteration 11799 : loss : 0.045150, loss_ce: 0.005863\n","iteration 11800 : loss : 0.041429, loss_ce: 0.016921\n","iteration 11801 : loss : 0.041227, loss_ce: 0.010911\n","iteration 11802 : loss : 0.037814, loss_ce: 0.011365\n","iteration 11803 : loss : 0.040851, loss_ce: 0.015046\n","iteration 11804 : loss : 0.039640, loss_ce: 0.015662\n","iteration 11805 : loss : 0.044426, loss_ce: 0.019408\n","iteration 11806 : loss : 0.038539, loss_ce: 0.008449\n","iteration 11807 : loss : 0.040004, loss_ce: 0.012297\n","iteration 11808 : loss : 0.041687, loss_ce: 0.014116\n","iteration 11809 : loss : 0.036090, loss_ce: 0.010492\n","iteration 11810 : loss : 0.036650, loss_ce: 0.009109\n","iteration 11811 : loss : 0.063810, loss_ce: 0.022919\n"," 85%|████████████████████████▌    | 127/150 [2:29:20<26:54, 70.20s/it]iteration 11812 : loss : 0.039525, loss_ce: 0.014992\n","iteration 11813 : loss : 0.045970, loss_ce: 0.017144\n","iteration 11814 : loss : 0.038920, loss_ce: 0.013716\n","iteration 11815 : loss : 0.104553, loss_ce: 0.014496\n","iteration 11816 : loss : 0.047745, loss_ce: 0.009400\n","iteration 11817 : loss : 0.043592, loss_ce: 0.012361\n","iteration 11818 : loss : 0.038961, loss_ce: 0.017748\n","iteration 11819 : loss : 0.041193, loss_ce: 0.013309\n","iteration 11820 : loss : 0.032638, loss_ce: 0.009819\n","iteration 11821 : loss : 0.047575, loss_ce: 0.009157\n","iteration 11822 : loss : 0.040655, loss_ce: 0.012089\n","iteration 11823 : loss : 0.031451, loss_ce: 0.004957\n","iteration 11824 : loss : 0.038735, loss_ce: 0.014279\n","iteration 11825 : loss : 0.041533, loss_ce: 0.011238\n","iteration 11826 : loss : 0.044390, loss_ce: 0.011724\n","iteration 11827 : loss : 0.041426, loss_ce: 0.011541\n","iteration 11828 : loss : 0.039389, loss_ce: 0.018825\n","iteration 11829 : loss : 0.037986, loss_ce: 0.010221\n","iteration 11830 : loss : 0.095455, loss_ce: 0.007608\n","iteration 11831 : loss : 0.035223, loss_ce: 0.011910\n","iteration 11832 : loss : 0.040886, loss_ce: 0.012361\n","iteration 11833 : loss : 0.029407, loss_ce: 0.008687\n","iteration 11834 : loss : 0.036008, loss_ce: 0.015404\n","iteration 11835 : loss : 0.037343, loss_ce: 0.011639\n","iteration 11836 : loss : 0.041845, loss_ce: 0.009127\n","iteration 11837 : loss : 0.034724, loss_ce: 0.008925\n","iteration 11838 : loss : 0.042098, loss_ce: 0.013483\n","iteration 11839 : loss : 0.042845, loss_ce: 0.008233\n","iteration 11840 : loss : 0.039377, loss_ce: 0.016363\n","iteration 11841 : loss : 0.041313, loss_ce: 0.010722\n","iteration 11842 : loss : 0.047823, loss_ce: 0.012122\n","iteration 11843 : loss : 0.032778, loss_ce: 0.010894\n","iteration 11844 : loss : 0.092481, loss_ce: 0.009084\n","iteration 11845 : loss : 0.033923, loss_ce: 0.012709\n","iteration 11846 : loss : 0.031417, loss_ce: 0.008929\n","iteration 11847 : loss : 0.042410, loss_ce: 0.011909\n","iteration 11848 : loss : 0.069942, loss_ce: 0.009805\n","iteration 11849 : loss : 0.042052, loss_ce: 0.019495\n","iteration 11850 : loss : 0.101387, loss_ce: 0.011476\n","iteration 11851 : loss : 0.035833, loss_ce: 0.009589\n","iteration 11852 : loss : 0.039592, loss_ce: 0.014310\n","iteration 11853 : loss : 0.047838, loss_ce: 0.015926\n","iteration 11854 : loss : 0.048717, loss_ce: 0.017084\n","iteration 11855 : loss : 0.044672, loss_ce: 0.020228\n","iteration 11856 : loss : 0.044863, loss_ce: 0.016078\n","iteration 11857 : loss : 0.035462, loss_ce: 0.011608\n","iteration 11858 : loss : 0.036985, loss_ce: 0.015184\n","iteration 11859 : loss : 0.095289, loss_ce: 0.007968\n","iteration 11860 : loss : 0.040234, loss_ce: 0.009503\n","iteration 11861 : loss : 0.033514, loss_ce: 0.010936\n","iteration 11862 : loss : 0.038528, loss_ce: 0.010778\n","iteration 11863 : loss : 0.043395, loss_ce: 0.013521\n","iteration 11864 : loss : 0.036366, loss_ce: 0.016612\n","iteration 11865 : loss : 0.030547, loss_ce: 0.006754\n","iteration 11866 : loss : 0.039974, loss_ce: 0.007762\n","iteration 11867 : loss : 0.040643, loss_ce: 0.012088\n","iteration 11868 : loss : 0.042765, loss_ce: 0.014031\n","iteration 11869 : loss : 0.036720, loss_ce: 0.013046\n","iteration 11870 : loss : 0.040360, loss_ce: 0.013525\n","iteration 11871 : loss : 0.040758, loss_ce: 0.012549\n","iteration 11872 : loss : 0.038912, loss_ce: 0.009335\n","iteration 11873 : loss : 0.041664, loss_ce: 0.014685\n","iteration 11874 : loss : 0.039299, loss_ce: 0.013344\n","iteration 11875 : loss : 0.046005, loss_ce: 0.007068\n","iteration 11876 : loss : 0.040898, loss_ce: 0.014578\n","iteration 11877 : loss : 0.032074, loss_ce: 0.012761\n","iteration 11878 : loss : 0.034888, loss_ce: 0.008028\n","iteration 11879 : loss : 0.044913, loss_ce: 0.008857\n","iteration 11880 : loss : 0.060537, loss_ce: 0.010459\n","iteration 11881 : loss : 0.032901, loss_ce: 0.003441\n","iteration 11882 : loss : 0.035565, loss_ce: 0.008980\n","iteration 11883 : loss : 0.040299, loss_ce: 0.013282\n","iteration 11884 : loss : 0.048548, loss_ce: 0.015985\n","iteration 11885 : loss : 0.036200, loss_ce: 0.016089\n","iteration 11886 : loss : 0.039827, loss_ce: 0.014903\n","iteration 11887 : loss : 0.030210, loss_ce: 0.007346\n","iteration 11888 : loss : 0.037010, loss_ce: 0.016344\n","iteration 11889 : loss : 0.034955, loss_ce: 0.012094\n","iteration 11890 : loss : 0.031302, loss_ce: 0.007968\n","iteration 11891 : loss : 0.044100, loss_ce: 0.015070\n","iteration 11892 : loss : 0.035855, loss_ce: 0.012725\n","iteration 11893 : loss : 0.053600, loss_ce: 0.015168\n","iteration 11894 : loss : 0.092608, loss_ce: 0.008919\n","iteration 11895 : loss : 0.031661, loss_ce: 0.006351\n","iteration 11896 : loss : 0.039483, loss_ce: 0.010021\n","iteration 11897 : loss : 0.035105, loss_ce: 0.010877\n","iteration 11898 : loss : 0.032306, loss_ce: 0.010339\n","iteration 11899 : loss : 0.031923, loss_ce: 0.009484\n","iteration 11900 : loss : 0.041995, loss_ce: 0.013956\n","iteration 11901 : loss : 0.042208, loss_ce: 0.011562\n","iteration 11902 : loss : 0.038820, loss_ce: 0.007624\n","iteration 11903 : loss : 0.029496, loss_ce: 0.006272\n","iteration 11904 : loss : 0.039083, loss_ce: 0.025251\n"," 85%|████████████████████████▋    | 128/150 [2:30:30<25:43, 70.14s/it]iteration 11905 : loss : 0.042984, loss_ce: 0.006380\n","iteration 11906 : loss : 0.043417, loss_ce: 0.007830\n","iteration 11907 : loss : 0.038342, loss_ce: 0.010853\n","iteration 11908 : loss : 0.041849, loss_ce: 0.013289\n","iteration 11909 : loss : 0.040201, loss_ce: 0.009853\n","iteration 11910 : loss : 0.037998, loss_ce: 0.011936\n","iteration 11911 : loss : 0.036824, loss_ce: 0.013463\n","iteration 11912 : loss : 0.099083, loss_ce: 0.009173\n","iteration 11913 : loss : 0.037703, loss_ce: 0.009112\n","iteration 11914 : loss : 0.044006, loss_ce: 0.012142\n","iteration 11915 : loss : 0.061476, loss_ce: 0.008102\n","iteration 11916 : loss : 0.042089, loss_ce: 0.009957\n","iteration 11917 : loss : 0.041517, loss_ce: 0.012325\n","iteration 11918 : loss : 0.035868, loss_ce: 0.013236\n","iteration 11919 : loss : 0.036700, loss_ce: 0.012754\n","iteration 11920 : loss : 0.041972, loss_ce: 0.013956\n","iteration 11921 : loss : 0.042464, loss_ce: 0.016594\n","iteration 11922 : loss : 0.038022, loss_ce: 0.012249\n","iteration 11923 : loss : 0.033000, loss_ce: 0.014407\n","iteration 11924 : loss : 0.097224, loss_ce: 0.007931\n","iteration 11925 : loss : 0.034066, loss_ce: 0.016312\n","iteration 11926 : loss : 0.036435, loss_ce: 0.012543\n","iteration 11927 : loss : 0.046630, loss_ce: 0.011082\n","iteration 11928 : loss : 0.044384, loss_ce: 0.009691\n","iteration 11929 : loss : 0.096134, loss_ce: 0.008286\n","iteration 11930 : loss : 0.031861, loss_ce: 0.007237\n","iteration 11931 : loss : 0.045386, loss_ce: 0.013176\n","iteration 11932 : loss : 0.035645, loss_ce: 0.010484\n","iteration 11933 : loss : 0.031873, loss_ce: 0.015210\n","iteration 11934 : loss : 0.040631, loss_ce: 0.009624\n","iteration 11935 : loss : 0.037412, loss_ce: 0.012602\n","iteration 11936 : loss : 0.039369, loss_ce: 0.015237\n","iteration 11937 : loss : 0.044120, loss_ce: 0.010855\n","iteration 11938 : loss : 0.042224, loss_ce: 0.010988\n","iteration 11939 : loss : 0.042632, loss_ce: 0.011466\n","iteration 11940 : loss : 0.042272, loss_ce: 0.015372\n","iteration 11941 : loss : 0.038046, loss_ce: 0.011884\n","iteration 11942 : loss : 0.040574, loss_ce: 0.010738\n","iteration 11943 : loss : 0.044337, loss_ce: 0.010346\n","iteration 11944 : loss : 0.037211, loss_ce: 0.011658\n","iteration 11945 : loss : 0.035485, loss_ce: 0.009441\n","iteration 11946 : loss : 0.031820, loss_ce: 0.011072\n","iteration 11947 : loss : 0.041873, loss_ce: 0.012741\n","iteration 11948 : loss : 0.038396, loss_ce: 0.016633\n","iteration 11949 : loss : 0.061570, loss_ce: 0.012154\n","iteration 11950 : loss : 0.039859, loss_ce: 0.011926\n","iteration 11951 : loss : 0.030093, loss_ce: 0.011483\n","iteration 11952 : loss : 0.036662, loss_ce: 0.009357\n","iteration 11953 : loss : 0.032341, loss_ce: 0.007774\n","iteration 11954 : loss : 0.040252, loss_ce: 0.013946\n","iteration 11955 : loss : 0.032419, loss_ce: 0.011396\n","iteration 11956 : loss : 0.062867, loss_ce: 0.013077\n","iteration 11957 : loss : 0.035566, loss_ce: 0.012864\n","iteration 11958 : loss : 0.037348, loss_ce: 0.013894\n","iteration 11959 : loss : 0.034485, loss_ce: 0.010273\n","iteration 11960 : loss : 0.038865, loss_ce: 0.012508\n","iteration 11961 : loss : 0.039520, loss_ce: 0.023037\n","iteration 11962 : loss : 0.039577, loss_ce: 0.017326\n","iteration 11963 : loss : 0.094100, loss_ce: 0.008803\n","iteration 11964 : loss : 0.043293, loss_ce: 0.008439\n","iteration 11965 : loss : 0.034975, loss_ce: 0.010260\n","iteration 11966 : loss : 0.037013, loss_ce: 0.008760\n","iteration 11967 : loss : 0.048817, loss_ce: 0.009766\n","iteration 11968 : loss : 0.038847, loss_ce: 0.013969\n","iteration 11969 : loss : 0.038967, loss_ce: 0.013345\n","iteration 11970 : loss : 0.036475, loss_ce: 0.012012\n","iteration 11971 : loss : 0.044487, loss_ce: 0.021968\n","iteration 11972 : loss : 0.066095, loss_ce: 0.007998\n","iteration 11973 : loss : 0.037805, loss_ce: 0.006018\n","iteration 11974 : loss : 0.035782, loss_ce: 0.010479\n","iteration 11975 : loss : 0.041795, loss_ce: 0.013864\n","iteration 11976 : loss : 0.043931, loss_ce: 0.013679\n","iteration 11977 : loss : 0.089222, loss_ce: 0.026318\n","iteration 11978 : loss : 0.047593, loss_ce: 0.010430\n","iteration 11979 : loss : 0.041306, loss_ce: 0.011061\n","iteration 11980 : loss : 0.037761, loss_ce: 0.009849\n","iteration 11981 : loss : 0.101905, loss_ce: 0.008670\n","iteration 11982 : loss : 0.036628, loss_ce: 0.012196\n","iteration 11983 : loss : 0.099151, loss_ce: 0.013038\n","iteration 11984 : loss : 0.034706, loss_ce: 0.006423\n","iteration 11985 : loss : 0.034512, loss_ce: 0.012362\n","iteration 11986 : loss : 0.042795, loss_ce: 0.011320\n","iteration 11987 : loss : 0.038565, loss_ce: 0.008104\n","iteration 11988 : loss : 0.040646, loss_ce: 0.015112\n","iteration 11989 : loss : 0.038158, loss_ce: 0.015995\n","iteration 11990 : loss : 0.040828, loss_ce: 0.010796\n","iteration 11991 : loss : 0.036484, loss_ce: 0.009311\n","iteration 11992 : loss : 0.039782, loss_ce: 0.015729\n","iteration 11993 : loss : 0.033824, loss_ce: 0.006719\n","iteration 11994 : loss : 0.039382, loss_ce: 0.016251\n","iteration 11995 : loss : 0.096812, loss_ce: 0.007183\n","iteration 11996 : loss : 0.048666, loss_ce: 0.014010\n","iteration 11997 : loss : 0.214394, loss_ce: 0.000247\n"," 86%|████████████████████████▉    | 129/150 [2:31:40<24:32, 70.14s/it]iteration 11998 : loss : 0.040256, loss_ce: 0.010250\n","iteration 11999 : loss : 0.056291, loss_ce: 0.022175\n","iteration 12000 : loss : 0.046420, loss_ce: 0.012987\n","iteration 12001 : loss : 0.069805, loss_ce: 0.025005\n","iteration 12002 : loss : 0.216668, loss_ce: 0.032532\n","iteration 12003 : loss : 0.070512, loss_ce: 0.020542\n","iteration 12004 : loss : 0.058684, loss_ce: 0.017973\n","iteration 12005 : loss : 0.034294, loss_ce: 0.011763\n","iteration 12006 : loss : 0.037337, loss_ce: 0.013671\n","iteration 12007 : loss : 0.046349, loss_ce: 0.010903\n","iteration 12008 : loss : 0.052778, loss_ce: 0.015419\n","iteration 12009 : loss : 0.064122, loss_ce: 0.029673\n","iteration 12010 : loss : 0.107999, loss_ce: 0.018253\n","iteration 12011 : loss : 0.049047, loss_ce: 0.019399\n","iteration 12012 : loss : 0.052996, loss_ce: 0.017825\n","iteration 12013 : loss : 0.047549, loss_ce: 0.015797\n","iteration 12014 : loss : 0.110867, loss_ce: 0.011096\n","iteration 12015 : loss : 0.049458, loss_ce: 0.017348\n","iteration 12016 : loss : 0.041294, loss_ce: 0.014456\n","iteration 12017 : loss : 0.055317, loss_ce: 0.025834\n","iteration 12018 : loss : 0.038578, loss_ce: 0.008136\n","iteration 12019 : loss : 0.045563, loss_ce: 0.017206\n","iteration 12020 : loss : 0.046078, loss_ce: 0.008766\n","iteration 12021 : loss : 0.040740, loss_ce: 0.014843\n","iteration 12022 : loss : 0.048097, loss_ce: 0.013324\n","iteration 12023 : loss : 0.067739, loss_ce: 0.020261\n","iteration 12024 : loss : 0.050212, loss_ce: 0.012038\n","iteration 12025 : loss : 0.045186, loss_ce: 0.014851\n","iteration 12026 : loss : 0.052720, loss_ce: 0.015439\n","iteration 12027 : loss : 0.037628, loss_ce: 0.011619\n","iteration 12028 : loss : 0.042076, loss_ce: 0.015851\n","iteration 12029 : loss : 0.048851, loss_ce: 0.015783\n","iteration 12030 : loss : 0.042823, loss_ce: 0.011971\n","iteration 12031 : loss : 0.056637, loss_ce: 0.008750\n","iteration 12032 : loss : 0.043875, loss_ce: 0.015816\n","iteration 12033 : loss : 0.047157, loss_ce: 0.014909\n","iteration 12034 : loss : 0.039750, loss_ce: 0.011316\n","iteration 12035 : loss : 0.049425, loss_ce: 0.020402\n","iteration 12036 : loss : 0.050263, loss_ce: 0.014009\n","iteration 12037 : loss : 0.030096, loss_ce: 0.004882\n","iteration 12038 : loss : 0.034798, loss_ce: 0.009281\n","iteration 12039 : loss : 0.044888, loss_ce: 0.018226\n","iteration 12040 : loss : 0.039744, loss_ce: 0.013650\n","iteration 12041 : loss : 0.039664, loss_ce: 0.012187\n","iteration 12042 : loss : 0.046054, loss_ce: 0.013518\n","iteration 12043 : loss : 0.051837, loss_ce: 0.014970\n","iteration 12044 : loss : 0.042508, loss_ce: 0.018102\n","iteration 12045 : loss : 0.047907, loss_ce: 0.019795\n","iteration 12046 : loss : 0.035505, loss_ce: 0.006951\n","iteration 12047 : loss : 0.038809, loss_ce: 0.013278\n","iteration 12048 : loss : 0.035375, loss_ce: 0.007363\n","iteration 12049 : loss : 0.041058, loss_ce: 0.007241\n","iteration 12050 : loss : 0.046973, loss_ce: 0.013332\n","iteration 12051 : loss : 0.040567, loss_ce: 0.011154\n","iteration 12052 : loss : 0.033445, loss_ce: 0.012572\n","iteration 12053 : loss : 0.043782, loss_ce: 0.014841\n","iteration 12054 : loss : 0.029560, loss_ce: 0.010427\n","iteration 12055 : loss : 0.038728, loss_ce: 0.017108\n","iteration 12056 : loss : 0.040234, loss_ce: 0.014322\n","iteration 12057 : loss : 0.042655, loss_ce: 0.006830\n","iteration 12058 : loss : 0.030209, loss_ce: 0.003927\n","iteration 12059 : loss : 0.045197, loss_ce: 0.020301\n","iteration 12060 : loss : 0.045370, loss_ce: 0.018857\n","iteration 12061 : loss : 0.034866, loss_ce: 0.009209\n","iteration 12062 : loss : 0.038584, loss_ce: 0.011941\n","iteration 12063 : loss : 0.042745, loss_ce: 0.012134\n","iteration 12064 : loss : 0.039103, loss_ce: 0.010626\n","iteration 12065 : loss : 0.041020, loss_ce: 0.014807\n","iteration 12066 : loss : 0.038529, loss_ce: 0.010799\n","iteration 12067 : loss : 0.034223, loss_ce: 0.011409\n","iteration 12068 : loss : 0.043369, loss_ce: 0.019335\n","iteration 12069 : loss : 0.045070, loss_ce: 0.020283\n","iteration 12070 : loss : 0.036402, loss_ce: 0.011680\n","iteration 12071 : loss : 0.037656, loss_ce: 0.015413\n","iteration 12072 : loss : 0.041937, loss_ce: 0.010213\n","iteration 12073 : loss : 0.039434, loss_ce: 0.012175\n","iteration 12074 : loss : 0.044050, loss_ce: 0.009421\n","iteration 12075 : loss : 0.048853, loss_ce: 0.017771\n","iteration 12076 : loss : 0.093612, loss_ce: 0.009703\n","iteration 12077 : loss : 0.041493, loss_ce: 0.012228\n","iteration 12078 : loss : 0.045054, loss_ce: 0.008690\n","iteration 12079 : loss : 0.042034, loss_ce: 0.014329\n","iteration 12080 : loss : 0.033971, loss_ce: 0.009072\n","iteration 12081 : loss : 0.098336, loss_ce: 0.009761\n","iteration 12082 : loss : 0.096589, loss_ce: 0.009528\n","iteration 12083 : loss : 0.038445, loss_ce: 0.016491\n","iteration 12084 : loss : 0.039289, loss_ce: 0.006748\n","iteration 12085 : loss : 0.040454, loss_ce: 0.011093\n","iteration 12086 : loss : 0.031241, loss_ce: 0.007303\n","iteration 12087 : loss : 0.035473, loss_ce: 0.014606\n","iteration 12088 : loss : 0.047681, loss_ce: 0.010074\n","iteration 12089 : loss : 0.040251, loss_ce: 0.011267\n","iteration 12090 : loss : 0.313127, loss_ce: 0.003035\n"," 87%|█████████████████████████▏   | 130/150 [2:32:50<23:20, 70.05s/it]iteration 12091 : loss : 0.041045, loss_ce: 0.014103\n","iteration 12092 : loss : 0.037043, loss_ce: 0.014155\n","iteration 12093 : loss : 0.043443, loss_ce: 0.008403\n","iteration 12094 : loss : 0.040373, loss_ce: 0.015812\n","iteration 12095 : loss : 0.042527, loss_ce: 0.015007\n","iteration 12096 : loss : 0.048062, loss_ce: 0.008307\n","iteration 12097 : loss : 0.052019, loss_ce: 0.009765\n","iteration 12098 : loss : 0.038458, loss_ce: 0.012975\n","iteration 12099 : loss : 0.097369, loss_ce: 0.007741\n","iteration 12100 : loss : 0.041432, loss_ce: 0.011141\n","iteration 12101 : loss : 0.035558, loss_ce: 0.008493\n","iteration 12102 : loss : 0.037520, loss_ce: 0.007041\n","iteration 12103 : loss : 0.043890, loss_ce: 0.014078\n","iteration 12104 : loss : 0.038731, loss_ce: 0.012375\n","iteration 12105 : loss : 0.033841, loss_ce: 0.010187\n","iteration 12106 : loss : 0.041874, loss_ce: 0.010838\n","iteration 12107 : loss : 0.038024, loss_ce: 0.013544\n","iteration 12108 : loss : 0.046671, loss_ce: 0.011758\n","iteration 12109 : loss : 0.033024, loss_ce: 0.007557\n","iteration 12110 : loss : 0.064655, loss_ce: 0.014639\n","iteration 12111 : loss : 0.161766, loss_ce: 0.003767\n","iteration 12112 : loss : 0.045716, loss_ce: 0.012404\n","iteration 12113 : loss : 0.040399, loss_ce: 0.013513\n","iteration 12114 : loss : 0.043565, loss_ce: 0.019139\n","iteration 12115 : loss : 0.039887, loss_ce: 0.016229\n","iteration 12116 : loss : 0.037728, loss_ce: 0.015649\n","iteration 12117 : loss : 0.035417, loss_ce: 0.012954\n","iteration 12118 : loss : 0.046299, loss_ce: 0.013292\n","iteration 12119 : loss : 0.047678, loss_ce: 0.015848\n","iteration 12120 : loss : 0.034664, loss_ce: 0.016146\n","iteration 12121 : loss : 0.039800, loss_ce: 0.015279\n","iteration 12122 : loss : 0.037164, loss_ce: 0.012327\n","iteration 12123 : loss : 0.053456, loss_ce: 0.011285\n","iteration 12124 : loss : 0.038550, loss_ce: 0.014375\n","iteration 12125 : loss : 0.040242, loss_ce: 0.017380\n","iteration 12126 : loss : 0.045753, loss_ce: 0.015511\n","iteration 12127 : loss : 0.038673, loss_ce: 0.006978\n","iteration 12128 : loss : 0.044039, loss_ce: 0.014583\n","iteration 12129 : loss : 0.041716, loss_ce: 0.021631\n","iteration 12130 : loss : 0.026009, loss_ce: 0.008827\n","iteration 12131 : loss : 0.046495, loss_ce: 0.007639\n","iteration 12132 : loss : 0.044263, loss_ce: 0.018516\n","iteration 12133 : loss : 0.039327, loss_ce: 0.006625\n","iteration 12134 : loss : 0.101619, loss_ce: 0.006339\n","iteration 12135 : loss : 0.221297, loss_ce: 0.003716\n","iteration 12136 : loss : 0.042123, loss_ce: 0.011505\n","iteration 12137 : loss : 0.036947, loss_ce: 0.013277\n","iteration 12138 : loss : 0.040243, loss_ce: 0.010334\n","iteration 12139 : loss : 0.041821, loss_ce: 0.011160\n","iteration 12140 : loss : 0.038971, loss_ce: 0.010636\n","iteration 12141 : loss : 0.037952, loss_ce: 0.007190\n","iteration 12142 : loss : 0.039187, loss_ce: 0.011869\n","iteration 12143 : loss : 0.041874, loss_ce: 0.017968\n","iteration 12144 : loss : 0.032042, loss_ce: 0.012161\n","iteration 12145 : loss : 0.037410, loss_ce: 0.009176\n","iteration 12146 : loss : 0.035128, loss_ce: 0.010876\n","iteration 12147 : loss : 0.048980, loss_ce: 0.010283\n","iteration 12148 : loss : 0.046718, loss_ce: 0.013000\n","iteration 12149 : loss : 0.041444, loss_ce: 0.012410\n","iteration 12150 : loss : 0.037806, loss_ce: 0.009770\n","iteration 12151 : loss : 0.036234, loss_ce: 0.012506\n","iteration 12152 : loss : 0.036247, loss_ce: 0.013967\n","iteration 12153 : loss : 0.036584, loss_ce: 0.014102\n","iteration 12154 : loss : 0.038524, loss_ce: 0.016844\n","iteration 12155 : loss : 0.042646, loss_ce: 0.014186\n","iteration 12156 : loss : 0.035959, loss_ce: 0.009981\n","iteration 12157 : loss : 0.039069, loss_ce: 0.010184\n","iteration 12158 : loss : 0.032803, loss_ce: 0.009896\n","iteration 12159 : loss : 0.041981, loss_ce: 0.016946\n","iteration 12160 : loss : 0.039814, loss_ce: 0.019782\n","iteration 12161 : loss : 0.045419, loss_ce: 0.003432\n","iteration 12162 : loss : 0.048664, loss_ce: 0.011778\n","iteration 12163 : loss : 0.036876, loss_ce: 0.010085\n","iteration 12164 : loss : 0.048085, loss_ce: 0.014221\n","iteration 12165 : loss : 0.030458, loss_ce: 0.010526\n","iteration 12166 : loss : 0.053389, loss_ce: 0.014239\n","iteration 12167 : loss : 0.045382, loss_ce: 0.017293\n","iteration 12168 : loss : 0.040454, loss_ce: 0.018109\n","iteration 12169 : loss : 0.036439, loss_ce: 0.010048\n","iteration 12170 : loss : 0.039247, loss_ce: 0.008867\n","iteration 12171 : loss : 0.033982, loss_ce: 0.012541\n","iteration 12172 : loss : 0.097866, loss_ce: 0.009197\n","iteration 12173 : loss : 0.036239, loss_ce: 0.008142\n","iteration 12174 : loss : 0.042141, loss_ce: 0.018537\n","iteration 12175 : loss : 0.057745, loss_ce: 0.016644\n","iteration 12176 : loss : 0.066475, loss_ce: 0.012810\n","iteration 12177 : loss : 0.045781, loss_ce: 0.011860\n","iteration 12178 : loss : 0.032921, loss_ce: 0.008624\n","iteration 12179 : loss : 0.043452, loss_ce: 0.013814\n","iteration 12180 : loss : 0.038351, loss_ce: 0.012181\n","iteration 12181 : loss : 0.044360, loss_ce: 0.009413\n","iteration 12182 : loss : 0.039359, loss_ce: 0.013639\n","iteration 12183 : loss : 0.058899, loss_ce: 0.000012\n"," 87%|█████████████████████████▎   | 131/150 [2:34:01<22:12, 70.14s/it]iteration 12184 : loss : 0.032558, loss_ce: 0.008171\n","iteration 12185 : loss : 0.047458, loss_ce: 0.016698\n","iteration 12186 : loss : 0.103966, loss_ce: 0.010962\n","iteration 12187 : loss : 0.043340, loss_ce: 0.011890\n","iteration 12188 : loss : 0.043949, loss_ce: 0.012222\n","iteration 12189 : loss : 0.039711, loss_ce: 0.017278\n","iteration 12190 : loss : 0.101535, loss_ce: 0.012129\n","iteration 12191 : loss : 0.036981, loss_ce: 0.007540\n","iteration 12192 : loss : 0.039390, loss_ce: 0.014071\n","iteration 12193 : loss : 0.043764, loss_ce: 0.011412\n","iteration 12194 : loss : 0.040154, loss_ce: 0.020659\n","iteration 12195 : loss : 0.035363, loss_ce: 0.011692\n","iteration 12196 : loss : 0.048068, loss_ce: 0.013040\n","iteration 12197 : loss : 0.037773, loss_ce: 0.016894\n","iteration 12198 : loss : 0.156450, loss_ce: 0.007986\n","iteration 12199 : loss : 0.038506, loss_ce: 0.009815\n","iteration 12200 : loss : 0.041935, loss_ce: 0.015683\n","iteration 12201 : loss : 0.052134, loss_ce: 0.022426\n","iteration 12202 : loss : 0.033104, loss_ce: 0.008569\n","iteration 12203 : loss : 0.038402, loss_ce: 0.012554\n","iteration 12204 : loss : 0.039305, loss_ce: 0.010159\n","iteration 12205 : loss : 0.037390, loss_ce: 0.009718\n","iteration 12206 : loss : 0.037046, loss_ce: 0.010885\n","iteration 12207 : loss : 0.039964, loss_ce: 0.009128\n","iteration 12208 : loss : 0.099249, loss_ce: 0.004610\n","iteration 12209 : loss : 0.035270, loss_ce: 0.014016\n","iteration 12210 : loss : 0.035583, loss_ce: 0.012837\n","iteration 12211 : loss : 0.038368, loss_ce: 0.010492\n","iteration 12212 : loss : 0.030213, loss_ce: 0.008480\n","iteration 12213 : loss : 0.035897, loss_ce: 0.014774\n","iteration 12214 : loss : 0.039514, loss_ce: 0.014680\n","iteration 12215 : loss : 0.043462, loss_ce: 0.013083\n","iteration 12216 : loss : 0.038787, loss_ce: 0.009582\n","iteration 12217 : loss : 0.036366, loss_ce: 0.016388\n","iteration 12218 : loss : 0.037892, loss_ce: 0.015703\n","iteration 12219 : loss : 0.043673, loss_ce: 0.014938\n","iteration 12220 : loss : 0.034479, loss_ce: 0.013219\n","iteration 12221 : loss : 0.036332, loss_ce: 0.009741\n","iteration 12222 : loss : 0.281563, loss_ce: 0.001942\n","iteration 12223 : loss : 0.041164, loss_ce: 0.015762\n","iteration 12224 : loss : 0.045812, loss_ce: 0.010497\n","iteration 12225 : loss : 0.037950, loss_ce: 0.010957\n","iteration 12226 : loss : 0.041954, loss_ce: 0.014738\n","iteration 12227 : loss : 0.046664, loss_ce: 0.016066\n","iteration 12228 : loss : 0.041549, loss_ce: 0.012399\n","iteration 12229 : loss : 0.040838, loss_ce: 0.011162\n","iteration 12230 : loss : 0.038067, loss_ce: 0.011965\n","iteration 12231 : loss : 0.033726, loss_ce: 0.010494\n","iteration 12232 : loss : 0.043739, loss_ce: 0.017859\n","iteration 12233 : loss : 0.039811, loss_ce: 0.011702\n","iteration 12234 : loss : 0.037943, loss_ce: 0.013447\n","iteration 12235 : loss : 0.044655, loss_ce: 0.012567\n","iteration 12236 : loss : 0.042353, loss_ce: 0.006931\n","iteration 12237 : loss : 0.036426, loss_ce: 0.010285\n","iteration 12238 : loss : 0.160982, loss_ce: 0.008803\n","iteration 12239 : loss : 0.036028, loss_ce: 0.013568\n","iteration 12240 : loss : 0.039408, loss_ce: 0.006247\n","iteration 12241 : loss : 0.033631, loss_ce: 0.012428\n","iteration 12242 : loss : 0.041857, loss_ce: 0.017196\n","iteration 12243 : loss : 0.041769, loss_ce: 0.012382\n","iteration 12244 : loss : 0.060617, loss_ce: 0.019187\n","iteration 12245 : loss : 0.044428, loss_ce: 0.012802\n","iteration 12246 : loss : 0.033097, loss_ce: 0.009724\n","iteration 12247 : loss : 0.035620, loss_ce: 0.013249\n","iteration 12248 : loss : 0.042574, loss_ce: 0.010324\n","iteration 12249 : loss : 0.034726, loss_ce: 0.009466\n","iteration 12250 : loss : 0.037984, loss_ce: 0.015485\n","iteration 12251 : loss : 0.040863, loss_ce: 0.014540\n","iteration 12252 : loss : 0.046895, loss_ce: 0.012421\n","iteration 12253 : loss : 0.038045, loss_ce: 0.015917\n","iteration 12254 : loss : 0.034215, loss_ce: 0.013403\n","iteration 12255 : loss : 0.042113, loss_ce: 0.008694\n","iteration 12256 : loss : 0.039074, loss_ce: 0.013061\n","iteration 12257 : loss : 0.050976, loss_ce: 0.016764\n","iteration 12258 : loss : 0.041037, loss_ce: 0.010125\n","iteration 12259 : loss : 0.043671, loss_ce: 0.007455\n","iteration 12260 : loss : 0.051880, loss_ce: 0.009263\n","iteration 12261 : loss : 0.044705, loss_ce: 0.016965\n","iteration 12262 : loss : 0.041979, loss_ce: 0.011997\n","iteration 12263 : loss : 0.039829, loss_ce: 0.012720\n","iteration 12264 : loss : 0.041579, loss_ce: 0.013097\n","iteration 12265 : loss : 0.037692, loss_ce: 0.009823\n","iteration 12266 : loss : 0.034677, loss_ce: 0.011353\n","iteration 12267 : loss : 0.043719, loss_ce: 0.019509\n","iteration 12268 : loss : 0.047065, loss_ce: 0.012188\n","iteration 12269 : loss : 0.044568, loss_ce: 0.016988\n","iteration 12270 : loss : 0.036711, loss_ce: 0.011815\n","iteration 12271 : loss : 0.030570, loss_ce: 0.012131\n","iteration 12272 : loss : 0.039159, loss_ce: 0.005773\n","iteration 12273 : loss : 0.035420, loss_ce: 0.009719\n","iteration 12274 : loss : 0.043824, loss_ce: 0.014496\n","iteration 12275 : loss : 0.038307, loss_ce: 0.013354\n","iteration 12276 : loss : 0.095750, loss_ce: 0.022458\n"," 88%|█████████████████████████▌   | 132/150 [2:35:11<21:02, 70.12s/it]iteration 12277 : loss : 0.036346, loss_ce: 0.017185\n","iteration 12278 : loss : 0.054755, loss_ce: 0.012563\n","iteration 12279 : loss : 0.032860, loss_ce: 0.010242\n","iteration 12280 : loss : 0.031349, loss_ce: 0.015200\n","iteration 12281 : loss : 0.040234, loss_ce: 0.014059\n","iteration 12282 : loss : 0.092542, loss_ce: 0.009373\n","iteration 12283 : loss : 0.041309, loss_ce: 0.007303\n","iteration 12284 : loss : 0.040120, loss_ce: 0.016747\n","iteration 12285 : loss : 0.040203, loss_ce: 0.013106\n","iteration 12286 : loss : 0.040547, loss_ce: 0.014073\n","iteration 12287 : loss : 0.036580, loss_ce: 0.011059\n","iteration 12288 : loss : 0.036795, loss_ce: 0.014623\n","iteration 12289 : loss : 0.035982, loss_ce: 0.013744\n","iteration 12290 : loss : 0.035398, loss_ce: 0.014087\n","iteration 12291 : loss : 0.039560, loss_ce: 0.009577\n","iteration 12292 : loss : 0.036467, loss_ce: 0.010928\n","iteration 12293 : loss : 0.033164, loss_ce: 0.013672\n","iteration 12294 : loss : 0.045489, loss_ce: 0.018111\n","iteration 12295 : loss : 0.035181, loss_ce: 0.006125\n","iteration 12296 : loss : 0.037494, loss_ce: 0.010373\n","iteration 12297 : loss : 0.043242, loss_ce: 0.013929\n","iteration 12298 : loss : 0.038740, loss_ce: 0.013897\n","iteration 12299 : loss : 0.044711, loss_ce: 0.008198\n","iteration 12300 : loss : 0.107003, loss_ce: 0.015476\n","iteration 12301 : loss : 0.039737, loss_ce: 0.008431\n","iteration 12302 : loss : 0.036283, loss_ce: 0.008558\n","iteration 12303 : loss : 0.035837, loss_ce: 0.016188\n","iteration 12304 : loss : 0.043993, loss_ce: 0.011732\n","iteration 12305 : loss : 0.042734, loss_ce: 0.018545\n","iteration 12306 : loss : 0.029785, loss_ce: 0.010700\n","iteration 12307 : loss : 0.039428, loss_ce: 0.010009\n","iteration 12308 : loss : 0.098701, loss_ce: 0.012063\n","iteration 12309 : loss : 0.035813, loss_ce: 0.009384\n","iteration 12310 : loss : 0.045480, loss_ce: 0.019402\n","iteration 12311 : loss : 0.047543, loss_ce: 0.007866\n","iteration 12312 : loss : 0.040665, loss_ce: 0.016552\n","iteration 12313 : loss : 0.041685, loss_ce: 0.004618\n","iteration 12314 : loss : 0.034994, loss_ce: 0.014177\n","iteration 12315 : loss : 0.047120, loss_ce: 0.014784\n","iteration 12316 : loss : 0.037676, loss_ce: 0.014999\n","iteration 12317 : loss : 0.042445, loss_ce: 0.010080\n","iteration 12318 : loss : 0.039664, loss_ce: 0.010928\n","iteration 12319 : loss : 0.045043, loss_ce: 0.016688\n","iteration 12320 : loss : 0.036444, loss_ce: 0.013861\n","iteration 12321 : loss : 0.037796, loss_ce: 0.009425\n","iteration 12322 : loss : 0.039789, loss_ce: 0.008658\n","iteration 12323 : loss : 0.042859, loss_ce: 0.017816\n","iteration 12324 : loss : 0.032270, loss_ce: 0.011083\n","iteration 12325 : loss : 0.080521, loss_ce: 0.008052\n","iteration 12326 : loss : 0.039912, loss_ce: 0.008075\n","iteration 12327 : loss : 0.045261, loss_ce: 0.018055\n","iteration 12328 : loss : 0.043935, loss_ce: 0.008065\n","iteration 12329 : loss : 0.099947, loss_ce: 0.004253\n","iteration 12330 : loss : 0.032546, loss_ce: 0.009362\n","iteration 12331 : loss : 0.047269, loss_ce: 0.006457\n","iteration 12332 : loss : 0.037409, loss_ce: 0.012935\n","iteration 12333 : loss : 0.043540, loss_ce: 0.015737\n","iteration 12334 : loss : 0.039258, loss_ce: 0.016221\n","iteration 12335 : loss : 0.038566, loss_ce: 0.015502\n","iteration 12336 : loss : 0.047018, loss_ce: 0.016835\n","iteration 12337 : loss : 0.037000, loss_ce: 0.006019\n","iteration 12338 : loss : 0.045037, loss_ce: 0.013281\n","iteration 12339 : loss : 0.035775, loss_ce: 0.010768\n","iteration 12340 : loss : 0.040084, loss_ce: 0.012456\n","iteration 12341 : loss : 0.045731, loss_ce: 0.014140\n","iteration 12342 : loss : 0.030906, loss_ce: 0.007303\n","iteration 12343 : loss : 0.043917, loss_ce: 0.010451\n","iteration 12344 : loss : 0.024052, loss_ce: 0.003520\n","iteration 12345 : loss : 0.045243, loss_ce: 0.014041\n","iteration 12346 : loss : 0.046728, loss_ce: 0.016211\n","iteration 12347 : loss : 0.034930, loss_ce: 0.012578\n","iteration 12348 : loss : 0.035943, loss_ce: 0.014005\n","iteration 12349 : loss : 0.045309, loss_ce: 0.011018\n","iteration 12350 : loss : 0.041357, loss_ce: 0.011454\n","iteration 12351 : loss : 0.034035, loss_ce: 0.010048\n","iteration 12352 : loss : 0.036577, loss_ce: 0.013294\n","iteration 12353 : loss : 0.035841, loss_ce: 0.015576\n","iteration 12354 : loss : 0.055970, loss_ce: 0.013213\n","iteration 12355 : loss : 0.041235, loss_ce: 0.012276\n","iteration 12356 : loss : 0.094868, loss_ce: 0.010602\n","iteration 12357 : loss : 0.044416, loss_ce: 0.015181\n","iteration 12358 : loss : 0.041833, loss_ce: 0.010993\n","iteration 12359 : loss : 0.037142, loss_ce: 0.007899\n","iteration 12360 : loss : 0.033900, loss_ce: 0.010998\n","iteration 12361 : loss : 0.038533, loss_ce: 0.011526\n","iteration 12362 : loss : 0.103054, loss_ce: 0.011305\n","iteration 12363 : loss : 0.032324, loss_ce: 0.011527\n","iteration 12364 : loss : 0.034772, loss_ce: 0.009349\n","iteration 12365 : loss : 0.100869, loss_ce: 0.007113\n","iteration 12366 : loss : 0.042846, loss_ce: 0.013098\n","iteration 12367 : loss : 0.050833, loss_ce: 0.010670\n","iteration 12368 : loss : 0.040787, loss_ce: 0.014733\n","iteration 12369 : loss : 0.040489, loss_ce: 0.018776\n"," 89%|█████████████████████████▋   | 133/150 [2:36:21<19:51, 70.08s/it]iteration 12370 : loss : 0.037961, loss_ce: 0.012741\n","iteration 12371 : loss : 0.036741, loss_ce: 0.011622\n","iteration 12372 : loss : 0.041889, loss_ce: 0.017148\n","iteration 12373 : loss : 0.042659, loss_ce: 0.019221\n","iteration 12374 : loss : 0.034293, loss_ce: 0.011100\n","iteration 12375 : loss : 0.040114, loss_ce: 0.014537\n","iteration 12376 : loss : 0.041784, loss_ce: 0.018091\n","iteration 12377 : loss : 0.035543, loss_ce: 0.012965\n","iteration 12378 : loss : 0.093212, loss_ce: 0.007764\n","iteration 12379 : loss : 0.043531, loss_ce: 0.015501\n","iteration 12380 : loss : 0.056899, loss_ce: 0.007542\n","iteration 12381 : loss : 0.037904, loss_ce: 0.013275\n","iteration 12382 : loss : 0.032501, loss_ce: 0.011501\n","iteration 12383 : loss : 0.034162, loss_ce: 0.011549\n","iteration 12384 : loss : 0.040311, loss_ce: 0.008489\n","iteration 12385 : loss : 0.099313, loss_ce: 0.006997\n","iteration 12386 : loss : 0.039304, loss_ce: 0.010711\n","iteration 12387 : loss : 0.032414, loss_ce: 0.012998\n","iteration 12388 : loss : 0.039190, loss_ce: 0.011794\n","iteration 12389 : loss : 0.044893, loss_ce: 0.014648\n","iteration 12390 : loss : 0.037909, loss_ce: 0.008096\n","iteration 12391 : loss : 0.032447, loss_ce: 0.013387\n","iteration 12392 : loss : 0.039255, loss_ce: 0.015198\n","iteration 12393 : loss : 0.032753, loss_ce: 0.009782\n","iteration 12394 : loss : 0.042923, loss_ce: 0.006975\n","iteration 12395 : loss : 0.038829, loss_ce: 0.013887\n","iteration 12396 : loss : 0.047978, loss_ce: 0.005547\n","iteration 12397 : loss : 0.092345, loss_ce: 0.007175\n","iteration 12398 : loss : 0.097928, loss_ce: 0.010602\n","iteration 12399 : loss : 0.054135, loss_ce: 0.014552\n","iteration 12400 : loss : 0.036574, loss_ce: 0.010352\n","iteration 12401 : loss : 0.039716, loss_ce: 0.008416\n","iteration 12402 : loss : 0.039546, loss_ce: 0.014757\n","iteration 12403 : loss : 0.047025, loss_ce: 0.013434\n","iteration 12404 : loss : 0.046994, loss_ce: 0.013642\n","iteration 12405 : loss : 0.034793, loss_ce: 0.011536\n","iteration 12406 : loss : 0.047065, loss_ce: 0.015147\n","iteration 12407 : loss : 0.049607, loss_ce: 0.011459\n","iteration 12408 : loss : 0.045926, loss_ce: 0.008431\n","iteration 12409 : loss : 0.039114, loss_ce: 0.008906\n","iteration 12410 : loss : 0.038324, loss_ce: 0.008384\n","iteration 12411 : loss : 0.039557, loss_ce: 0.009865\n","iteration 12412 : loss : 0.042641, loss_ce: 0.016585\n","iteration 12413 : loss : 0.039859, loss_ce: 0.006989\n","iteration 12414 : loss : 0.036385, loss_ce: 0.011780\n","iteration 12415 : loss : 0.045926, loss_ce: 0.011160\n","iteration 12416 : loss : 0.039476, loss_ce: 0.013900\n","iteration 12417 : loss : 0.033735, loss_ce: 0.013065\n","iteration 12418 : loss : 0.039612, loss_ce: 0.017219\n","iteration 12419 : loss : 0.043120, loss_ce: 0.016210\n","iteration 12420 : loss : 0.096535, loss_ce: 0.005263\n","iteration 12421 : loss : 0.037058, loss_ce: 0.011848\n","iteration 12422 : loss : 0.037687, loss_ce: 0.015580\n","iteration 12423 : loss : 0.036434, loss_ce: 0.009594\n","iteration 12424 : loss : 0.037316, loss_ce: 0.019014\n","iteration 12425 : loss : 0.033742, loss_ce: 0.010733\n","iteration 12426 : loss : 0.099214, loss_ce: 0.010005\n","iteration 12427 : loss : 0.040332, loss_ce: 0.013791\n","iteration 12428 : loss : 0.044879, loss_ce: 0.008752\n","iteration 12429 : loss : 0.039237, loss_ce: 0.007896\n","iteration 12430 : loss : 0.037470, loss_ce: 0.008330\n","iteration 12431 : loss : 0.038709, loss_ce: 0.011237\n","iteration 12432 : loss : 0.037703, loss_ce: 0.015570\n","iteration 12433 : loss : 0.042358, loss_ce: 0.014450\n","iteration 12434 : loss : 0.039892, loss_ce: 0.013674\n","iteration 12435 : loss : 0.041060, loss_ce: 0.014485\n","iteration 12436 : loss : 0.044590, loss_ce: 0.011613\n","iteration 12437 : loss : 0.037490, loss_ce: 0.008244\n","iteration 12438 : loss : 0.099630, loss_ce: 0.009819\n","iteration 12439 : loss : 0.037953, loss_ce: 0.019407\n","iteration 12440 : loss : 0.043995, loss_ce: 0.011305\n","iteration 12441 : loss : 0.036480, loss_ce: 0.012135\n","iteration 12442 : loss : 0.042943, loss_ce: 0.010467\n","iteration 12443 : loss : 0.033831, loss_ce: 0.008782\n","iteration 12444 : loss : 0.043564, loss_ce: 0.011910\n","iteration 12445 : loss : 0.042142, loss_ce: 0.012956\n","iteration 12446 : loss : 0.033579, loss_ce: 0.009372\n","iteration 12447 : loss : 0.042299, loss_ce: 0.014708\n","iteration 12448 : loss : 0.034939, loss_ce: 0.008445\n","iteration 12449 : loss : 0.099183, loss_ce: 0.008904\n","iteration 12450 : loss : 0.032099, loss_ce: 0.010258\n","iteration 12451 : loss : 0.036918, loss_ce: 0.011005\n","iteration 12452 : loss : 0.048589, loss_ce: 0.013750\n","iteration 12453 : loss : 0.051062, loss_ce: 0.014254\n","iteration 12454 : loss : 0.034192, loss_ce: 0.011243\n","iteration 12455 : loss : 0.039346, loss_ce: 0.012501\n","iteration 12456 : loss : 0.035931, loss_ce: 0.014471\n","iteration 12457 : loss : 0.035593, loss_ce: 0.015078\n","iteration 12458 : loss : 0.091912, loss_ce: 0.008098\n","iteration 12459 : loss : 0.030529, loss_ce: 0.011254\n","iteration 12460 : loss : 0.044236, loss_ce: 0.012168\n","iteration 12461 : loss : 0.030913, loss_ce: 0.007287\n","iteration 12462 : loss : 0.003188, loss_ce: 0.000004\n"," 89%|█████████████████████████▉   | 134/150 [2:37:31<18:40, 70.02s/it]iteration 12463 : loss : 0.038185, loss_ce: 0.010527\n","iteration 12464 : loss : 0.041085, loss_ce: 0.012337\n","iteration 12465 : loss : 0.094290, loss_ce: 0.008474\n","iteration 12466 : loss : 0.151963, loss_ce: 0.009200\n","iteration 12467 : loss : 0.040376, loss_ce: 0.019370\n","iteration 12468 : loss : 0.045238, loss_ce: 0.016251\n","iteration 12469 : loss : 0.036251, loss_ce: 0.014347\n","iteration 12470 : loss : 0.036768, loss_ce: 0.011118\n","iteration 12471 : loss : 0.031175, loss_ce: 0.010734\n","iteration 12472 : loss : 0.038215, loss_ce: 0.014200\n","iteration 12473 : loss : 0.035652, loss_ce: 0.011622\n","iteration 12474 : loss : 0.036175, loss_ce: 0.012353\n","iteration 12475 : loss : 0.037002, loss_ce: 0.008805\n","iteration 12476 : loss : 0.031286, loss_ce: 0.006403\n","iteration 12477 : loss : 0.032541, loss_ce: 0.012590\n","iteration 12478 : loss : 0.034983, loss_ce: 0.014950\n","iteration 12479 : loss : 0.026316, loss_ce: 0.006584\n","iteration 12480 : loss : 0.034678, loss_ce: 0.012932\n","iteration 12481 : loss : 0.036515, loss_ce: 0.013672\n","iteration 12482 : loss : 0.094107, loss_ce: 0.011573\n","iteration 12483 : loss : 0.036009, loss_ce: 0.015901\n","iteration 12484 : loss : 0.036430, loss_ce: 0.010401\n","iteration 12485 : loss : 0.037220, loss_ce: 0.008900\n","iteration 12486 : loss : 0.096445, loss_ce: 0.010367\n","iteration 12487 : loss : 0.037822, loss_ce: 0.008164\n","iteration 12488 : loss : 0.042658, loss_ce: 0.012095\n","iteration 12489 : loss : 0.034946, loss_ce: 0.014191\n","iteration 12490 : loss : 0.041044, loss_ce: 0.011713\n","iteration 12491 : loss : 0.037855, loss_ce: 0.011505\n","iteration 12492 : loss : 0.036546, loss_ce: 0.006820\n","iteration 12493 : loss : 0.040932, loss_ce: 0.012211\n","iteration 12494 : loss : 0.031297, loss_ce: 0.013076\n","iteration 12495 : loss : 0.099398, loss_ce: 0.007742\n","iteration 12496 : loss : 0.043156, loss_ce: 0.007789\n","iteration 12497 : loss : 0.045390, loss_ce: 0.009036\n","iteration 12498 : loss : 0.096256, loss_ce: 0.009761\n","iteration 12499 : loss : 0.036272, loss_ce: 0.009879\n","iteration 12500 : loss : 0.052077, loss_ce: 0.008496\n","iteration 12501 : loss : 0.036877, loss_ce: 0.013141\n","iteration 12502 : loss : 0.096094, loss_ce: 0.010411\n","iteration 12503 : loss : 0.032276, loss_ce: 0.008992\n","iteration 12504 : loss : 0.037510, loss_ce: 0.014329\n","iteration 12505 : loss : 0.050250, loss_ce: 0.014986\n","iteration 12506 : loss : 0.034536, loss_ce: 0.011288\n","iteration 12507 : loss : 0.052677, loss_ce: 0.012408\n","iteration 12508 : loss : 0.040984, loss_ce: 0.012101\n","iteration 12509 : loss : 0.048272, loss_ce: 0.019356\n","iteration 12510 : loss : 0.037036, loss_ce: 0.012803\n","iteration 12511 : loss : 0.035976, loss_ce: 0.010247\n","iteration 12512 : loss : 0.041120, loss_ce: 0.014837\n","iteration 12513 : loss : 0.047062, loss_ce: 0.019566\n","iteration 12514 : loss : 0.044929, loss_ce: 0.012104\n","iteration 12515 : loss : 0.042823, loss_ce: 0.013443\n","iteration 12516 : loss : 0.037591, loss_ce: 0.009856\n","iteration 12517 : loss : 0.041831, loss_ce: 0.009631\n","iteration 12518 : loss : 0.045699, loss_ce: 0.011847\n","iteration 12519 : loss : 0.033740, loss_ce: 0.010279\n","iteration 12520 : loss : 0.031972, loss_ce: 0.013674\n","iteration 12521 : loss : 0.037911, loss_ce: 0.014348\n","iteration 12522 : loss : 0.038181, loss_ce: 0.014389\n","iteration 12523 : loss : 0.035783, loss_ce: 0.014174\n","iteration 12524 : loss : 0.038940, loss_ce: 0.011766\n","iteration 12525 : loss : 0.040006, loss_ce: 0.017964\n","iteration 12526 : loss : 0.095490, loss_ce: 0.007260\n","iteration 12527 : loss : 0.038724, loss_ce: 0.012497\n","iteration 12528 : loss : 0.037928, loss_ce: 0.010658\n","iteration 12529 : loss : 0.038547, loss_ce: 0.009714\n","iteration 12530 : loss : 0.095343, loss_ce: 0.009771\n","iteration 12531 : loss : 0.045514, loss_ce: 0.017843\n","iteration 12532 : loss : 0.063479, loss_ce: 0.006806\n","iteration 12533 : loss : 0.041475, loss_ce: 0.012070\n","iteration 12534 : loss : 0.092783, loss_ce: 0.007964\n","iteration 12535 : loss : 0.036565, loss_ce: 0.011684\n","iteration 12536 : loss : 0.049673, loss_ce: 0.007620\n","iteration 12537 : loss : 0.050811, loss_ce: 0.007933\n","iteration 12538 : loss : 0.037014, loss_ce: 0.007418\n","iteration 12539 : loss : 0.048501, loss_ce: 0.016442\n","iteration 12540 : loss : 0.040608, loss_ce: 0.021447\n","iteration 12541 : loss : 0.035453, loss_ce: 0.007846\n","iteration 12542 : loss : 0.045219, loss_ce: 0.014405\n","iteration 12543 : loss : 0.035074, loss_ce: 0.006312\n","iteration 12544 : loss : 0.036418, loss_ce: 0.010162\n","iteration 12545 : loss : 0.039152, loss_ce: 0.012893\n","iteration 12546 : loss : 0.035467, loss_ce: 0.012564\n","iteration 12547 : loss : 0.039308, loss_ce: 0.014747\n","iteration 12548 : loss : 0.039790, loss_ce: 0.012688\n","iteration 12549 : loss : 0.031708, loss_ce: 0.008626\n","iteration 12550 : loss : 0.033580, loss_ce: 0.011181\n","iteration 12551 : loss : 0.038867, loss_ce: 0.013385\n","iteration 12552 : loss : 0.033028, loss_ce: 0.009534\n","iteration 12553 : loss : 0.102908, loss_ce: 0.011056\n","iteration 12554 : loss : 0.030820, loss_ce: 0.007852\n","iteration 12555 : loss : 0.189670, loss_ce: 0.000427\n"," 90%|██████████████████████████   | 135/150 [2:38:40<17:29, 69.94s/it]iteration 12556 : loss : 0.040408, loss_ce: 0.013848\n","iteration 12557 : loss : 0.035657, loss_ce: 0.010833\n","iteration 12558 : loss : 0.051772, loss_ce: 0.026713\n","iteration 12559 : loss : 0.063971, loss_ce: 0.053629\n","iteration 12560 : loss : 0.072438, loss_ce: 0.014424\n","iteration 12561 : loss : 0.075099, loss_ce: 0.044016\n","iteration 12562 : loss : 0.096219, loss_ce: 0.068781\n","iteration 12563 : loss : 0.089564, loss_ce: 0.035781\n","iteration 12564 : loss : 0.038024, loss_ce: 0.011611\n","iteration 12565 : loss : 0.105262, loss_ce: 0.024702\n","iteration 12566 : loss : 0.040328, loss_ce: 0.018584\n","iteration 12567 : loss : 0.051761, loss_ce: 0.024152\n","iteration 12568 : loss : 0.104652, loss_ce: 0.016430\n","iteration 12569 : loss : 0.108355, loss_ce: 0.017975\n","iteration 12570 : loss : 0.043039, loss_ce: 0.014367\n","iteration 12571 : loss : 0.044661, loss_ce: 0.015559\n","iteration 12572 : loss : 0.052531, loss_ce: 0.020440\n","iteration 12573 : loss : 0.037272, loss_ce: 0.016953\n","iteration 12574 : loss : 0.042194, loss_ce: 0.011344\n","iteration 12575 : loss : 0.051365, loss_ce: 0.012548\n","iteration 12576 : loss : 0.046237, loss_ce: 0.014674\n","iteration 12577 : loss : 0.034263, loss_ce: 0.012208\n","iteration 12578 : loss : 0.049859, loss_ce: 0.019753\n","iteration 12579 : loss : 0.040653, loss_ce: 0.008112\n","iteration 12580 : loss : 0.039934, loss_ce: 0.015031\n","iteration 12581 : loss : 0.034013, loss_ce: 0.013471\n","iteration 12582 : loss : 0.040199, loss_ce: 0.017089\n","iteration 12583 : loss : 0.041378, loss_ce: 0.014735\n","iteration 12584 : loss : 0.041307, loss_ce: 0.013028\n","iteration 12585 : loss : 0.040596, loss_ce: 0.012695\n","iteration 12586 : loss : 0.037173, loss_ce: 0.010861\n","iteration 12587 : loss : 0.037217, loss_ce: 0.013877\n","iteration 12588 : loss : 0.040665, loss_ce: 0.010575\n","iteration 12589 : loss : 0.040132, loss_ce: 0.018631\n","iteration 12590 : loss : 0.101221, loss_ce: 0.013811\n","iteration 12591 : loss : 0.037434, loss_ce: 0.014580\n","iteration 12592 : loss : 0.045643, loss_ce: 0.013008\n","iteration 12593 : loss : 0.096966, loss_ce: 0.012058\n","iteration 12594 : loss : 0.050367, loss_ce: 0.017588\n","iteration 12595 : loss : 0.053278, loss_ce: 0.011095\n","iteration 12596 : loss : 0.047968, loss_ce: 0.009862\n","iteration 12597 : loss : 0.044238, loss_ce: 0.011478\n","iteration 12598 : loss : 0.045281, loss_ce: 0.012103\n","iteration 12599 : loss : 0.038926, loss_ce: 0.014328\n","iteration 12600 : loss : 0.030618, loss_ce: 0.006431\n","iteration 12601 : loss : 0.044713, loss_ce: 0.019134\n","iteration 12602 : loss : 0.042313, loss_ce: 0.017102\n","iteration 12603 : loss : 0.042154, loss_ce: 0.011011\n","iteration 12604 : loss : 0.041746, loss_ce: 0.012438\n","iteration 12605 : loss : 0.079484, loss_ce: 0.010776\n","iteration 12606 : loss : 0.044322, loss_ce: 0.015277\n","iteration 12607 : loss : 0.050587, loss_ce: 0.016664\n","iteration 12608 : loss : 0.044235, loss_ce: 0.012954\n","iteration 12609 : loss : 0.044580, loss_ce: 0.024053\n","iteration 12610 : loss : 0.040718, loss_ce: 0.015242\n","iteration 12611 : loss : 0.106913, loss_ce: 0.013022\n","iteration 12612 : loss : 0.037011, loss_ce: 0.011171\n","iteration 12613 : loss : 0.041542, loss_ce: 0.012094\n","iteration 12614 : loss : 0.053073, loss_ce: 0.019547\n","iteration 12615 : loss : 0.035753, loss_ce: 0.014810\n","iteration 12616 : loss : 0.042073, loss_ce: 0.016613\n","iteration 12617 : loss : 0.068625, loss_ce: 0.012593\n","iteration 12618 : loss : 0.040324, loss_ce: 0.012770\n","iteration 12619 : loss : 0.048607, loss_ce: 0.009648\n","iteration 12620 : loss : 0.038909, loss_ce: 0.006283\n","iteration 12621 : loss : 0.041255, loss_ce: 0.018503\n","iteration 12622 : loss : 0.054634, loss_ce: 0.017008\n","iteration 12623 : loss : 0.040127, loss_ce: 0.015967\n","iteration 12624 : loss : 0.041115, loss_ce: 0.015449\n","iteration 12625 : loss : 0.053714, loss_ce: 0.016237\n","iteration 12626 : loss : 0.039992, loss_ce: 0.009480\n","iteration 12627 : loss : 0.045437, loss_ce: 0.013003\n","iteration 12628 : loss : 0.041707, loss_ce: 0.012880\n","iteration 12629 : loss : 0.037966, loss_ce: 0.011101\n","iteration 12630 : loss : 0.044441, loss_ce: 0.009406\n","iteration 12631 : loss : 0.033711, loss_ce: 0.007678\n","iteration 12632 : loss : 0.040956, loss_ce: 0.012376\n","iteration 12633 : loss : 0.105137, loss_ce: 0.011683\n","iteration 12634 : loss : 0.046574, loss_ce: 0.010947\n","iteration 12635 : loss : 0.040735, loss_ce: 0.011248\n","iteration 12636 : loss : 0.045892, loss_ce: 0.015894\n","iteration 12637 : loss : 0.039480, loss_ce: 0.011208\n","iteration 12638 : loss : 0.032805, loss_ce: 0.007288\n","iteration 12639 : loss : 0.045637, loss_ce: 0.019612\n","iteration 12640 : loss : 0.043078, loss_ce: 0.012700\n","iteration 12641 : loss : 0.040038, loss_ce: 0.013555\n","iteration 12642 : loss : 0.066072, loss_ce: 0.010617\n","iteration 12643 : loss : 0.031320, loss_ce: 0.007678\n","iteration 12644 : loss : 0.054954, loss_ce: 0.011016\n","iteration 12645 : loss : 0.041284, loss_ce: 0.017232\n","iteration 12646 : loss : 0.037762, loss_ce: 0.014147\n","iteration 12647 : loss : 0.035169, loss_ce: 0.012367\n","iteration 12648 : loss : 0.157751, loss_ce: 0.005881\n"," 91%|██████████████████████████▎  | 136/150 [2:39:50<16:19, 69.93s/it]iteration 12649 : loss : 0.057678, loss_ce: 0.015779\n","iteration 12650 : loss : 0.031022, loss_ce: 0.010875\n","iteration 12651 : loss : 0.041084, loss_ce: 0.018546\n","iteration 12652 : loss : 0.044276, loss_ce: 0.017662\n","iteration 12653 : loss : 0.047868, loss_ce: 0.014427\n","iteration 12654 : loss : 0.069823, loss_ce: 0.013843\n","iteration 12655 : loss : 0.041580, loss_ce: 0.013818\n","iteration 12656 : loss : 0.038984, loss_ce: 0.012059\n","iteration 12657 : loss : 0.036901, loss_ce: 0.006120\n","iteration 12658 : loss : 0.043067, loss_ce: 0.009809\n","iteration 12659 : loss : 0.029005, loss_ce: 0.007984\n","iteration 12660 : loss : 0.037074, loss_ce: 0.017233\n","iteration 12661 : loss : 0.035625, loss_ce: 0.015824\n","iteration 12662 : loss : 0.038759, loss_ce: 0.010824\n","iteration 12663 : loss : 0.029907, loss_ce: 0.007106\n","iteration 12664 : loss : 0.039189, loss_ce: 0.010360\n","iteration 12665 : loss : 0.043170, loss_ce: 0.014341\n","iteration 12666 : loss : 0.043881, loss_ce: 0.008594\n","iteration 12667 : loss : 0.036933, loss_ce: 0.012823\n","iteration 12668 : loss : 0.030959, loss_ce: 0.006832\n","iteration 12669 : loss : 0.039818, loss_ce: 0.010458\n","iteration 12670 : loss : 0.038030, loss_ce: 0.008006\n","iteration 12671 : loss : 0.032695, loss_ce: 0.009061\n","iteration 12672 : loss : 0.035007, loss_ce: 0.009091\n","iteration 12673 : loss : 0.036857, loss_ce: 0.016005\n","iteration 12674 : loss : 0.038365, loss_ce: 0.009437\n","iteration 12675 : loss : 0.038721, loss_ce: 0.024627\n","iteration 12676 : loss : 0.039168, loss_ce: 0.009566\n","iteration 12677 : loss : 0.040356, loss_ce: 0.013033\n","iteration 12678 : loss : 0.041650, loss_ce: 0.007563\n","iteration 12679 : loss : 0.096033, loss_ce: 0.005959\n","iteration 12680 : loss : 0.029788, loss_ce: 0.005688\n","iteration 12681 : loss : 0.042009, loss_ce: 0.008004\n","iteration 12682 : loss : 0.042107, loss_ce: 0.014130\n","iteration 12683 : loss : 0.041252, loss_ce: 0.011526\n","iteration 12684 : loss : 0.040173, loss_ce: 0.006541\n","iteration 12685 : loss : 0.032999, loss_ce: 0.009704\n","iteration 12686 : loss : 0.046651, loss_ce: 0.020540\n","iteration 12687 : loss : 0.034902, loss_ce: 0.010318\n","iteration 12688 : loss : 0.032922, loss_ce: 0.012217\n","iteration 12689 : loss : 0.039295, loss_ce: 0.012203\n","iteration 12690 : loss : 0.043620, loss_ce: 0.010586\n","iteration 12691 : loss : 0.043953, loss_ce: 0.014768\n","iteration 12692 : loss : 0.036922, loss_ce: 0.010855\n","iteration 12693 : loss : 0.041938, loss_ce: 0.011552\n","iteration 12694 : loss : 0.037956, loss_ce: 0.007205\n","iteration 12695 : loss : 0.041880, loss_ce: 0.008380\n","iteration 12696 : loss : 0.092771, loss_ce: 0.009059\n","iteration 12697 : loss : 0.043375, loss_ce: 0.014602\n","iteration 12698 : loss : 0.045623, loss_ce: 0.019139\n","iteration 12699 : loss : 0.045322, loss_ce: 0.015905\n","iteration 12700 : loss : 0.038675, loss_ce: 0.017443\n","iteration 12701 : loss : 0.037936, loss_ce: 0.011310\n","iteration 12702 : loss : 0.037232, loss_ce: 0.012459\n","iteration 12703 : loss : 0.041946, loss_ce: 0.015337\n","iteration 12704 : loss : 0.045346, loss_ce: 0.010885\n","iteration 12705 : loss : 0.048187, loss_ce: 0.013616\n","iteration 12706 : loss : 0.033386, loss_ce: 0.009580\n","iteration 12707 : loss : 0.032875, loss_ce: 0.007960\n","iteration 12708 : loss : 0.040028, loss_ce: 0.011017\n","iteration 12709 : loss : 0.046275, loss_ce: 0.018384\n","iteration 12710 : loss : 0.034365, loss_ce: 0.013900\n","iteration 12711 : loss : 0.040801, loss_ce: 0.011016\n","iteration 12712 : loss : 0.034945, loss_ce: 0.013964\n","iteration 12713 : loss : 0.099334, loss_ce: 0.009024\n","iteration 12714 : loss : 0.038329, loss_ce: 0.015169\n","iteration 12715 : loss : 0.040838, loss_ce: 0.011958\n","iteration 12716 : loss : 0.044403, loss_ce: 0.010720\n","iteration 12717 : loss : 0.039901, loss_ce: 0.012661\n","iteration 12718 : loss : 0.063202, loss_ce: 0.008838\n","iteration 12719 : loss : 0.035706, loss_ce: 0.013517\n","iteration 12720 : loss : 0.033612, loss_ce: 0.014687\n","iteration 12721 : loss : 0.045231, loss_ce: 0.015451\n","iteration 12722 : loss : 0.037781, loss_ce: 0.005518\n","iteration 12723 : loss : 0.033850, loss_ce: 0.010783\n","iteration 12724 : loss : 0.048930, loss_ce: 0.015795\n","iteration 12725 : loss : 0.035057, loss_ce: 0.011838\n","iteration 12726 : loss : 0.036485, loss_ce: 0.010926\n","iteration 12727 : loss : 0.033617, loss_ce: 0.009288\n","iteration 12728 : loss : 0.040094, loss_ce: 0.010669\n","iteration 12729 : loss : 0.035823, loss_ce: 0.012674\n","iteration 12730 : loss : 0.046656, loss_ce: 0.018432\n","iteration 12731 : loss : 0.035629, loss_ce: 0.010205\n","iteration 12732 : loss : 0.039151, loss_ce: 0.020332\n","iteration 12733 : loss : 0.039980, loss_ce: 0.015050\n","iteration 12734 : loss : 0.037114, loss_ce: 0.010464\n","iteration 12735 : loss : 0.038774, loss_ce: 0.012631\n","iteration 12736 : loss : 0.035912, loss_ce: 0.010444\n","iteration 12737 : loss : 0.044096, loss_ce: 0.013216\n","iteration 12738 : loss : 0.040432, loss_ce: 0.013248\n","iteration 12739 : loss : 0.035486, loss_ce: 0.015577\n","iteration 12740 : loss : 0.043653, loss_ce: 0.015958\n","iteration 12741 : loss : 0.161126, loss_ce: 0.019234\n"," 91%|██████████████████████████▍  | 137/150 [2:41:00<15:10, 70.04s/it]iteration 12742 : loss : 0.041800, loss_ce: 0.010564\n","iteration 12743 : loss : 0.040423, loss_ce: 0.013266\n","iteration 12744 : loss : 0.035429, loss_ce: 0.013362\n","iteration 12745 : loss : 0.048051, loss_ce: 0.006508\n","iteration 12746 : loss : 0.045809, loss_ce: 0.011247\n","iteration 12747 : loss : 0.038192, loss_ce: 0.008346\n","iteration 12748 : loss : 0.107570, loss_ce: 0.012296\n","iteration 12749 : loss : 0.042648, loss_ce: 0.014533\n","iteration 12750 : loss : 0.105343, loss_ce: 0.017461\n","iteration 12751 : loss : 0.028957, loss_ce: 0.008638\n","iteration 12752 : loss : 0.048597, loss_ce: 0.012806\n","iteration 12753 : loss : 0.032791, loss_ce: 0.010850\n","iteration 12754 : loss : 0.034575, loss_ce: 0.012991\n","iteration 12755 : loss : 0.043255, loss_ce: 0.012093\n","iteration 12756 : loss : 0.038109, loss_ce: 0.017806\n","iteration 12757 : loss : 0.154850, loss_ce: 0.010599\n","iteration 12758 : loss : 0.029026, loss_ce: 0.006573\n","iteration 12759 : loss : 0.042036, loss_ce: 0.014729\n","iteration 12760 : loss : 0.030050, loss_ce: 0.009610\n","iteration 12761 : loss : 0.037630, loss_ce: 0.016131\n","iteration 12762 : loss : 0.033170, loss_ce: 0.012466\n","iteration 12763 : loss : 0.042176, loss_ce: 0.018332\n","iteration 12764 : loss : 0.034827, loss_ce: 0.012846\n","iteration 12765 : loss : 0.035977, loss_ce: 0.012885\n","iteration 12766 : loss : 0.032540, loss_ce: 0.012644\n","iteration 12767 : loss : 0.038671, loss_ce: 0.011332\n","iteration 12768 : loss : 0.036517, loss_ce: 0.013597\n","iteration 12769 : loss : 0.037738, loss_ce: 0.009653\n","iteration 12770 : loss : 0.037706, loss_ce: 0.012665\n","iteration 12771 : loss : 0.067908, loss_ce: 0.010108\n","iteration 12772 : loss : 0.040947, loss_ce: 0.017835\n","iteration 12773 : loss : 0.045236, loss_ce: 0.018675\n","iteration 12774 : loss : 0.035605, loss_ce: 0.011020\n","iteration 12775 : loss : 0.045061, loss_ce: 0.013687\n","iteration 12776 : loss : 0.036466, loss_ce: 0.008320\n","iteration 12777 : loss : 0.049803, loss_ce: 0.015928\n","iteration 12778 : loss : 0.035396, loss_ce: 0.012373\n","iteration 12779 : loss : 0.032742, loss_ce: 0.016057\n","iteration 12780 : loss : 0.050838, loss_ce: 0.014137\n","iteration 12781 : loss : 0.034914, loss_ce: 0.010206\n","iteration 12782 : loss : 0.039750, loss_ce: 0.011026\n","iteration 12783 : loss : 0.037309, loss_ce: 0.008606\n","iteration 12784 : loss : 0.033051, loss_ce: 0.010864\n","iteration 12785 : loss : 0.039224, loss_ce: 0.008518\n","iteration 12786 : loss : 0.038297, loss_ce: 0.013064\n","iteration 12787 : loss : 0.038749, loss_ce: 0.010471\n","iteration 12788 : loss : 0.038892, loss_ce: 0.012379\n","iteration 12789 : loss : 0.042561, loss_ce: 0.013437\n","iteration 12790 : loss : 0.036047, loss_ce: 0.012740\n","iteration 12791 : loss : 0.035428, loss_ce: 0.010658\n","iteration 12792 : loss : 0.040902, loss_ce: 0.011338\n","iteration 12793 : loss : 0.033999, loss_ce: 0.011374\n","iteration 12794 : loss : 0.041888, loss_ce: 0.006692\n","iteration 12795 : loss : 0.024971, loss_ce: 0.005436\n","iteration 12796 : loss : 0.032307, loss_ce: 0.009246\n","iteration 12797 : loss : 0.059412, loss_ce: 0.007848\n","iteration 12798 : loss : 0.041222, loss_ce: 0.018757\n","iteration 12799 : loss : 0.041129, loss_ce: 0.009117\n","iteration 12800 : loss : 0.041175, loss_ce: 0.015839\n","iteration 12801 : loss : 0.107292, loss_ce: 0.011182\n","iteration 12802 : loss : 0.030690, loss_ce: 0.012408\n","iteration 12803 : loss : 0.037424, loss_ce: 0.014134\n","iteration 12804 : loss : 0.047993, loss_ce: 0.010388\n","iteration 12805 : loss : 0.040963, loss_ce: 0.014137\n","iteration 12806 : loss : 0.029541, loss_ce: 0.008620\n","iteration 12807 : loss : 0.228658, loss_ce: 0.001195\n","iteration 12808 : loss : 0.037208, loss_ce: 0.014563\n","iteration 12809 : loss : 0.043565, loss_ce: 0.017495\n","iteration 12810 : loss : 0.039423, loss_ce: 0.011518\n","iteration 12811 : loss : 0.039395, loss_ce: 0.011195\n","iteration 12812 : loss : 0.045038, loss_ce: 0.015829\n","iteration 12813 : loss : 0.042918, loss_ce: 0.014770\n","iteration 12814 : loss : 0.039439, loss_ce: 0.007358\n","iteration 12815 : loss : 0.037728, loss_ce: 0.013784\n","iteration 12816 : loss : 0.042291, loss_ce: 0.011050\n","iteration 12817 : loss : 0.043659, loss_ce: 0.015931\n","iteration 12818 : loss : 0.037245, loss_ce: 0.011239\n","iteration 12819 : loss : 0.037228, loss_ce: 0.010462\n","iteration 12820 : loss : 0.036583, loss_ce: 0.012817\n","iteration 12821 : loss : 0.040905, loss_ce: 0.011867\n","iteration 12822 : loss : 0.070624, loss_ce: 0.014679\n","iteration 12823 : loss : 0.093773, loss_ce: 0.006559\n","iteration 12824 : loss : 0.095384, loss_ce: 0.008071\n","iteration 12825 : loss : 0.033704, loss_ce: 0.007383\n","iteration 12826 : loss : 0.103521, loss_ce: 0.013075\n","iteration 12827 : loss : 0.037645, loss_ce: 0.013763\n","iteration 12828 : loss : 0.040936, loss_ce: 0.012504\n","iteration 12829 : loss : 0.035539, loss_ce: 0.009757\n","iteration 12830 : loss : 0.040655, loss_ce: 0.012248\n","iteration 12831 : loss : 0.044211, loss_ce: 0.008739\n","iteration 12832 : loss : 0.034545, loss_ce: 0.010413\n","iteration 12833 : loss : 0.030961, loss_ce: 0.010042\n","iteration 12834 : loss : 0.335272, loss_ce: 0.001768\n"," 92%|██████████████████████████▋  | 138/150 [2:42:10<13:59, 69.96s/it]iteration 12835 : loss : 0.029745, loss_ce: 0.014157\n","iteration 12836 : loss : 0.039974, loss_ce: 0.016891\n","iteration 12837 : loss : 0.103635, loss_ce: 0.011394\n","iteration 12838 : loss : 0.043571, loss_ce: 0.013391\n","iteration 12839 : loss : 0.050112, loss_ce: 0.013636\n","iteration 12840 : loss : 0.032451, loss_ce: 0.013119\n","iteration 12841 : loss : 0.039383, loss_ce: 0.012936\n","iteration 12842 : loss : 0.091238, loss_ce: 0.009531\n","iteration 12843 : loss : 0.045098, loss_ce: 0.016623\n","iteration 12844 : loss : 0.041443, loss_ce: 0.009401\n","iteration 12845 : loss : 0.039952, loss_ce: 0.013358\n","iteration 12846 : loss : 0.037219, loss_ce: 0.014256\n","iteration 12847 : loss : 0.094148, loss_ce: 0.008717\n","iteration 12848 : loss : 0.040023, loss_ce: 0.014196\n","iteration 12849 : loss : 0.044118, loss_ce: 0.008955\n","iteration 12850 : loss : 0.032430, loss_ce: 0.009010\n","iteration 12851 : loss : 0.043482, loss_ce: 0.012641\n","iteration 12852 : loss : 0.040589, loss_ce: 0.011294\n","iteration 12853 : loss : 0.102899, loss_ce: 0.007494\n","iteration 12854 : loss : 0.041639, loss_ce: 0.015041\n","iteration 12855 : loss : 0.036562, loss_ce: 0.011939\n","iteration 12856 : loss : 0.032848, loss_ce: 0.010425\n","iteration 12857 : loss : 0.107587, loss_ce: 0.010144\n","iteration 12858 : loss : 0.037925, loss_ce: 0.011293\n","iteration 12859 : loss : 0.036457, loss_ce: 0.010046\n","iteration 12860 : loss : 0.042584, loss_ce: 0.009252\n","iteration 12861 : loss : 0.077460, loss_ce: 0.014610\n","iteration 12862 : loss : 0.045857, loss_ce: 0.016346\n","iteration 12863 : loss : 0.052412, loss_ce: 0.010371\n","iteration 12864 : loss : 0.036997, loss_ce: 0.015310\n","iteration 12865 : loss : 0.033345, loss_ce: 0.009022\n","iteration 12866 : loss : 0.037836, loss_ce: 0.014595\n","iteration 12867 : loss : 0.031912, loss_ce: 0.008868\n","iteration 12868 : loss : 0.037667, loss_ce: 0.018501\n","iteration 12869 : loss : 0.038480, loss_ce: 0.015468\n","iteration 12870 : loss : 0.041345, loss_ce: 0.008551\n","iteration 12871 : loss : 0.041839, loss_ce: 0.012120\n","iteration 12872 : loss : 0.039297, loss_ce: 0.012408\n","iteration 12873 : loss : 0.038862, loss_ce: 0.017593\n","iteration 12874 : loss : 0.039805, loss_ce: 0.018015\n","iteration 12875 : loss : 0.037885, loss_ce: 0.009551\n","iteration 12876 : loss : 0.032242, loss_ce: 0.006397\n","iteration 12877 : loss : 0.060608, loss_ce: 0.009142\n","iteration 12878 : loss : 0.040152, loss_ce: 0.009895\n","iteration 12879 : loss : 0.062594, loss_ce: 0.011670\n","iteration 12880 : loss : 0.043753, loss_ce: 0.014692\n","iteration 12881 : loss : 0.037972, loss_ce: 0.013946\n","iteration 12882 : loss : 0.037505, loss_ce: 0.012391\n","iteration 12883 : loss : 0.041191, loss_ce: 0.012339\n","iteration 12884 : loss : 0.036634, loss_ce: 0.007088\n","iteration 12885 : loss : 0.041951, loss_ce: 0.015166\n","iteration 12886 : loss : 0.040178, loss_ce: 0.015349\n","iteration 12887 : loss : 0.103246, loss_ce: 0.009226\n","iteration 12888 : loss : 0.041600, loss_ce: 0.010603\n","iteration 12889 : loss : 0.046833, loss_ce: 0.014186\n","iteration 12890 : loss : 0.036799, loss_ce: 0.010991\n","iteration 12891 : loss : 0.048602, loss_ce: 0.009449\n","iteration 12892 : loss : 0.098187, loss_ce: 0.007405\n","iteration 12893 : loss : 0.034110, loss_ce: 0.008408\n","iteration 12894 : loss : 0.063536, loss_ce: 0.007522\n","iteration 12895 : loss : 0.040951, loss_ce: 0.012810\n","iteration 12896 : loss : 0.034083, loss_ce: 0.010519\n","iteration 12897 : loss : 0.093174, loss_ce: 0.006834\n","iteration 12898 : loss : 0.033107, loss_ce: 0.009118\n","iteration 12899 : loss : 0.102965, loss_ce: 0.010582\n","iteration 12900 : loss : 0.045464, loss_ce: 0.015597\n","iteration 12901 : loss : 0.040448, loss_ce: 0.010955\n","iteration 12902 : loss : 0.045203, loss_ce: 0.016029\n","iteration 12903 : loss : 0.038741, loss_ce: 0.006872\n","iteration 12904 : loss : 0.034231, loss_ce: 0.015237\n","iteration 12905 : loss : 0.041321, loss_ce: 0.009038\n","iteration 12906 : loss : 0.056194, loss_ce: 0.009356\n","iteration 12907 : loss : 0.037713, loss_ce: 0.009229\n","iteration 12908 : loss : 0.034643, loss_ce: 0.012239\n","iteration 12909 : loss : 0.095987, loss_ce: 0.010877\n","iteration 12910 : loss : 0.042787, loss_ce: 0.011781\n","iteration 12911 : loss : 0.040828, loss_ce: 0.009454\n","iteration 12912 : loss : 0.033733, loss_ce: 0.010556\n","iteration 12913 : loss : 0.035135, loss_ce: 0.013767\n","iteration 12914 : loss : 0.098206, loss_ce: 0.009327\n","iteration 12915 : loss : 0.048176, loss_ce: 0.015210\n","iteration 12916 : loss : 0.041124, loss_ce: 0.009426\n","iteration 12917 : loss : 0.039311, loss_ce: 0.011236\n","iteration 12918 : loss : 0.047825, loss_ce: 0.015186\n","iteration 12919 : loss : 0.042764, loss_ce: 0.011204\n","iteration 12920 : loss : 0.047431, loss_ce: 0.012449\n","iteration 12921 : loss : 0.046129, loss_ce: 0.016043\n","iteration 12922 : loss : 0.038737, loss_ce: 0.015793\n","iteration 12923 : loss : 0.045276, loss_ce: 0.024952\n","iteration 12924 : loss : 0.038143, loss_ce: 0.014027\n","iteration 12925 : loss : 0.043158, loss_ce: 0.013983\n","iteration 12926 : loss : 0.097681, loss_ce: 0.009626\n","iteration 12927 : loss : 0.097217, loss_ce: 0.016036\n"," 93%|██████████████████████████▊  | 139/150 [2:43:20<12:49, 69.99s/it]iteration 12928 : loss : 0.045935, loss_ce: 0.014501\n","iteration 12929 : loss : 0.040976, loss_ce: 0.011623\n","iteration 12930 : loss : 0.038077, loss_ce: 0.015774\n","iteration 12931 : loss : 0.037675, loss_ce: 0.011540\n","iteration 12932 : loss : 0.032057, loss_ce: 0.008945\n","iteration 12933 : loss : 0.037817, loss_ce: 0.011789\n","iteration 12934 : loss : 0.050041, loss_ce: 0.009561\n","iteration 12935 : loss : 0.038732, loss_ce: 0.009973\n","iteration 12936 : loss : 0.040522, loss_ce: 0.016664\n","iteration 12937 : loss : 0.033775, loss_ce: 0.012550\n","iteration 12938 : loss : 0.036101, loss_ce: 0.010686\n","iteration 12939 : loss : 0.049473, loss_ce: 0.010876\n","iteration 12940 : loss : 0.038526, loss_ce: 0.013606\n","iteration 12941 : loss : 0.043352, loss_ce: 0.014854\n","iteration 12942 : loss : 0.037215, loss_ce: 0.012646\n","iteration 12943 : loss : 0.037844, loss_ce: 0.013342\n","iteration 12944 : loss : 0.046978, loss_ce: 0.011314\n","iteration 12945 : loss : 0.035724, loss_ce: 0.011943\n","iteration 12946 : loss : 0.044220, loss_ce: 0.016637\n","iteration 12947 : loss : 0.039966, loss_ce: 0.009896\n","iteration 12948 : loss : 0.034693, loss_ce: 0.012336\n","iteration 12949 : loss : 0.040936, loss_ce: 0.010052\n","iteration 12950 : loss : 0.032052, loss_ce: 0.010167\n","iteration 12951 : loss : 0.037036, loss_ce: 0.009243\n","iteration 12952 : loss : 0.043646, loss_ce: 0.013333\n","iteration 12953 : loss : 0.095819, loss_ce: 0.007727\n","iteration 12954 : loss : 0.053834, loss_ce: 0.008950\n","iteration 12955 : loss : 0.032516, loss_ce: 0.007674\n","iteration 12956 : loss : 0.035982, loss_ce: 0.008199\n","iteration 12957 : loss : 0.086107, loss_ce: 0.011513\n","iteration 12958 : loss : 0.031341, loss_ce: 0.015152\n","iteration 12959 : loss : 0.041773, loss_ce: 0.015549\n","iteration 12960 : loss : 0.049296, loss_ce: 0.019165\n","iteration 12961 : loss : 0.042381, loss_ce: 0.011784\n","iteration 12962 : loss : 0.038069, loss_ce: 0.011289\n","iteration 12963 : loss : 0.050624, loss_ce: 0.011458\n","iteration 12964 : loss : 0.046240, loss_ce: 0.015195\n","iteration 12965 : loss : 0.043528, loss_ce: 0.009615\n","iteration 12966 : loss : 0.030048, loss_ce: 0.005097\n","iteration 12967 : loss : 0.035435, loss_ce: 0.011380\n","iteration 12968 : loss : 0.104482, loss_ce: 0.020184\n","iteration 12969 : loss : 0.036053, loss_ce: 0.012063\n","iteration 12970 : loss : 0.037349, loss_ce: 0.014644\n","iteration 12971 : loss : 0.042295, loss_ce: 0.007315\n","iteration 12972 : loss : 0.036523, loss_ce: 0.012612\n","iteration 12973 : loss : 0.045791, loss_ce: 0.024170\n","iteration 12974 : loss : 0.033944, loss_ce: 0.013645\n","iteration 12975 : loss : 0.036468, loss_ce: 0.011054\n","iteration 12976 : loss : 0.033905, loss_ce: 0.011188\n","iteration 12977 : loss : 0.034407, loss_ce: 0.006898\n","iteration 12978 : loss : 0.034560, loss_ce: 0.009971\n","iteration 12979 : loss : 0.036934, loss_ce: 0.013008\n","iteration 12980 : loss : 0.031931, loss_ce: 0.008017\n","iteration 12981 : loss : 0.035630, loss_ce: 0.006705\n","iteration 12982 : loss : 0.038877, loss_ce: 0.014524\n","iteration 12983 : loss : 0.043021, loss_ce: 0.011070\n","iteration 12984 : loss : 0.028918, loss_ce: 0.008938\n","iteration 12985 : loss : 0.097814, loss_ce: 0.008746\n","iteration 12986 : loss : 0.037452, loss_ce: 0.012490\n","iteration 12987 : loss : 0.043929, loss_ce: 0.011330\n","iteration 12988 : loss : 0.044826, loss_ce: 0.014141\n","iteration 12989 : loss : 0.041320, loss_ce: 0.013031\n","iteration 12990 : loss : 0.040905, loss_ce: 0.015445\n","iteration 12991 : loss : 0.036830, loss_ce: 0.007375\n","iteration 12992 : loss : 0.037559, loss_ce: 0.012295\n","iteration 12993 : loss : 0.094452, loss_ce: 0.005207\n","iteration 12994 : loss : 0.033834, loss_ce: 0.011165\n","iteration 12995 : loss : 0.043514, loss_ce: 0.019706\n","iteration 12996 : loss : 0.035300, loss_ce: 0.009642\n","iteration 12997 : loss : 0.040265, loss_ce: 0.015217\n","iteration 12998 : loss : 0.040179, loss_ce: 0.013858\n","iteration 12999 : loss : 0.041170, loss_ce: 0.007214\n","iteration 13000 : loss : 0.042202, loss_ce: 0.011342\n","iteration 13001 : loss : 0.043483, loss_ce: 0.006696\n","iteration 13002 : loss : 0.037567, loss_ce: 0.012744\n","iteration 13003 : loss : 0.033158, loss_ce: 0.005373\n","iteration 13004 : loss : 0.036079, loss_ce: 0.015153\n","iteration 13005 : loss : 0.097729, loss_ce: 0.011106\n","iteration 13006 : loss : 0.036887, loss_ce: 0.016021\n","iteration 13007 : loss : 0.040638, loss_ce: 0.013413\n","iteration 13008 : loss : 0.036495, loss_ce: 0.011563\n","iteration 13009 : loss : 0.038250, loss_ce: 0.009100\n","iteration 13010 : loss : 0.035641, loss_ce: 0.009675\n","iteration 13011 : loss : 0.033496, loss_ce: 0.008941\n","iteration 13012 : loss : 0.041129, loss_ce: 0.012588\n","iteration 13013 : loss : 0.028951, loss_ce: 0.007878\n","iteration 13014 : loss : 0.042400, loss_ce: 0.011791\n","iteration 13015 : loss : 0.043374, loss_ce: 0.011892\n","iteration 13016 : loss : 0.041465, loss_ce: 0.013814\n","iteration 13017 : loss : 0.039049, loss_ce: 0.012089\n","iteration 13018 : loss : 0.043016, loss_ce: 0.018380\n","iteration 13019 : loss : 0.035529, loss_ce: 0.016624\n","iteration 13020 : loss : 0.154770, loss_ce: 0.001643\n"," 93%|███████████████████████████  | 140/150 [2:44:30<11:40, 70.02s/it]iteration 13021 : loss : 0.095639, loss_ce: 0.006327\n","iteration 13022 : loss : 0.041555, loss_ce: 0.018962\n","iteration 13023 : loss : 0.038570, loss_ce: 0.016866\n","iteration 13024 : loss : 0.032422, loss_ce: 0.012388\n","iteration 13025 : loss : 0.033498, loss_ce: 0.012896\n","iteration 13026 : loss : 0.037440, loss_ce: 0.014373\n","iteration 13027 : loss : 0.041592, loss_ce: 0.012640\n","iteration 13028 : loss : 0.037119, loss_ce: 0.012908\n","iteration 13029 : loss : 0.041455, loss_ce: 0.011568\n","iteration 13030 : loss : 0.034079, loss_ce: 0.013007\n","iteration 13031 : loss : 0.041156, loss_ce: 0.011924\n","iteration 13032 : loss : 0.032562, loss_ce: 0.010456\n","iteration 13033 : loss : 0.040732, loss_ce: 0.017478\n","iteration 13034 : loss : 0.027322, loss_ce: 0.007360\n","iteration 13035 : loss : 0.040665, loss_ce: 0.014353\n","iteration 13036 : loss : 0.042389, loss_ce: 0.011545\n","iteration 13037 : loss : 0.036554, loss_ce: 0.014075\n","iteration 13038 : loss : 0.032008, loss_ce: 0.010103\n","iteration 13039 : loss : 0.036590, loss_ce: 0.010527\n","iteration 13040 : loss : 0.031813, loss_ce: 0.009853\n","iteration 13041 : loss : 0.034731, loss_ce: 0.009363\n","iteration 13042 : loss : 0.096156, loss_ce: 0.009662\n","iteration 13043 : loss : 0.029973, loss_ce: 0.014342\n","iteration 13044 : loss : 0.039991, loss_ce: 0.014511\n","iteration 13045 : loss : 0.036778, loss_ce: 0.012138\n","iteration 13046 : loss : 0.038379, loss_ce: 0.012848\n","iteration 13047 : loss : 0.038149, loss_ce: 0.011311\n","iteration 13048 : loss : 0.036468, loss_ce: 0.006356\n","iteration 13049 : loss : 0.036996, loss_ce: 0.013666\n","iteration 13050 : loss : 0.039549, loss_ce: 0.010203\n","iteration 13051 : loss : 0.040309, loss_ce: 0.013708\n","iteration 13052 : loss : 0.035403, loss_ce: 0.011106\n","iteration 13053 : loss : 0.035229, loss_ce: 0.009176\n","iteration 13054 : loss : 0.042517, loss_ce: 0.015871\n","iteration 13055 : loss : 0.093129, loss_ce: 0.013575\n","iteration 13056 : loss : 0.038301, loss_ce: 0.010145\n","iteration 13057 : loss : 0.036769, loss_ce: 0.015611\n","iteration 13058 : loss : 0.032730, loss_ce: 0.013058\n","iteration 13059 : loss : 0.034311, loss_ce: 0.009824\n","iteration 13060 : loss : 0.037990, loss_ce: 0.016650\n","iteration 13061 : loss : 0.041973, loss_ce: 0.009358\n","iteration 13062 : loss : 0.047996, loss_ce: 0.014059\n","iteration 13063 : loss : 0.041303, loss_ce: 0.011623\n","iteration 13064 : loss : 0.047575, loss_ce: 0.011677\n","iteration 13065 : loss : 0.037739, loss_ce: 0.008180\n","iteration 13066 : loss : 0.059432, loss_ce: 0.011676\n","iteration 13067 : loss : 0.031449, loss_ce: 0.007565\n","iteration 13068 : loss : 0.035300, loss_ce: 0.010482\n","iteration 13069 : loss : 0.044988, loss_ce: 0.011705\n","iteration 13070 : loss : 0.079336, loss_ce: 0.011274\n","iteration 13071 : loss : 0.033298, loss_ce: 0.009606\n","iteration 13072 : loss : 0.041677, loss_ce: 0.011971\n","iteration 13073 : loss : 0.041207, loss_ce: 0.019098\n","iteration 13074 : loss : 0.040768, loss_ce: 0.012587\n","iteration 13075 : loss : 0.028573, loss_ce: 0.010048\n","iteration 13076 : loss : 0.039509, loss_ce: 0.011310\n","iteration 13077 : loss : 0.037446, loss_ce: 0.015573\n","iteration 13078 : loss : 0.035279, loss_ce: 0.015097\n","iteration 13079 : loss : 0.036577, loss_ce: 0.013657\n","iteration 13080 : loss : 0.101402, loss_ce: 0.011859\n","iteration 13081 : loss : 0.099318, loss_ce: 0.008915\n","iteration 13082 : loss : 0.095492, loss_ce: 0.008082\n","iteration 13083 : loss : 0.032039, loss_ce: 0.007645\n","iteration 13084 : loss : 0.047184, loss_ce: 0.013793\n","iteration 13085 : loss : 0.045267, loss_ce: 0.007652\n","iteration 13086 : loss : 0.100600, loss_ce: 0.009624\n","iteration 13087 : loss : 0.076042, loss_ce: 0.009264\n","iteration 13088 : loss : 0.030314, loss_ce: 0.010268\n","iteration 13089 : loss : 0.033905, loss_ce: 0.015697\n","iteration 13090 : loss : 0.100826, loss_ce: 0.010476\n","iteration 13091 : loss : 0.038599, loss_ce: 0.012582\n","iteration 13092 : loss : 0.039139, loss_ce: 0.015104\n","iteration 13093 : loss : 0.105690, loss_ce: 0.007491\n","iteration 13094 : loss : 0.042072, loss_ce: 0.014210\n","iteration 13095 : loss : 0.033868, loss_ce: 0.013387\n","iteration 13096 : loss : 0.042162, loss_ce: 0.011448\n","iteration 13097 : loss : 0.034313, loss_ce: 0.011397\n","iteration 13098 : loss : 0.034977, loss_ce: 0.012512\n","iteration 13099 : loss : 0.032871, loss_ce: 0.009629\n","iteration 13100 : loss : 0.039448, loss_ce: 0.014669\n","iteration 13101 : loss : 0.097037, loss_ce: 0.004653\n","iteration 13102 : loss : 0.036229, loss_ce: 0.007427\n","iteration 13103 : loss : 0.036998, loss_ce: 0.009614\n","iteration 13104 : loss : 0.044969, loss_ce: 0.014157\n","iteration 13105 : loss : 0.038520, loss_ce: 0.010787\n","iteration 13106 : loss : 0.046454, loss_ce: 0.015167\n","iteration 13107 : loss : 0.092854, loss_ce: 0.009194\n","iteration 13108 : loss : 0.036293, loss_ce: 0.009115\n","iteration 13109 : loss : 0.040376, loss_ce: 0.007131\n","iteration 13110 : loss : 0.040875, loss_ce: 0.014997\n","iteration 13111 : loss : 0.033576, loss_ce: 0.008754\n","iteration 13112 : loss : 0.073007, loss_ce: 0.006911\n","iteration 13113 : loss : 0.095934, loss_ce: 0.012465\n"," 94%|███████████████████████████▎ | 141/150 [2:45:40<10:30, 70.01s/it]iteration 13114 : loss : 0.039633, loss_ce: 0.013227\n","iteration 13115 : loss : 0.029392, loss_ce: 0.014470\n","iteration 13116 : loss : 0.048375, loss_ce: 0.014400\n","iteration 13117 : loss : 0.032821, loss_ce: 0.013779\n","iteration 13118 : loss : 0.063011, loss_ce: 0.008256\n","iteration 13119 : loss : 0.039095, loss_ce: 0.009724\n","iteration 13120 : loss : 0.054598, loss_ce: 0.006069\n","iteration 13121 : loss : 0.036502, loss_ce: 0.018915\n","iteration 13122 : loss : 0.039028, loss_ce: 0.013590\n","iteration 13123 : loss : 0.033604, loss_ce: 0.011587\n","iteration 13124 : loss : 0.038941, loss_ce: 0.011815\n","iteration 13125 : loss : 0.039871, loss_ce: 0.010674\n","iteration 13126 : loss : 0.037787, loss_ce: 0.012179\n","iteration 13127 : loss : 0.033034, loss_ce: 0.007583\n","iteration 13128 : loss : 0.033416, loss_ce: 0.008563\n","iteration 13129 : loss : 0.041651, loss_ce: 0.012972\n","iteration 13130 : loss : 0.034711, loss_ce: 0.011517\n","iteration 13131 : loss : 0.040081, loss_ce: 0.013463\n","iteration 13132 : loss : 0.040898, loss_ce: 0.013936\n","iteration 13133 : loss : 0.044108, loss_ce: 0.009953\n","iteration 13134 : loss : 0.038339, loss_ce: 0.012559\n","iteration 13135 : loss : 0.044057, loss_ce: 0.013444\n","iteration 13136 : loss : 0.040582, loss_ce: 0.012648\n","iteration 13137 : loss : 0.039537, loss_ce: 0.013515\n","iteration 13138 : loss : 0.038968, loss_ce: 0.014431\n","iteration 13139 : loss : 0.096667, loss_ce: 0.005092\n","iteration 13140 : loss : 0.037253, loss_ce: 0.011996\n","iteration 13141 : loss : 0.043219, loss_ce: 0.014842\n","iteration 13142 : loss : 0.038332, loss_ce: 0.010667\n","iteration 13143 : loss : 0.121240, loss_ce: 0.010848\n","iteration 13144 : loss : 0.095155, loss_ce: 0.009616\n","iteration 13145 : loss : 0.038159, loss_ce: 0.010938\n","iteration 13146 : loss : 0.032815, loss_ce: 0.008139\n","iteration 13147 : loss : 0.036373, loss_ce: 0.011221\n","iteration 13148 : loss : 0.045613, loss_ce: 0.012439\n","iteration 13149 : loss : 0.037924, loss_ce: 0.013103\n","iteration 13150 : loss : 0.037479, loss_ce: 0.013171\n","iteration 13151 : loss : 0.043472, loss_ce: 0.016365\n","iteration 13152 : loss : 0.030389, loss_ce: 0.010335\n","iteration 13153 : loss : 0.033662, loss_ce: 0.008080\n","iteration 13154 : loss : 0.040230, loss_ce: 0.012448\n","iteration 13155 : loss : 0.032227, loss_ce: 0.013815\n","iteration 13156 : loss : 0.035233, loss_ce: 0.014235\n","iteration 13157 : loss : 0.034088, loss_ce: 0.010346\n","iteration 13158 : loss : 0.033779, loss_ce: 0.006471\n","iteration 13159 : loss : 0.038083, loss_ce: 0.010066\n","iteration 13160 : loss : 0.037454, loss_ce: 0.010059\n","iteration 13161 : loss : 0.047645, loss_ce: 0.011447\n","iteration 13162 : loss : 0.049392, loss_ce: 0.009596\n","iteration 13163 : loss : 0.094552, loss_ce: 0.007972\n","iteration 13164 : loss : 0.035144, loss_ce: 0.012618\n","iteration 13165 : loss : 0.034631, loss_ce: 0.011983\n","iteration 13166 : loss : 0.037100, loss_ce: 0.013502\n","iteration 13167 : loss : 0.038082, loss_ce: 0.013818\n","iteration 13168 : loss : 0.036767, loss_ce: 0.009124\n","iteration 13169 : loss : 0.046532, loss_ce: 0.014775\n","iteration 13170 : loss : 0.038981, loss_ce: 0.016920\n","iteration 13171 : loss : 0.036121, loss_ce: 0.012478\n","iteration 13172 : loss : 0.037509, loss_ce: 0.011119\n","iteration 13173 : loss : 0.043138, loss_ce: 0.017136\n","iteration 13174 : loss : 0.046633, loss_ce: 0.015252\n","iteration 13175 : loss : 0.104307, loss_ce: 0.008218\n","iteration 13176 : loss : 0.041996, loss_ce: 0.011379\n","iteration 13177 : loss : 0.039125, loss_ce: 0.009105\n","iteration 13178 : loss : 0.095391, loss_ce: 0.009987\n","iteration 13179 : loss : 0.046010, loss_ce: 0.014629\n","iteration 13180 : loss : 0.037985, loss_ce: 0.009275\n","iteration 13181 : loss : 0.031803, loss_ce: 0.007381\n","iteration 13182 : loss : 0.035432, loss_ce: 0.012058\n","iteration 13183 : loss : 0.039184, loss_ce: 0.011299\n","iteration 13184 : loss : 0.047020, loss_ce: 0.015035\n","iteration 13185 : loss : 0.098923, loss_ce: 0.014871\n","iteration 13186 : loss : 0.096073, loss_ce: 0.009116\n","iteration 13187 : loss : 0.035282, loss_ce: 0.008565\n","iteration 13188 : loss : 0.037304, loss_ce: 0.015793\n","iteration 13189 : loss : 0.032018, loss_ce: 0.013939\n","iteration 13190 : loss : 0.048279, loss_ce: 0.011891\n","iteration 13191 : loss : 0.037915, loss_ce: 0.009264\n","iteration 13192 : loss : 0.042721, loss_ce: 0.011245\n","iteration 13193 : loss : 0.034223, loss_ce: 0.009291\n","iteration 13194 : loss : 0.031922, loss_ce: 0.011721\n","iteration 13195 : loss : 0.033203, loss_ce: 0.007825\n","iteration 13196 : loss : 0.045267, loss_ce: 0.013164\n","iteration 13197 : loss : 0.036488, loss_ce: 0.013759\n","iteration 13198 : loss : 0.042552, loss_ce: 0.013401\n","iteration 13199 : loss : 0.036339, loss_ce: 0.008639\n","iteration 13200 : loss : 0.042207, loss_ce: 0.011204\n","iteration 13201 : loss : 0.035809, loss_ce: 0.011713\n","iteration 13202 : loss : 0.035001, loss_ce: 0.012949\n","iteration 13203 : loss : 0.040045, loss_ce: 0.012571\n","iteration 13204 : loss : 0.035912, loss_ce: 0.013282\n","iteration 13205 : loss : 0.035911, loss_ce: 0.014253\n","iteration 13206 : loss : 0.039003, loss_ce: 0.018208\n"," 95%|███████████████████████████▍ | 142/150 [2:46:51<09:20, 70.07s/it]iteration 13207 : loss : 0.037520, loss_ce: 0.006696\n","iteration 13208 : loss : 0.038947, loss_ce: 0.012413\n","iteration 13209 : loss : 0.044163, loss_ce: 0.011815\n","iteration 13210 : loss : 0.038517, loss_ce: 0.014033\n","iteration 13211 : loss : 0.043978, loss_ce: 0.009817\n","iteration 13212 : loss : 0.033799, loss_ce: 0.008479\n","iteration 13213 : loss : 0.038258, loss_ce: 0.011436\n","iteration 13214 : loss : 0.039845, loss_ce: 0.013224\n","iteration 13215 : loss : 0.052811, loss_ce: 0.015324\n","iteration 13216 : loss : 0.037468, loss_ce: 0.007123\n","iteration 13217 : loss : 0.049910, loss_ce: 0.012316\n","iteration 13218 : loss : 0.036991, loss_ce: 0.011647\n","iteration 13219 : loss : 0.038950, loss_ce: 0.014260\n","iteration 13220 : loss : 0.036045, loss_ce: 0.012530\n","iteration 13221 : loss : 0.040162, loss_ce: 0.008856\n","iteration 13222 : loss : 0.034848, loss_ce: 0.010366\n","iteration 13223 : loss : 0.037415, loss_ce: 0.010998\n","iteration 13224 : loss : 0.041494, loss_ce: 0.010963\n","iteration 13225 : loss : 0.037308, loss_ce: 0.008315\n","iteration 13226 : loss : 0.050549, loss_ce: 0.009181\n","iteration 13227 : loss : 0.033860, loss_ce: 0.010939\n","iteration 13228 : loss : 0.037292, loss_ce: 0.017331\n","iteration 13229 : loss : 0.040671, loss_ce: 0.017505\n","iteration 13230 : loss : 0.034190, loss_ce: 0.016006\n","iteration 13231 : loss : 0.040109, loss_ce: 0.010271\n","iteration 13232 : loss : 0.038138, loss_ce: 0.011472\n","iteration 13233 : loss : 0.035092, loss_ce: 0.010761\n","iteration 13234 : loss : 0.040846, loss_ce: 0.009783\n","iteration 13235 : loss : 0.041717, loss_ce: 0.013665\n","iteration 13236 : loss : 0.045251, loss_ce: 0.016975\n","iteration 13237 : loss : 0.030453, loss_ce: 0.011481\n","iteration 13238 : loss : 0.037178, loss_ce: 0.009976\n","iteration 13239 : loss : 0.040162, loss_ce: 0.014437\n","iteration 13240 : loss : 0.041915, loss_ce: 0.012622\n","iteration 13241 : loss : 0.036516, loss_ce: 0.008203\n","iteration 13242 : loss : 0.091900, loss_ce: 0.004918\n","iteration 13243 : loss : 0.043931, loss_ce: 0.013933\n","iteration 13244 : loss : 0.033717, loss_ce: 0.012386\n","iteration 13245 : loss : 0.093387, loss_ce: 0.010831\n","iteration 13246 : loss : 0.039416, loss_ce: 0.014324\n","iteration 13247 : loss : 0.036420, loss_ce: 0.008924\n","iteration 13248 : loss : 0.037112, loss_ce: 0.011690\n","iteration 13249 : loss : 0.100757, loss_ce: 0.008140\n","iteration 13250 : loss : 0.035728, loss_ce: 0.011955\n","iteration 13251 : loss : 0.031696, loss_ce: 0.010175\n","iteration 13252 : loss : 0.037913, loss_ce: 0.009322\n","iteration 13253 : loss : 0.041606, loss_ce: 0.008675\n","iteration 13254 : loss : 0.038482, loss_ce: 0.009963\n","iteration 13255 : loss : 0.035989, loss_ce: 0.009267\n","iteration 13256 : loss : 0.035851, loss_ce: 0.009650\n","iteration 13257 : loss : 0.036716, loss_ce: 0.011613\n","iteration 13258 : loss : 0.033353, loss_ce: 0.008302\n","iteration 13259 : loss : 0.041004, loss_ce: 0.012812\n","iteration 13260 : loss : 0.036932, loss_ce: 0.016284\n","iteration 13261 : loss : 0.031941, loss_ce: 0.008080\n","iteration 13262 : loss : 0.037868, loss_ce: 0.011859\n","iteration 13263 : loss : 0.032785, loss_ce: 0.011435\n","iteration 13264 : loss : 0.036097, loss_ce: 0.012658\n","iteration 13265 : loss : 0.032808, loss_ce: 0.010622\n","iteration 13266 : loss : 0.029166, loss_ce: 0.008798\n","iteration 13267 : loss : 0.057005, loss_ce: 0.013158\n","iteration 13268 : loss : 0.037459, loss_ce: 0.011800\n","iteration 13269 : loss : 0.034642, loss_ce: 0.013899\n","iteration 13270 : loss : 0.027679, loss_ce: 0.009866\n","iteration 13271 : loss : 0.048276, loss_ce: 0.009694\n","iteration 13272 : loss : 0.100821, loss_ce: 0.011193\n","iteration 13273 : loss : 0.033221, loss_ce: 0.011690\n","iteration 13274 : loss : 0.037079, loss_ce: 0.012005\n","iteration 13275 : loss : 0.039984, loss_ce: 0.008219\n","iteration 13276 : loss : 0.044268, loss_ce: 0.018827\n","iteration 13277 : loss : 0.040318, loss_ce: 0.011669\n","iteration 13278 : loss : 0.045186, loss_ce: 0.011428\n","iteration 13279 : loss : 0.087841, loss_ce: 0.011365\n","iteration 13280 : loss : 0.037942, loss_ce: 0.010155\n","iteration 13281 : loss : 0.037002, loss_ce: 0.015874\n","iteration 13282 : loss : 0.047635, loss_ce: 0.012812\n","iteration 13283 : loss : 0.033517, loss_ce: 0.011346\n","iteration 13284 : loss : 0.037416, loss_ce: 0.012113\n","iteration 13285 : loss : 0.037257, loss_ce: 0.016818\n","iteration 13286 : loss : 0.038998, loss_ce: 0.015535\n","iteration 13287 : loss : 0.036355, loss_ce: 0.017112\n","iteration 13288 : loss : 0.042524, loss_ce: 0.014681\n","iteration 13289 : loss : 0.094887, loss_ce: 0.005566\n","iteration 13290 : loss : 0.046484, loss_ce: 0.012128\n","iteration 13291 : loss : 0.041717, loss_ce: 0.005825\n","iteration 13292 : loss : 0.070114, loss_ce: 0.012036\n","iteration 13293 : loss : 0.043412, loss_ce: 0.012049\n","iteration 13294 : loss : 0.035868, loss_ce: 0.011673\n","iteration 13295 : loss : 0.040432, loss_ce: 0.014534\n","iteration 13296 : loss : 0.029377, loss_ce: 0.007457\n","iteration 13297 : loss : 0.039687, loss_ce: 0.012243\n","iteration 13298 : loss : 0.028096, loss_ce: 0.011191\n","iteration 13299 : loss : 0.239009, loss_ce: 0.007517\n"," 95%|███████████████████████████▋ | 143/150 [2:48:00<08:09, 69.92s/it]iteration 13300 : loss : 0.029055, loss_ce: 0.011642\n","iteration 13301 : loss : 0.035119, loss_ce: 0.007766\n","iteration 13302 : loss : 0.037070, loss_ce: 0.012332\n","iteration 13303 : loss : 0.035282, loss_ce: 0.011928\n","iteration 13304 : loss : 0.034146, loss_ce: 0.006239\n","iteration 13305 : loss : 0.033460, loss_ce: 0.010950\n","iteration 13306 : loss : 0.038302, loss_ce: 0.011751\n","iteration 13307 : loss : 0.031690, loss_ce: 0.008625\n","iteration 13308 : loss : 0.046804, loss_ce: 0.014386\n","iteration 13309 : loss : 0.038729, loss_ce: 0.008749\n","iteration 13310 : loss : 0.035969, loss_ce: 0.008984\n","iteration 13311 : loss : 0.037863, loss_ce: 0.015561\n","iteration 13312 : loss : 0.042530, loss_ce: 0.012067\n","iteration 13313 : loss : 0.030473, loss_ce: 0.012173\n","iteration 13314 : loss : 0.035985, loss_ce: 0.009227\n","iteration 13315 : loss : 0.034849, loss_ce: 0.012269\n","iteration 13316 : loss : 0.034234, loss_ce: 0.010529\n","iteration 13317 : loss : 0.098957, loss_ce: 0.010096\n","iteration 13318 : loss : 0.031116, loss_ce: 0.010370\n","iteration 13319 : loss : 0.032638, loss_ce: 0.007894\n","iteration 13320 : loss : 0.026687, loss_ce: 0.006922\n","iteration 13321 : loss : 0.037606, loss_ce: 0.015000\n","iteration 13322 : loss : 0.036521, loss_ce: 0.015781\n","iteration 13323 : loss : 0.050242, loss_ce: 0.011074\n","iteration 13324 : loss : 0.031154, loss_ce: 0.008765\n","iteration 13325 : loss : 0.037923, loss_ce: 0.010820\n","iteration 13326 : loss : 0.038199, loss_ce: 0.009487\n","iteration 13327 : loss : 0.040988, loss_ce: 0.015155\n","iteration 13328 : loss : 0.041650, loss_ce: 0.010266\n","iteration 13329 : loss : 0.033617, loss_ce: 0.014376\n","iteration 13330 : loss : 0.093101, loss_ce: 0.006527\n","iteration 13331 : loss : 0.037922, loss_ce: 0.013225\n","iteration 13332 : loss : 0.037654, loss_ce: 0.012151\n","iteration 13333 : loss : 0.036311, loss_ce: 0.016938\n","iteration 13334 : loss : 0.101136, loss_ce: 0.006389\n","iteration 13335 : loss : 0.041490, loss_ce: 0.012048\n","iteration 13336 : loss : 0.035076, loss_ce: 0.012061\n","iteration 13337 : loss : 0.040678, loss_ce: 0.013291\n","iteration 13338 : loss : 0.041519, loss_ce: 0.020121\n","iteration 13339 : loss : 0.034649, loss_ce: 0.012696\n","iteration 13340 : loss : 0.039270, loss_ce: 0.018787\n","iteration 13341 : loss : 0.101093, loss_ce: 0.005205\n","iteration 13342 : loss : 0.039602, loss_ce: 0.014142\n","iteration 13343 : loss : 0.039447, loss_ce: 0.008599\n","iteration 13344 : loss : 0.043912, loss_ce: 0.014566\n","iteration 13345 : loss : 0.036146, loss_ce: 0.010475\n","iteration 13346 : loss : 0.038844, loss_ce: 0.013044\n","iteration 13347 : loss : 0.036310, loss_ce: 0.009223\n","iteration 13348 : loss : 0.036455, loss_ce: 0.014233\n","iteration 13349 : loss : 0.036135, loss_ce: 0.012494\n","iteration 13350 : loss : 0.036221, loss_ce: 0.012015\n","iteration 13351 : loss : 0.040943, loss_ce: 0.011633\n","iteration 13352 : loss : 0.035309, loss_ce: 0.009173\n","iteration 13353 : loss : 0.048623, loss_ce: 0.016581\n","iteration 13354 : loss : 0.095335, loss_ce: 0.013238\n","iteration 13355 : loss : 0.039672, loss_ce: 0.022838\n","iteration 13356 : loss : 0.043665, loss_ce: 0.010760\n","iteration 13357 : loss : 0.039096, loss_ce: 0.009897\n","iteration 13358 : loss : 0.038286, loss_ce: 0.009599\n","iteration 13359 : loss : 0.040045, loss_ce: 0.009551\n","iteration 13360 : loss : 0.038552, loss_ce: 0.009282\n","iteration 13361 : loss : 0.039983, loss_ce: 0.011779\n","iteration 13362 : loss : 0.040048, loss_ce: 0.010226\n","iteration 13363 : loss : 0.033797, loss_ce: 0.011602\n","iteration 13364 : loss : 0.037258, loss_ce: 0.013081\n","iteration 13365 : loss : 0.046732, loss_ce: 0.010981\n","iteration 13366 : loss : 0.036927, loss_ce: 0.018425\n","iteration 13367 : loss : 0.096962, loss_ce: 0.006071\n","iteration 13368 : loss : 0.036612, loss_ce: 0.011572\n","iteration 13369 : loss : 0.040280, loss_ce: 0.011755\n","iteration 13370 : loss : 0.097303, loss_ce: 0.005731\n","iteration 13371 : loss : 0.031499, loss_ce: 0.013369\n","iteration 13372 : loss : 0.030548, loss_ce: 0.007805\n","iteration 13373 : loss : 0.039476, loss_ce: 0.011834\n","iteration 13374 : loss : 0.033924, loss_ce: 0.013765\n","iteration 13375 : loss : 0.034332, loss_ce: 0.010466\n","iteration 13376 : loss : 0.046201, loss_ce: 0.005808\n","iteration 13377 : loss : 0.038849, loss_ce: 0.010559\n","iteration 13378 : loss : 0.036999, loss_ce: 0.008347\n","iteration 13379 : loss : 0.044334, loss_ce: 0.017911\n","iteration 13380 : loss : 0.033272, loss_ce: 0.007704\n","iteration 13381 : loss : 0.041329, loss_ce: 0.010596\n","iteration 13382 : loss : 0.036237, loss_ce: 0.014352\n","iteration 13383 : loss : 0.035821, loss_ce: 0.015594\n","iteration 13384 : loss : 0.040989, loss_ce: 0.009843\n","iteration 13385 : loss : 0.043384, loss_ce: 0.011354\n","iteration 13386 : loss : 0.035776, loss_ce: 0.008561\n","iteration 13387 : loss : 0.033682, loss_ce: 0.014695\n","iteration 13388 : loss : 0.038451, loss_ce: 0.011223\n","iteration 13389 : loss : 0.035281, loss_ce: 0.013937\n","iteration 13390 : loss : 0.029722, loss_ce: 0.005146\n","iteration 13391 : loss : 0.037620, loss_ce: 0.010827\n","iteration 13392 : loss : 0.105096, loss_ce: 0.020656\n"," 96%|███████████████████████████▊ | 144/150 [2:49:10<06:59, 70.00s/it]iteration 13393 : loss : 0.038402, loss_ce: 0.013214\n","iteration 13394 : loss : 0.041337, loss_ce: 0.012566\n","iteration 13395 : loss : 0.045520, loss_ce: 0.013781\n","iteration 13396 : loss : 0.033275, loss_ce: 0.007247\n","iteration 13397 : loss : 0.041884, loss_ce: 0.015288\n","iteration 13398 : loss : 0.036339, loss_ce: 0.011786\n","iteration 13399 : loss : 0.093084, loss_ce: 0.012062\n","iteration 13400 : loss : 0.039148, loss_ce: 0.013120\n","iteration 13401 : loss : 0.033562, loss_ce: 0.008129\n","iteration 13402 : loss : 0.042808, loss_ce: 0.007233\n","iteration 13403 : loss : 0.037755, loss_ce: 0.009374\n","iteration 13404 : loss : 0.034018, loss_ce: 0.012935\n","iteration 13405 : loss : 0.036640, loss_ce: 0.012264\n","iteration 13406 : loss : 0.038077, loss_ce: 0.016870\n","iteration 13407 : loss : 0.034137, loss_ce: 0.011801\n","iteration 13408 : loss : 0.041385, loss_ce: 0.013163\n","iteration 13409 : loss : 0.037814, loss_ce: 0.015190\n","iteration 13410 : loss : 0.050088, loss_ce: 0.006153\n","iteration 13411 : loss : 0.043322, loss_ce: 0.017910\n","iteration 13412 : loss : 0.039217, loss_ce: 0.014194\n","iteration 13413 : loss : 0.034594, loss_ce: 0.013617\n","iteration 13414 : loss : 0.047973, loss_ce: 0.012383\n","iteration 13415 : loss : 0.042816, loss_ce: 0.018732\n","iteration 13416 : loss : 0.037266, loss_ce: 0.012532\n","iteration 13417 : loss : 0.039003, loss_ce: 0.016220\n","iteration 13418 : loss : 0.097736, loss_ce: 0.008016\n","iteration 13419 : loss : 0.046271, loss_ce: 0.006380\n","iteration 13420 : loss : 0.035555, loss_ce: 0.011518\n","iteration 13421 : loss : 0.031830, loss_ce: 0.009056\n","iteration 13422 : loss : 0.039498, loss_ce: 0.010668\n","iteration 13423 : loss : 0.032987, loss_ce: 0.011129\n","iteration 13424 : loss : 0.040283, loss_ce: 0.013096\n","iteration 13425 : loss : 0.040336, loss_ce: 0.013928\n","iteration 13426 : loss : 0.041009, loss_ce: 0.013546\n","iteration 13427 : loss : 0.035615, loss_ce: 0.015449\n","iteration 13428 : loss : 0.027736, loss_ce: 0.006870\n","iteration 13429 : loss : 0.038376, loss_ce: 0.009507\n","iteration 13430 : loss : 0.042090, loss_ce: 0.012754\n","iteration 13431 : loss : 0.046796, loss_ce: 0.016057\n","iteration 13432 : loss : 0.096423, loss_ce: 0.009280\n","iteration 13433 : loss : 0.035585, loss_ce: 0.015377\n","iteration 13434 : loss : 0.038561, loss_ce: 0.012549\n","iteration 13435 : loss : 0.038544, loss_ce: 0.009626\n","iteration 13436 : loss : 0.098451, loss_ce: 0.011142\n","iteration 13437 : loss : 0.045104, loss_ce: 0.012865\n","iteration 13438 : loss : 0.040844, loss_ce: 0.010274\n","iteration 13439 : loss : 0.034729, loss_ce: 0.007290\n","iteration 13440 : loss : 0.038955, loss_ce: 0.009829\n","iteration 13441 : loss : 0.100864, loss_ce: 0.009290\n","iteration 13442 : loss : 0.040190, loss_ce: 0.015901\n","iteration 13443 : loss : 0.035607, loss_ce: 0.011096\n","iteration 13444 : loss : 0.039232, loss_ce: 0.011310\n","iteration 13445 : loss : 0.033311, loss_ce: 0.012251\n","iteration 13446 : loss : 0.037165, loss_ce: 0.009572\n","iteration 13447 : loss : 0.041695, loss_ce: 0.012619\n","iteration 13448 : loss : 0.038187, loss_ce: 0.013104\n","iteration 13449 : loss : 0.031338, loss_ce: 0.008703\n","iteration 13450 : loss : 0.039576, loss_ce: 0.009711\n","iteration 13451 : loss : 0.070719, loss_ce: 0.015205\n","iteration 13452 : loss : 0.040135, loss_ce: 0.013309\n","iteration 13453 : loss : 0.036481, loss_ce: 0.011363\n","iteration 13454 : loss : 0.045732, loss_ce: 0.009246\n","iteration 13455 : loss : 0.035248, loss_ce: 0.012923\n","iteration 13456 : loss : 0.040279, loss_ce: 0.008372\n","iteration 13457 : loss : 0.098135, loss_ce: 0.006605\n","iteration 13458 : loss : 0.038258, loss_ce: 0.013669\n","iteration 13459 : loss : 0.032462, loss_ce: 0.009243\n","iteration 13460 : loss : 0.035275, loss_ce: 0.006088\n","iteration 13461 : loss : 0.043934, loss_ce: 0.009275\n","iteration 13462 : loss : 0.035466, loss_ce: 0.008545\n","iteration 13463 : loss : 0.038980, loss_ce: 0.007190\n","iteration 13464 : loss : 0.036208, loss_ce: 0.010223\n","iteration 13465 : loss : 0.040250, loss_ce: 0.013854\n","iteration 13466 : loss : 0.034048, loss_ce: 0.009254\n","iteration 13467 : loss : 0.035347, loss_ce: 0.006139\n","iteration 13468 : loss : 0.031036, loss_ce: 0.009997\n","iteration 13469 : loss : 0.030585, loss_ce: 0.012538\n","iteration 13470 : loss : 0.037292, loss_ce: 0.009682\n","iteration 13471 : loss : 0.035106, loss_ce: 0.009315\n","iteration 13472 : loss : 0.101563, loss_ce: 0.005210\n","iteration 13473 : loss : 0.037312, loss_ce: 0.010654\n","iteration 13474 : loss : 0.037152, loss_ce: 0.008808\n","iteration 13475 : loss : 0.035516, loss_ce: 0.013958\n","iteration 13476 : loss : 0.040891, loss_ce: 0.012203\n","iteration 13477 : loss : 0.031111, loss_ce: 0.009778\n","iteration 13478 : loss : 0.034816, loss_ce: 0.014013\n","iteration 13479 : loss : 0.039634, loss_ce: 0.014649\n","iteration 13480 : loss : 0.041777, loss_ce: 0.016836\n","iteration 13481 : loss : 0.037061, loss_ce: 0.014015\n","iteration 13482 : loss : 0.036357, loss_ce: 0.014732\n","iteration 13483 : loss : 0.054655, loss_ce: 0.011466\n","iteration 13484 : loss : 0.030233, loss_ce: 0.010886\n","iteration 13485 : loss : 0.096160, loss_ce: 0.013650\n"," 97%|████████████████████████████ | 145/150 [2:50:20<05:50, 70.00s/it]iteration 13486 : loss : 0.042400, loss_ce: 0.016069\n","iteration 13487 : loss : 0.040113, loss_ce: 0.017429\n","iteration 13488 : loss : 0.042039, loss_ce: 0.014813\n","iteration 13489 : loss : 0.044410, loss_ce: 0.014098\n","iteration 13490 : loss : 0.036435, loss_ce: 0.012553\n","iteration 13491 : loss : 0.045124, loss_ce: 0.014359\n","iteration 13492 : loss : 0.050276, loss_ce: 0.010617\n","iteration 13493 : loss : 0.033897, loss_ce: 0.008849\n","iteration 13494 : loss : 0.043141, loss_ce: 0.011233\n","iteration 13495 : loss : 0.043989, loss_ce: 0.013534\n","iteration 13496 : loss : 0.037068, loss_ce: 0.011692\n","iteration 13497 : loss : 0.040771, loss_ce: 0.008256\n","iteration 13498 : loss : 0.038748, loss_ce: 0.011397\n","iteration 13499 : loss : 0.030560, loss_ce: 0.012705\n","iteration 13500 : loss : 0.035339, loss_ce: 0.007379\n","iteration 13501 : loss : 0.053103, loss_ce: 0.007091\n","iteration 13502 : loss : 0.035377, loss_ce: 0.011226\n","iteration 13503 : loss : 0.097053, loss_ce: 0.009444\n","iteration 13504 : loss : 0.036769, loss_ce: 0.012821\n","iteration 13505 : loss : 0.096520, loss_ce: 0.002883\n","iteration 13506 : loss : 0.059358, loss_ce: 0.008799\n","iteration 13507 : loss : 0.031210, loss_ce: 0.009621\n","iteration 13508 : loss : 0.037754, loss_ce: 0.015722\n","iteration 13509 : loss : 0.039204, loss_ce: 0.008153\n","iteration 13510 : loss : 0.029702, loss_ce: 0.006595\n","iteration 13511 : loss : 0.032201, loss_ce: 0.008111\n","iteration 13512 : loss : 0.039228, loss_ce: 0.010013\n","iteration 13513 : loss : 0.039474, loss_ce: 0.014726\n","iteration 13514 : loss : 0.044156, loss_ce: 0.008529\n","iteration 13515 : loss : 0.045217, loss_ce: 0.010841\n","iteration 13516 : loss : 0.059435, loss_ce: 0.009288\n","iteration 13517 : loss : 0.038405, loss_ce: 0.009898\n","iteration 13518 : loss : 0.037412, loss_ce: 0.011490\n","iteration 13519 : loss : 0.030455, loss_ce: 0.011686\n","iteration 13520 : loss : 0.040574, loss_ce: 0.012974\n","iteration 13521 : loss : 0.033597, loss_ce: 0.010201\n","iteration 13522 : loss : 0.039591, loss_ce: 0.010691\n","iteration 13523 : loss : 0.033042, loss_ce: 0.012629\n","iteration 13524 : loss : 0.030309, loss_ce: 0.008981\n","iteration 13525 : loss : 0.037910, loss_ce: 0.011828\n","iteration 13526 : loss : 0.030707, loss_ce: 0.007808\n","iteration 13527 : loss : 0.034401, loss_ce: 0.009622\n","iteration 13528 : loss : 0.032592, loss_ce: 0.011139\n","iteration 13529 : loss : 0.044836, loss_ce: 0.009540\n","iteration 13530 : loss : 0.043734, loss_ce: 0.013130\n","iteration 13531 : loss : 0.049119, loss_ce: 0.008900\n","iteration 13532 : loss : 0.032254, loss_ce: 0.008981\n","iteration 13533 : loss : 0.031865, loss_ce: 0.009122\n","iteration 13534 : loss : 0.038848, loss_ce: 0.012943\n","iteration 13535 : loss : 0.032534, loss_ce: 0.015430\n","iteration 13536 : loss : 0.038242, loss_ce: 0.013070\n","iteration 13537 : loss : 0.056600, loss_ce: 0.011053\n","iteration 13538 : loss : 0.034523, loss_ce: 0.016420\n","iteration 13539 : loss : 0.037047, loss_ce: 0.014679\n","iteration 13540 : loss : 0.034685, loss_ce: 0.010548\n","iteration 13541 : loss : 0.036771, loss_ce: 0.010268\n","iteration 13542 : loss : 0.038224, loss_ce: 0.011989\n","iteration 13543 : loss : 0.035143, loss_ce: 0.009627\n","iteration 13544 : loss : 0.049925, loss_ce: 0.010922\n","iteration 13545 : loss : 0.037975, loss_ce: 0.015985\n","iteration 13546 : loss : 0.037286, loss_ce: 0.013862\n","iteration 13547 : loss : 0.095111, loss_ce: 0.006669\n","iteration 13548 : loss : 0.040932, loss_ce: 0.010077\n","iteration 13549 : loss : 0.040012, loss_ce: 0.013947\n","iteration 13550 : loss : 0.104586, loss_ce: 0.008669\n","iteration 13551 : loss : 0.034426, loss_ce: 0.012374\n","iteration 13552 : loss : 0.099581, loss_ce: 0.016204\n","iteration 13553 : loss : 0.046692, loss_ce: 0.013865\n","iteration 13554 : loss : 0.030093, loss_ce: 0.014337\n","iteration 13555 : loss : 0.036254, loss_ce: 0.014412\n","iteration 13556 : loss : 0.046886, loss_ce: 0.009562\n","iteration 13557 : loss : 0.034466, loss_ce: 0.008345\n","iteration 13558 : loss : 0.037558, loss_ce: 0.016431\n","iteration 13559 : loss : 0.037448, loss_ce: 0.010740\n","iteration 13560 : loss : 0.043047, loss_ce: 0.008616\n","iteration 13561 : loss : 0.037398, loss_ce: 0.016392\n","iteration 13562 : loss : 0.030080, loss_ce: 0.007097\n","iteration 13563 : loss : 0.037810, loss_ce: 0.010767\n","iteration 13564 : loss : 0.055053, loss_ce: 0.009867\n","iteration 13565 : loss : 0.077688, loss_ce: 0.015176\n","iteration 13566 : loss : 0.033496, loss_ce: 0.008160\n","iteration 13567 : loss : 0.039118, loss_ce: 0.011691\n","iteration 13568 : loss : 0.043050, loss_ce: 0.013499\n","iteration 13569 : loss : 0.041271, loss_ce: 0.019640\n","iteration 13570 : loss : 0.039698, loss_ce: 0.013565\n","iteration 13571 : loss : 0.038178, loss_ce: 0.009640\n","iteration 13572 : loss : 0.039036, loss_ce: 0.013754\n","iteration 13573 : loss : 0.035493, loss_ce: 0.011277\n","iteration 13574 : loss : 0.035698, loss_ce: 0.010732\n","iteration 13575 : loss : 0.048808, loss_ce: 0.008599\n","iteration 13576 : loss : 0.035158, loss_ce: 0.010013\n","iteration 13577 : loss : 0.039111, loss_ce: 0.007057\n","iteration 13578 : loss : 0.346940, loss_ce: 0.023238\n"," 97%|████████████████████████████▏| 146/150 [2:51:30<04:39, 69.99s/it]iteration 13579 : loss : 0.039141, loss_ce: 0.018138\n","iteration 13580 : loss : 0.038639, loss_ce: 0.010190\n","iteration 13581 : loss : 0.032555, loss_ce: 0.008214\n","iteration 13582 : loss : 0.095922, loss_ce: 0.013018\n","iteration 13583 : loss : 0.034845, loss_ce: 0.008785\n","iteration 13584 : loss : 0.043916, loss_ce: 0.006851\n","iteration 13585 : loss : 0.104137, loss_ce: 0.009958\n","iteration 13586 : loss : 0.032470, loss_ce: 0.008742\n","iteration 13587 : loss : 0.030441, loss_ce: 0.009451\n","iteration 13588 : loss : 0.039752, loss_ce: 0.017646\n","iteration 13589 : loss : 0.037923, loss_ce: 0.014672\n","iteration 13590 : loss : 0.039077, loss_ce: 0.009802\n","iteration 13591 : loss : 0.033229, loss_ce: 0.008309\n","iteration 13592 : loss : 0.032982, loss_ce: 0.007065\n","iteration 13593 : loss : 0.044258, loss_ce: 0.014344\n","iteration 13594 : loss : 0.038317, loss_ce: 0.010296\n","iteration 13595 : loss : 0.034585, loss_ce: 0.007631\n","iteration 13596 : loss : 0.048361, loss_ce: 0.011164\n","iteration 13597 : loss : 0.043214, loss_ce: 0.012208\n","iteration 13598 : loss : 0.031875, loss_ce: 0.013095\n","iteration 13599 : loss : 0.035431, loss_ce: 0.008763\n","iteration 13600 : loss : 0.035117, loss_ce: 0.011623\n","iteration 13601 : loss : 0.038935, loss_ce: 0.011518\n","iteration 13602 : loss : 0.035564, loss_ce: 0.006425\n","iteration 13603 : loss : 0.039203, loss_ce: 0.009484\n","iteration 13604 : loss : 0.036699, loss_ce: 0.010997\n","iteration 13605 : loss : 0.063926, loss_ce: 0.010631\n","iteration 13606 : loss : 0.036686, loss_ce: 0.006753\n","iteration 13607 : loss : 0.038481, loss_ce: 0.011506\n","iteration 13608 : loss : 0.034851, loss_ce: 0.013356\n","iteration 13609 : loss : 0.037895, loss_ce: 0.010550\n","iteration 13610 : loss : 0.032473, loss_ce: 0.008484\n","iteration 13611 : loss : 0.043874, loss_ce: 0.009297\n","iteration 13612 : loss : 0.040469, loss_ce: 0.011695\n","iteration 13613 : loss : 0.042562, loss_ce: 0.015680\n","iteration 13614 : loss : 0.039962, loss_ce: 0.019026\n","iteration 13615 : loss : 0.034856, loss_ce: 0.013893\n","iteration 13616 : loss : 0.039349, loss_ce: 0.013066\n","iteration 13617 : loss : 0.045806, loss_ce: 0.010136\n","iteration 13618 : loss : 0.093031, loss_ce: 0.008515\n","iteration 13619 : loss : 0.094806, loss_ce: 0.006406\n","iteration 13620 : loss : 0.037524, loss_ce: 0.014315\n","iteration 13621 : loss : 0.035971, loss_ce: 0.012573\n","iteration 13622 : loss : 0.039932, loss_ce: 0.014830\n","iteration 13623 : loss : 0.036531, loss_ce: 0.010107\n","iteration 13624 : loss : 0.096877, loss_ce: 0.013048\n","iteration 13625 : loss : 0.031332, loss_ce: 0.007592\n","iteration 13626 : loss : 0.035386, loss_ce: 0.007658\n","iteration 13627 : loss : 0.092502, loss_ce: 0.007947\n","iteration 13628 : loss : 0.038101, loss_ce: 0.014344\n","iteration 13629 : loss : 0.094345, loss_ce: 0.008065\n","iteration 13630 : loss : 0.040871, loss_ce: 0.016188\n","iteration 13631 : loss : 0.037068, loss_ce: 0.012700\n","iteration 13632 : loss : 0.036342, loss_ce: 0.017158\n","iteration 13633 : loss : 0.044422, loss_ce: 0.009161\n","iteration 13634 : loss : 0.037966, loss_ce: 0.019002\n","iteration 13635 : loss : 0.033824, loss_ce: 0.016181\n","iteration 13636 : loss : 0.038749, loss_ce: 0.014350\n","iteration 13637 : loss : 0.036829, loss_ce: 0.016683\n","iteration 13638 : loss : 0.041271, loss_ce: 0.017007\n","iteration 13639 : loss : 0.032337, loss_ce: 0.012937\n","iteration 13640 : loss : 0.036300, loss_ce: 0.012188\n","iteration 13641 : loss : 0.040738, loss_ce: 0.014815\n","iteration 13642 : loss : 0.041689, loss_ce: 0.017951\n","iteration 13643 : loss : 0.035926, loss_ce: 0.009921\n","iteration 13644 : loss : 0.042636, loss_ce: 0.017606\n","iteration 13645 : loss : 0.098885, loss_ce: 0.008387\n","iteration 13646 : loss : 0.039221, loss_ce: 0.017019\n","iteration 13647 : loss : 0.038357, loss_ce: 0.013626\n","iteration 13648 : loss : 0.037231, loss_ce: 0.010046\n","iteration 13649 : loss : 0.035978, loss_ce: 0.012975\n","iteration 13650 : loss : 0.041217, loss_ce: 0.011851\n","iteration 13651 : loss : 0.037102, loss_ce: 0.010498\n","iteration 13652 : loss : 0.033828, loss_ce: 0.009549\n","iteration 13653 : loss : 0.060451, loss_ce: 0.009871\n","iteration 13654 : loss : 0.093568, loss_ce: 0.006206\n","iteration 13655 : loss : 0.100475, loss_ce: 0.008088\n","iteration 13656 : loss : 0.032372, loss_ce: 0.012276\n","iteration 13657 : loss : 0.034172, loss_ce: 0.009898\n","iteration 13658 : loss : 0.044681, loss_ce: 0.010909\n","iteration 13659 : loss : 0.037414, loss_ce: 0.011724\n","iteration 13660 : loss : 0.037656, loss_ce: 0.008665\n","iteration 13661 : loss : 0.029606, loss_ce: 0.008481\n","iteration 13662 : loss : 0.034538, loss_ce: 0.007902\n","iteration 13663 : loss : 0.034440, loss_ce: 0.009095\n","iteration 13664 : loss : 0.049282, loss_ce: 0.016005\n","iteration 13665 : loss : 0.045381, loss_ce: 0.009125\n","iteration 13666 : loss : 0.035300, loss_ce: 0.010712\n","iteration 13667 : loss : 0.039183, loss_ce: 0.012612\n","iteration 13668 : loss : 0.036880, loss_ce: 0.013209\n","iteration 13669 : loss : 0.040605, loss_ce: 0.011215\n","iteration 13670 : loss : 0.037282, loss_ce: 0.010810\n","iteration 13671 : loss : 0.230245, loss_ce: 0.003761\n"," 98%|████████████████████████████▍| 147/150 [2:52:41<03:30, 70.10s/it]iteration 13672 : loss : 0.035963, loss_ce: 0.005610\n","iteration 13673 : loss : 0.042357, loss_ce: 0.017058\n","iteration 13674 : loss : 0.045139, loss_ce: 0.009869\n","iteration 13675 : loss : 0.036531, loss_ce: 0.015320\n","iteration 13676 : loss : 0.037583, loss_ce: 0.014079\n","iteration 13677 : loss : 0.037449, loss_ce: 0.012707\n","iteration 13678 : loss : 0.037333, loss_ce: 0.006160\n","iteration 13679 : loss : 0.038285, loss_ce: 0.010309\n","iteration 13680 : loss : 0.044389, loss_ce: 0.012948\n","iteration 13681 : loss : 0.034210, loss_ce: 0.008829\n","iteration 13682 : loss : 0.032006, loss_ce: 0.015685\n","iteration 13683 : loss : 0.040435, loss_ce: 0.010375\n","iteration 13684 : loss : 0.049479, loss_ce: 0.013786\n","iteration 13685 : loss : 0.049639, loss_ce: 0.017012\n","iteration 13686 : loss : 0.034993, loss_ce: 0.011312\n","iteration 13687 : loss : 0.042648, loss_ce: 0.009360\n","iteration 13688 : loss : 0.043978, loss_ce: 0.009251\n","iteration 13689 : loss : 0.042120, loss_ce: 0.010565\n","iteration 13690 : loss : 0.033010, loss_ce: 0.008740\n","iteration 13691 : loss : 0.033009, loss_ce: 0.009945\n","iteration 13692 : loss : 0.101921, loss_ce: 0.015198\n","iteration 13693 : loss : 0.051776, loss_ce: 0.016197\n","iteration 13694 : loss : 0.033626, loss_ce: 0.012178\n","iteration 13695 : loss : 0.037908, loss_ce: 0.005750\n","iteration 13696 : loss : 0.043896, loss_ce: 0.013511\n","iteration 13697 : loss : 0.040854, loss_ce: 0.019551\n","iteration 13698 : loss : 0.035627, loss_ce: 0.014396\n","iteration 13699 : loss : 0.031688, loss_ce: 0.011087\n","iteration 13700 : loss : 0.032197, loss_ce: 0.015112\n","iteration 13701 : loss : 0.098765, loss_ce: 0.007464\n","iteration 13702 : loss : 0.073899, loss_ce: 0.003742\n","iteration 13703 : loss : 0.037477, loss_ce: 0.008025\n","iteration 13704 : loss : 0.036107, loss_ce: 0.012288\n","iteration 13705 : loss : 0.102420, loss_ce: 0.010074\n","iteration 13706 : loss : 0.041684, loss_ce: 0.012574\n","iteration 13707 : loss : 0.045163, loss_ce: 0.016132\n","iteration 13708 : loss : 0.105749, loss_ce: 0.007338\n","iteration 13709 : loss : 0.034534, loss_ce: 0.012154\n","iteration 13710 : loss : 0.050324, loss_ce: 0.012772\n","iteration 13711 : loss : 0.036359, loss_ce: 0.007822\n","iteration 13712 : loss : 0.035658, loss_ce: 0.011170\n","iteration 13713 : loss : 0.036230, loss_ce: 0.011758\n","iteration 13714 : loss : 0.034222, loss_ce: 0.011937\n","iteration 13715 : loss : 0.045216, loss_ce: 0.013207\n","iteration 13716 : loss : 0.037035, loss_ce: 0.013793\n","iteration 13717 : loss : 0.104927, loss_ce: 0.008292\n","iteration 13718 : loss : 0.096705, loss_ce: 0.008215\n","iteration 13719 : loss : 0.106249, loss_ce: 0.007872\n","iteration 13720 : loss : 0.038139, loss_ce: 0.016405\n","iteration 13721 : loss : 0.033104, loss_ce: 0.008521\n","iteration 13722 : loss : 0.036904, loss_ce: 0.010980\n","iteration 13723 : loss : 0.036688, loss_ce: 0.010514\n","iteration 13724 : loss : 0.037517, loss_ce: 0.010525\n","iteration 13725 : loss : 0.036374, loss_ce: 0.015677\n","iteration 13726 : loss : 0.030095, loss_ce: 0.009708\n","iteration 13727 : loss : 0.036182, loss_ce: 0.010889\n","iteration 13728 : loss : 0.038268, loss_ce: 0.018058\n","iteration 13729 : loss : 0.035293, loss_ce: 0.010961\n","iteration 13730 : loss : 0.041445, loss_ce: 0.015384\n","iteration 13731 : loss : 0.028207, loss_ce: 0.006621\n","iteration 13732 : loss : 0.033736, loss_ce: 0.010818\n","iteration 13733 : loss : 0.038329, loss_ce: 0.010701\n","iteration 13734 : loss : 0.034677, loss_ce: 0.014650\n","iteration 13735 : loss : 0.038001, loss_ce: 0.012479\n","iteration 13736 : loss : 0.036664, loss_ce: 0.019169\n","iteration 13737 : loss : 0.032178, loss_ce: 0.008039\n","iteration 13738 : loss : 0.044676, loss_ce: 0.005459\n","iteration 13739 : loss : 0.035276, loss_ce: 0.010564\n","iteration 13740 : loss : 0.099429, loss_ce: 0.008684\n","iteration 13741 : loss : 0.038684, loss_ce: 0.009639\n","iteration 13742 : loss : 0.037081, loss_ce: 0.015074\n","iteration 13743 : loss : 0.039820, loss_ce: 0.011396\n","iteration 13744 : loss : 0.034862, loss_ce: 0.007535\n","iteration 13745 : loss : 0.034439, loss_ce: 0.008939\n","iteration 13746 : loss : 0.044700, loss_ce: 0.014947\n","iteration 13747 : loss : 0.028276, loss_ce: 0.008926\n","iteration 13748 : loss : 0.043791, loss_ce: 0.011714\n","iteration 13749 : loss : 0.049640, loss_ce: 0.007690\n","iteration 13750 : loss : 0.037643, loss_ce: 0.010525\n","iteration 13751 : loss : 0.035510, loss_ce: 0.009740\n","iteration 13752 : loss : 0.048888, loss_ce: 0.011062\n","iteration 13753 : loss : 0.040117, loss_ce: 0.012390\n","iteration 13754 : loss : 0.042932, loss_ce: 0.018796\n","iteration 13755 : loss : 0.036231, loss_ce: 0.015430\n","iteration 13756 : loss : 0.089143, loss_ce: 0.006868\n","iteration 13757 : loss : 0.032736, loss_ce: 0.007523\n","iteration 13758 : loss : 0.095093, loss_ce: 0.008963\n","iteration 13759 : loss : 0.098412, loss_ce: 0.007797\n","iteration 13760 : loss : 0.046475, loss_ce: 0.015986\n","iteration 13761 : loss : 0.042201, loss_ce: 0.011269\n","iteration 13762 : loss : 0.036814, loss_ce: 0.016658\n","iteration 13763 : loss : 0.045041, loss_ce: 0.015632\n","iteration 13764 : loss : 0.100944, loss_ce: 0.012394\n"," 99%|████████████████████████████▌| 148/150 [2:53:51<02:20, 70.14s/it]iteration 13765 : loss : 0.077940, loss_ce: 0.010485\n","iteration 13766 : loss : 0.035975, loss_ce: 0.013412\n","iteration 13767 : loss : 0.035090, loss_ce: 0.009898\n","iteration 13768 : loss : 0.033809, loss_ce: 0.012214\n","iteration 13769 : loss : 0.032693, loss_ce: 0.009294\n","iteration 13770 : loss : 0.097597, loss_ce: 0.009792\n","iteration 13771 : loss : 0.041364, loss_ce: 0.015362\n","iteration 13772 : loss : 0.031209, loss_ce: 0.014751\n","iteration 13773 : loss : 0.038785, loss_ce: 0.010131\n","iteration 13774 : loss : 0.032704, loss_ce: 0.008623\n","iteration 13775 : loss : 0.040425, loss_ce: 0.012134\n","iteration 13776 : loss : 0.037898, loss_ce: 0.011005\n","iteration 13777 : loss : 0.043379, loss_ce: 0.009358\n","iteration 13778 : loss : 0.030049, loss_ce: 0.007837\n","iteration 13779 : loss : 0.041272, loss_ce: 0.013957\n","iteration 13780 : loss : 0.098151, loss_ce: 0.005936\n","iteration 13781 : loss : 0.044043, loss_ce: 0.013539\n","iteration 13782 : loss : 0.029267, loss_ce: 0.008548\n","iteration 13783 : loss : 0.034962, loss_ce: 0.015100\n","iteration 13784 : loss : 0.037234, loss_ce: 0.014021\n","iteration 13785 : loss : 0.035840, loss_ce: 0.011487\n","iteration 13786 : loss : 0.030830, loss_ce: 0.009818\n","iteration 13787 : loss : 0.038038, loss_ce: 0.013003\n","iteration 13788 : loss : 0.037070, loss_ce: 0.010209\n","iteration 13789 : loss : 0.037947, loss_ce: 0.014172\n","iteration 13790 : loss : 0.036450, loss_ce: 0.009238\n","iteration 13791 : loss : 0.049862, loss_ce: 0.010773\n","iteration 13792 : loss : 0.046414, loss_ce: 0.017747\n","iteration 13793 : loss : 0.039295, loss_ce: 0.011740\n","iteration 13794 : loss : 0.034986, loss_ce: 0.012217\n","iteration 13795 : loss : 0.038359, loss_ce: 0.011037\n","iteration 13796 : loss : 0.040167, loss_ce: 0.008317\n","iteration 13797 : loss : 0.102601, loss_ce: 0.010546\n","iteration 13798 : loss : 0.036251, loss_ce: 0.013551\n","iteration 13799 : loss : 0.033759, loss_ce: 0.012962\n","iteration 13800 : loss : 0.036829, loss_ce: 0.010498\n","iteration 13801 : loss : 0.028026, loss_ce: 0.006474\n","iteration 13802 : loss : 0.050515, loss_ce: 0.016185\n","iteration 13803 : loss : 0.045211, loss_ce: 0.013727\n","iteration 13804 : loss : 0.041058, loss_ce: 0.010214\n","iteration 13805 : loss : 0.035861, loss_ce: 0.014700\n","iteration 13806 : loss : 0.036982, loss_ce: 0.011828\n","iteration 13807 : loss : 0.037008, loss_ce: 0.013370\n","iteration 13808 : loss : 0.044381, loss_ce: 0.014279\n","iteration 13809 : loss : 0.033273, loss_ce: 0.009830\n","iteration 13810 : loss : 0.040707, loss_ce: 0.010675\n","iteration 13811 : loss : 0.038233, loss_ce: 0.009998\n","iteration 13812 : loss : 0.105190, loss_ce: 0.006154\n","iteration 13813 : loss : 0.042667, loss_ce: 0.014231\n","iteration 13814 : loss : 0.028556, loss_ce: 0.008436\n","iteration 13815 : loss : 0.028536, loss_ce: 0.012816\n","iteration 13816 : loss : 0.035999, loss_ce: 0.009283\n","iteration 13817 : loss : 0.095062, loss_ce: 0.010915\n","iteration 13818 : loss : 0.039294, loss_ce: 0.006157\n","iteration 13819 : loss : 0.094348, loss_ce: 0.010375\n","iteration 13820 : loss : 0.036271, loss_ce: 0.010149\n","iteration 13821 : loss : 0.095582, loss_ce: 0.007237\n","iteration 13822 : loss : 0.038695, loss_ce: 0.010779\n","iteration 13823 : loss : 0.075327, loss_ce: 0.008521\n","iteration 13824 : loss : 0.033122, loss_ce: 0.012190\n","iteration 13825 : loss : 0.051050, loss_ce: 0.009869\n","iteration 13826 : loss : 0.042258, loss_ce: 0.019190\n","iteration 13827 : loss : 0.038978, loss_ce: 0.022804\n","iteration 13828 : loss : 0.036832, loss_ce: 0.012049\n","iteration 13829 : loss : 0.039036, loss_ce: 0.009339\n","iteration 13830 : loss : 0.037563, loss_ce: 0.008989\n","iteration 13831 : loss : 0.038611, loss_ce: 0.010912\n","iteration 13832 : loss : 0.035037, loss_ce: 0.010695\n","iteration 13833 : loss : 0.034864, loss_ce: 0.010352\n","iteration 13834 : loss : 0.103063, loss_ce: 0.015478\n","iteration 13835 : loss : 0.041215, loss_ce: 0.008632\n","iteration 13836 : loss : 0.039263, loss_ce: 0.019364\n","iteration 13837 : loss : 0.045106, loss_ce: 0.012415\n","iteration 13838 : loss : 0.031365, loss_ce: 0.014386\n","iteration 13839 : loss : 0.029717, loss_ce: 0.004799\n","iteration 13840 : loss : 0.097037, loss_ce: 0.006433\n","iteration 13841 : loss : 0.029129, loss_ce: 0.010413\n","iteration 13842 : loss : 0.040605, loss_ce: 0.022725\n","iteration 13843 : loss : 0.031394, loss_ce: 0.009315\n","iteration 13844 : loss : 0.043426, loss_ce: 0.013227\n","iteration 13845 : loss : 0.035983, loss_ce: 0.009942\n","iteration 13846 : loss : 0.038695, loss_ce: 0.018480\n","iteration 13847 : loss : 0.035463, loss_ce: 0.008996\n","iteration 13848 : loss : 0.041777, loss_ce: 0.006530\n","iteration 13849 : loss : 0.042490, loss_ce: 0.014450\n","iteration 13850 : loss : 0.034921, loss_ce: 0.011427\n","iteration 13851 : loss : 0.042872, loss_ce: 0.010740\n","iteration 13852 : loss : 0.035074, loss_ce: 0.014553\n","iteration 13853 : loss : 0.025544, loss_ce: 0.007699\n","iteration 13854 : loss : 0.039671, loss_ce: 0.008869\n","iteration 13855 : loss : 0.037241, loss_ce: 0.014099\n","iteration 13856 : loss : 0.027993, loss_ce: 0.008259\n","iteration 13857 : loss : 0.285163, loss_ce: 0.017868\n"," 99%|████████████████████████████▊| 149/150 [2:55:01<01:10, 70.12s/it]iteration 13858 : loss : 0.040614, loss_ce: 0.014389\n","iteration 13859 : loss : 0.040127, loss_ce: 0.010737\n","iteration 13860 : loss : 0.032863, loss_ce: 0.006814\n","iteration 13861 : loss : 0.033592, loss_ce: 0.017491\n","iteration 13862 : loss : 0.036929, loss_ce: 0.013112\n","iteration 13863 : loss : 0.038175, loss_ce: 0.009249\n","iteration 13864 : loss : 0.034369, loss_ce: 0.013685\n","iteration 13865 : loss : 0.040908, loss_ce: 0.015044\n","iteration 13866 : loss : 0.033013, loss_ce: 0.009664\n","iteration 13867 : loss : 0.030279, loss_ce: 0.006732\n","iteration 13868 : loss : 0.027205, loss_ce: 0.010413\n","iteration 13869 : loss : 0.032997, loss_ce: 0.007310\n","iteration 13870 : loss : 0.030637, loss_ce: 0.009248\n","iteration 13871 : loss : 0.045402, loss_ce: 0.004472\n","iteration 13872 : loss : 0.048447, loss_ce: 0.010071\n","iteration 13873 : loss : 0.046511, loss_ce: 0.011591\n","iteration 13874 : loss : 0.041876, loss_ce: 0.011258\n","iteration 13875 : loss : 0.101364, loss_ce: 0.013032\n","iteration 13876 : loss : 0.037704, loss_ce: 0.014245\n","iteration 13877 : loss : 0.033049, loss_ce: 0.009516\n","iteration 13878 : loss : 0.038465, loss_ce: 0.011585\n","iteration 13879 : loss : 0.034700, loss_ce: 0.010047\n","iteration 13880 : loss : 0.159425, loss_ce: 0.011977\n","iteration 13881 : loss : 0.042866, loss_ce: 0.015993\n","iteration 13882 : loss : 0.038901, loss_ce: 0.009126\n","iteration 13883 : loss : 0.032462, loss_ce: 0.011067\n","iteration 13884 : loss : 0.035523, loss_ce: 0.012390\n","iteration 13885 : loss : 0.043074, loss_ce: 0.014068\n","iteration 13886 : loss : 0.030079, loss_ce: 0.007239\n","iteration 13887 : loss : 0.026988, loss_ce: 0.007441\n","iteration 13888 : loss : 0.038639, loss_ce: 0.009261\n","iteration 13889 : loss : 0.039903, loss_ce: 0.010155\n","iteration 13890 : loss : 0.031882, loss_ce: 0.010911\n","iteration 13891 : loss : 0.036970, loss_ce: 0.009701\n","iteration 13892 : loss : 0.040680, loss_ce: 0.013038\n","iteration 13893 : loss : 0.034061, loss_ce: 0.014035\n","iteration 13894 : loss : 0.033791, loss_ce: 0.010257\n","iteration 13895 : loss : 0.050211, loss_ce: 0.008044\n","iteration 13896 : loss : 0.040214, loss_ce: 0.014692\n","iteration 13897 : loss : 0.038297, loss_ce: 0.011140\n","iteration 13898 : loss : 0.034075, loss_ce: 0.011569\n","iteration 13899 : loss : 0.098273, loss_ce: 0.008944\n","iteration 13900 : loss : 0.046821, loss_ce: 0.010323\n","iteration 13901 : loss : 0.038363, loss_ce: 0.013269\n","iteration 13902 : loss : 0.036256, loss_ce: 0.009408\n","iteration 13903 : loss : 0.035303, loss_ce: 0.011509\n","iteration 13904 : loss : 0.095547, loss_ce: 0.015470\n","iteration 13905 : loss : 0.033586, loss_ce: 0.014172\n","iteration 13906 : loss : 0.038812, loss_ce: 0.011150\n","iteration 13907 : loss : 0.031931, loss_ce: 0.009597\n","iteration 13908 : loss : 0.040968, loss_ce: 0.011779\n","iteration 13909 : loss : 0.035554, loss_ce: 0.017034\n","iteration 13910 : loss : 0.033431, loss_ce: 0.011202\n","iteration 13911 : loss : 0.042843, loss_ce: 0.012444\n","iteration 13912 : loss : 0.042449, loss_ce: 0.012198\n","iteration 13913 : loss : 0.042179, loss_ce: 0.016784\n","iteration 13914 : loss : 0.029459, loss_ce: 0.008331\n","iteration 13915 : loss : 0.044932, loss_ce: 0.013330\n","iteration 13916 : loss : 0.029175, loss_ce: 0.008829\n","iteration 13917 : loss : 0.035668, loss_ce: 0.011747\n","iteration 13918 : loss : 0.034968, loss_ce: 0.008481\n","iteration 13919 : loss : 0.046351, loss_ce: 0.009135\n","iteration 13920 : loss : 0.041129, loss_ce: 0.011880\n","iteration 13921 : loss : 0.043432, loss_ce: 0.017779\n","iteration 13922 : loss : 0.039467, loss_ce: 0.016717\n","iteration 13923 : loss : 0.041657, loss_ce: 0.013585\n","iteration 13924 : loss : 0.099232, loss_ce: 0.008427\n","iteration 13925 : loss : 0.031929, loss_ce: 0.008875\n","iteration 13926 : loss : 0.075235, loss_ce: 0.010025\n","iteration 13927 : loss : 0.035837, loss_ce: 0.015169\n","iteration 13928 : loss : 0.036580, loss_ce: 0.014743\n","iteration 13929 : loss : 0.045449, loss_ce: 0.017714\n","iteration 13930 : loss : 0.038139, loss_ce: 0.014980\n","iteration 13931 : loss : 0.040721, loss_ce: 0.010323\n","iteration 13932 : loss : 0.033870, loss_ce: 0.010879\n","iteration 13933 : loss : 0.035384, loss_ce: 0.006870\n","iteration 13934 : loss : 0.029804, loss_ce: 0.008756\n","iteration 13935 : loss : 0.097869, loss_ce: 0.009695\n","iteration 13936 : loss : 0.032186, loss_ce: 0.011478\n","iteration 13937 : loss : 0.040716, loss_ce: 0.010995\n","iteration 13938 : loss : 0.095868, loss_ce: 0.005279\n","iteration 13939 : loss : 0.043287, loss_ce: 0.015734\n","iteration 13940 : loss : 0.033410, loss_ce: 0.013131\n","iteration 13941 : loss : 0.039237, loss_ce: 0.011824\n","iteration 13942 : loss : 0.042504, loss_ce: 0.016846\n","iteration 13943 : loss : 0.043147, loss_ce: 0.011917\n","iteration 13944 : loss : 0.055510, loss_ce: 0.010555\n","iteration 13945 : loss : 0.032815, loss_ce: 0.012921\n","iteration 13946 : loss : 0.029429, loss_ce: 0.007529\n","iteration 13947 : loss : 0.032399, loss_ce: 0.010616\n","iteration 13948 : loss : 0.037846, loss_ce: 0.012110\n","iteration 13949 : loss : 0.032549, loss_ce: 0.011697\n","iteration 13950 : loss : 0.110766, loss_ce: 0.022627\n","save model to output/epoch_149.pth\n","save model to output/epoch_149.pth\n"," 99%|████████████████████████████▊| 149/150 [2:56:14<01:10, 70.97s/it]\n"]}],"source":["!python3 TransUNet/train.py --dataset Synapse --cfg TransUNet/configs/swin_tiny_patch4_window7_224_lite.yaml --root_path data/Synapse --max_epochs 150  --output_dir output  --img_size 224 --base_lr 0.05 --batch_size 24"]},{"cell_type":"markdown","metadata":{"id":"LYoyknVjpEFn"},"source":["13. \n","as an snapshot of last iteration here is the loss\n","\n","iteration 13950 : loss : 0.110766, loss_ce: 0.022627\n","\n","we have trained model saved in specified directory to be used for testing "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1650253,"status":"ok","timestamp":1649081077703,"user":{"displayName":"alireza samadifardheris","userId":"04854581352341172090"},"user_tz":-120},"id":"EYrc5qaLaYdP","outputId":"d5617d0e-44f1-43d5-d3e1-bc94f4059b70"},"outputs":[{"output_type":"stream","name":"stdout","text":["=> merge config from TransUNet/configs/swin_tiny_patch4_window7_224_lite.yaml\n","SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:9\n","---final upsample expand_first---\n","self trained swin unet <All keys matched successfully>\n","Namespace(Dataset=<class 'datasets.dataset_synapse.Synapse_dataset'>, accumulation_steps=None, amp_opt_level='O1', base_lr=0.05, batch_size=24, cache_mode='part', cfg='TransUNet/configs/swin_tiny_patch4_window7_224_lite.yaml', dataset='Synapse', deterministic=1, eval=False, img_size=224, is_pretrain=True, is_savenii=True, list_dir='./lists/lists_Synapse', max_epochs=150, max_iterations=30000, num_classes=9, opts=None, output_dir='output', resume=None, seed=1234, tag=None, test_save_dir='../predictions', throughput=False, use_checkpoint=False, volume_path='data/Synapse/test_vol_h5', z_spacing=1, zip=False)\n","epoch_149.pth\n","12 test iterations per epoch\n","0it [00:00, ?it/s]idx 0 case case0008 mean_dice 0.630743 mean_hd95 16.103153\n","1it [02:48, 168.51s/it]idx 1 case case0022 mean_dice 0.895896 mean_hd95 2.669805\n","2it [04:21, 124.15s/it]idx 2 case case0038 mean_dice 0.810431 mean_hd95 10.028434\n","3it [06:08, 116.30s/it]tcmalloc: large alloc 1157627904 bytes == 0x56509f61a000 @  0x7fd0c68a01e7 0x7fd0c3a5e0ce 0x7fd0c3ab8726 0x7fd0c3b4d841 0x564f6bc9d11c 0x564f6bd8ed4d 0x564f6bd10ec8 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0d719 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0c8f6 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd10d30 0x564f6bc9e7aa 0x564f6bd0c8f6 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0d719 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0c8f6 0x564f6bd0ba2e 0x564f6bd0b723 0x564f6bdd5812 0x564f6bdd5b8d 0x564f6bdd5a36 0x564f6bdad183 0x564f6bdace2c\n","idx 3 case case0036 mean_dice 0.839544 mean_hd95 17.118374\n","4it [09:24, 147.57s/it]idx 4 case case0032 mean_dice 0.875332 mean_hd95 6.108676\n","5it [11:47, 146.17s/it]idx 5 case case0002 mean_dice 0.827065 mean_hd95 8.875320\n","6it [14:04, 142.88s/it]idx 6 case case0029 mean_dice 0.676645 mean_hd95 59.992588\n","7it [15:43, 128.46s/it]tcmalloc: large alloc 1245708288 bytes == 0x56501691a000 @  0x7fd0c68a01e7 0x7fd0c3a5e0ce 0x7fd0c3ab8726 0x7fd0c3b4d841 0x564f6bc9d11c 0x564f6bd8ed4d 0x564f6bd10ec8 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0d719 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0c8f6 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd10d30 0x564f6bc9e7aa 0x564f6bd0c8f6 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0d719 0x564f6bd0ba2e 0x564f6bc9e88a 0x564f6bd0c8f6 0x564f6bd0ba2e 0x564f6bd0b723 0x564f6bdd5812 0x564f6bdd5b8d 0x564f6bdd5a36 0x564f6bdad183 0x564f6bdace2c\n","idx 7 case case0003 mean_dice 0.557981 mean_hd95 102.472105\n","8it [19:18, 156.08s/it]idx 8 case case0001 mean_dice 0.755987 mean_hd95 20.253285\n","9it [21:56, 156.78s/it]idx 9 case case0004 mean_dice 0.704588 mean_hd95 37.522198\n","10it [24:26, 154.81s/it]idx 10 case case0025 mean_dice 0.800707 mean_hd95 22.456046\n","11it [25:55, 134.41s/it]idx 11 case case0035 mean_dice 0.851593 mean_hd95 5.066271\n","12it [27:18, 136.56s/it]\n","Mean class 1 mean_dice 0.853545 mean_hd95 7.095818\n","Mean class 2 mean_dice 0.625262 mean_hd95 38.631334\n","Mean class 3 mean_dice 0.811244 mean_hd95 33.998574\n","Mean class 4 mean_dice 0.751730 mean_hd95 33.105737\n","Mean class 5 mean_dice 0.935798 mean_hd95 23.406778\n","Mean class 6 mean_dice 0.569976 mean_hd95 12.925390\n","Mean class 7 mean_dice 0.875304 mean_hd95 39.275486\n","Mean class 8 mean_dice 0.728149 mean_hd95 17.338385\n","Testing performance in best val model: mean_dice : 0.768876 mean_hd95 : 25.722188\n"]}],"source":["!python3 TransUNet/test.py --dataset Synapse --cfg TransUNet/configs/swin_tiny_patch4_window7_224_lite.yaml --is_saveni --volume_path data/Synapse --output_dir output --max_epoch 150 --base_lr 0.05 --img_size 224 --batch_size 24"]},{"cell_type":"markdown","source":["\n","15. \n","Mean class 1 mean_dice 0.853545 mean_hd95 7.095818\n","\n","Mean class 2 mean_dice 0.625262 mean_hd95 38.631334\n","\n","Mean class 3 mean_dice 0.811244 mean_hd95 33.998574\n","\n","Mean class 4 mean_dice 0.751730 mean_hd95 33.105737\n","\n","Mean class 5 mean_dice 0.935798 mean_hd95 23.406778\n","\n","Mean class 6 mean_dice 0.569976 mean_hd95 12.925390\n","\n","Mean class 7 mean_dice 0.875304 mean_hd95 39.275486\n","\n","\n","Testing performance in best val model:\n","\n"," mean_dice : 0.768876 mean_hd95 : 25.722188\n","\n"," so close to experiment results achived by auther ( for instance my result is 76.88 and paper result is 79.13 )"],"metadata":{"id":"8X31_nAPfUHs"}},{"cell_type":"markdown","source":["16. then the result of the code would be numerical output as well as predictions on tested images. so personally i was interested to see by my own eyes except numerical output\n","\n","17. so to visulize it i added some lines of code to be able to see the results"],"metadata":{"id":"MJlUrxQMhhlk"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cmd.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}